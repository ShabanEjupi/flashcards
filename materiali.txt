Stochastic Matrices
Many types of applications involve a finite set of states of a given
population. For instance, residents of a city may live downtown or in the suburbs. Voters
may vote Democrat, Republican, or for a third party. Soft drink consumers may buy
Coca-Cola, Pepsi Cola, or another brand.
S1, S2, . . . , Sn

2.5
48. (a) Show that the matrix
does not have an LU-factorization.
(b) Find the LU-factorization of the matrix
that has 1’s along the main diagonal of Are there any
restrictions on the matrix ?
In Exercises 49–54, determine whether the matrix is idempotent.
A square matrix is idempotent if
49. 50.
51. 52.
53. 54.
55. Determine and such that is idempotent.
56. Determine conditions on and such that is idempotent.
57. Prove that if is an matrix that is idempotent and
invertible, then
58. Guided Proof Prove that is idempotent if and only if is
idempotent.
Getting Started: The phrase “if and only if” means that you
have to prove two statements:
1. If is idempotent, then is idempotent.
2. If is idempotent, then A is idempotent.
(i) Begin your proof of the first statement by assuming
that A is idempotent.
(ii) This means that
(iii) Use the properties of the transpose to show that
is idempotent.
(iv) Begin your proof of the second statement by assuming that is idempotent.
59. Prove that if and are idempotent and then is
idempotent.
60. Prove that if is row-equivalent to B, then B is row-equivalent
to
61. Guided Proof Prove that if is row-equivalent to and is
row-equivalent to then is row-equivalent to
Getting Started: To prove that is row-equivalent to
you have to find elementary matrices such that
(i) Begin your proof by observing that is row-equivalent
to
(ii) Meaning, there exist elementary matrices
such that
(iii) There exist elementary matrices such that
(iv) Combine the matrix equations from steps (ii) and (iii).
62. Let A be a nonsingular matrix. Prove that if B is row-equivalent
to A, then B is also nonsingular.
B  G1 . . . GmC.
G1, . . . , Gm
A  Fn
. . . F1B.
F1, . . . , Fn
B.
A
Ek A  . . . E1C.
E1, . . . , Ek
A C,
, AC C.
A BB
A.
A
BA AB  BA, AB
AT
AT
A2  A.
AT
AT A
AT A
A  In.
A n  n
A  
a
b
0
c
, b, ca A
A  
1
a
0
b
ba A

0
1
0
1
0
0
0
0
1   0
0
1
0
1
0
1
0
0


2
1
3
2   2
1
3
2

0
1
1
0   1
0
0
0
A A 2  A.
A
L.
A  
a
c
b
d
A  
0
1
1
0
The probability that a member of a population will change from the th state to the th
state is represented by a number where A probability of means that
the member is certain not to change from the th state to the th state, whereas a probability of means that the member is certain to change from the th state to the th state.
From
. . .
To
is called the matrix of transition probabilities because it gives the probabilities of each
possible type of transition (or change) within the population.
At each transition, each member in a given state must either stay in that state or change
to another state. For probabilities, this means that the sum of the entries in any column of
is 1. For instance, in the first column we have
In general, such a matrix is called stochastic (the term “stochastic” means “regarding
conjecture”). That is, an matrix is called a stochastic matrix if each entry is a
number between 0 and 1 and each column of adds up to 1.
The matrices in parts (a) and (b) are stochastic, but the matrix in part (c) is not.
(a) (b) (c)
Example 2 describes the use of a stochastic matrix to measure consumer preferences.
Two competing companies offer cable television service to a city of 100,000 households.
The changes in cable subscriptions each year are shown in Figure 2.1. Company A now has
15,000 subscribers and Company B has 20,000 subscribers. How many subscribers will
each company have 1 year from now?
EXAMPLE 2 A Consumer Preference Model

0.1
0.2
0.3
0.2
0.3
0.4
0.3
0.4
0.5   1
2
1
4
1
4
1
3
0
2
3
1
4
3
4
0   1
0
0
0
1
0
0
0
1

EXAMPLE 1 Examples of Stochastic Matrices and Nonstochastic Matrices
P
n  n P
p11  p21  . . .  pn1  1.
P
P
S1
S2
.
.
.
Sn
P

p11
p21
.
.
.
pn1
p12
p22
.
.
.
pn2
...
...
...
p1n
p2n
.
.
.
pnn 
S1 S2 Sn
p j i ij  1
j i
0  p pij  0 p ij  1. ij,
j i
Section 2.5 Applications of Matrix Operations 99
100 Chapter 2 Matrices
Figure 2.1
SOLUTION The matrix representing the given transition probabilities is
From
A B None
and the state matrix representing the current populations in the three states is
To find the state matrix representing the populations in the three states after one year,
multiply by to obtain
After one year, Company A will have 23,250 subscribers and Company B will have 28,750
subscribers.
One of the appeals of the matrix solution in Example 2 is that once the model has been
created, it becomes easy to find the state matrices representing future years by repeatedly
multiplying by the matrix This process is demonstrated in Example 3. P.
PX  
0.70
0.20
0.10
0.15
0.80
0.05
0.15
0.15
0.70 15,000
20,000
65,000  
23,250
28,750
48,000
.
XP
X  
15,000
20,000
65,000
.
A
B
None P  To 
0.70
0.20
0.10
0.15
0.80
0.05
0.15
0.15
0.70
Cable
Company A
No Cable
Television
Cable
Company B
70% 80%
70%
20%
15%
5%
10% 15%
15%
A
B
None
Section 2.5 Applications of Matrix Operations 101
Assuming the matrix of transition probabilities from Example 2 remains the same year
after year, find the number of subscribers each cable television company will have after
(a) 3 years, (b) 5 years, and (c) 10 years. (The answers in this example have been rounded
to the nearest person.)
SOLUTION (a) From Example 2 you know that the number of subscribers after 1 year is
Because the matrix of transition probabilities is the same from the first to the third year,
the number of subscribers after 3 years is
After 3 years, Company A will have 30,283 subscribers and Company B will have
39,042 subscribers.
(b) The number of subscribers after 5 years is
After 5 years, Company A will have 32,411 subscribers and Company B will have
43,812 subscribers.
(c) The number of subscribers after 10 years is
After 10 years, Company A will have 33,287 subscribers and Company B will have
47,147 subscribers.
In Example 3, notice that there is little difference between the numbers of subscribers
after 5 years and after 10 years. If the process shown in this example is continued, the
numbers of subscribers eventually reach a steady state. That is, as long as the matrix
doesn’t change, the matrix product approaches a limit In this particular example, the
limit is the steady state matrix
You can check to see that PX  X.
X  
33,333
47,619
19,048
.
P X. nX
P
P10X  
33,287
47,147
19,566
.
P5X  
32,411
43,812
23,777
.
P3X  
30,283
39,042
30,675
.
PX  
23,250
28,750
48,000
.
EXAMPLE 3 A Consumer Preference Model
A
B After 10 years
None
A
B Steady state
None
A
B After 1 year
None
A
B After 3 years
None
A
B After 5 years
None
102 Chapter 2 Matrices
Cryptography
A cryptogram is a message written according to a secret code (the Greek word kryptos
means “hidden”). This section describes a method of using matrix multiplication to encode
and decode messages.
Begin by assigning a number to each letter in the alphabet (with 0 assigned to a blank
space), as follows.
Then the message is converted to numbers and partitioned into uncoded row matrices,
each having entries, as demonstrated in Example 4.
Write the uncoded row matrices of size for the message MEET ME MONDAY.
SOLUTION Partitioning the message (including blank spaces, but ignoring punctuation) into groups of
three produces the following uncoded row matrices.
Note that a blank space is used to fill out the last uncoded row matrix.
To encode a message, choose an invertible matrix and multiply the uncoded row
matrices (on the right) by to obtain coded row matrices. This process is demonstrated in
Example 5.
A
n  n A
[1
A
25
Y
0]
__
[15
O
14
N
4]
D
[5
E
0
__
13]
M
[20
T
0
__
13]
M
[13
M
5
E
5]
E
1  3
EXAMPLE 4 Forming Uncoded Row Matrices
n
13  M
12  L 26  Z
11  K 25  Y
10  J 24  X
9  I 23  W
8  H 22  V
7  G 21  U
6  F 20  T
5  E 19  S
4  D 18  R
3  C 17  Q
2  B 16  P
1  A 15  O
0  __ 14  N
Section 2.5 Applications of Matrix Operations 103
Use the matrix
to encode the message MEET ME MONDAY.
SOLUTION The coded row matrices are obtained by multiplying each of the uncoded row matrices found
in Example 4 by the matrix , as follows.
Uncoded Encoding Coded Row
Row Matrix Matrix A Matrix
The sequence of coded row matrices is
Finally, removing the brackets produces the cryptogram below.
13 21 33 18 5 56 23 77
For those who do not know the matrix decoding the cryptogram found in Example 5
is difficult. But for an authorized receiver who knows the matrix decoding is simple. The
receiver need only multiply the coded row matrices by to retrieve the uncoded row
matrices. In other words, if
X  x1 x2 . . . xn
A1
A,
A,
26 53 12 23 42 20 24
13 26 21 33 53 12 18 23 42 5 20 56 24 23 77.
1 25 0
 1
1
1
2
1
1
2
3
4
  24 23 77
15 14 4
 1
1
1
2
1
1
2
3
4
  5 20 56
5 0 13
 1
1
1
2
1
1
2
3
4
  18 23 42
20 0 13
 1
1
1
2
1
1
2
3
4
  33 53 12
13 5 5
 1
1
1
2
1
1
2
3
4
  13 26 21
A
A   1
1
1
2
1
1
2
3
4

EXAMPLE 5 Encoding a Message
104 Chapter 2 Matrices
is an uncoded matrix, then is the corresponding encoded matrix. The
receiver of the encoded matrix can decode by multiplying on the right by to obtain
This procedure is demonstrated in Example 6.
Use the inverse of the matrix
to decode the cryptogram
SOLUTION Begin by using Gauss-Jordan elimination to find
Now, to decode the message, partition the message into groups of three to form the coded
row matrices
.
To obtain the decoded row matrices, multiply each coded row matrix by (on the right).
Coded Row Decoding Decoded
Matrix Matrix Row Matrix
5 20 56

1
1
0
10
6
1
8
5
1
  15 14 4
18 23 42

1
1
0
10
6
1
8
5
1
  5 0 13
33 53 12

1
1
0
10
6
1
8
5
1
  20 0 13
13 26 21

1
1
0
10
6
1
8
5
1
  13 5 5
A1
A1
13 26 21 33 53 12 18 23 42 5 20 56 24 23 77
1
1
0
10
6
1
8
5
1   1
0
0
0
1
0
0
0
1






1
0
0
0
1
0
0
0
1   1
1
1
2
1
1
2
3
4
I  A1 A  I 
A1
.
13 26 21 33 53 12 18 23 42 5 20 56 24 23 77.
A   1
1
1
2
1
1
2
3
4

EXAMPLE 6 Decoding a Message
YA1  XAA1  X.
A1 Y
1  n Y  XA
Simulation
Explore this concept further with an
electronic simulation available on
the website college.hmco.com/
pic/larsonELA6e.
Coded Row Decoding Decoded
Matrix Matrix Row Matrix
The sequence of decoded row matrices is
and the message is
Leontief Input-Output Models
Matrix algebra has proved effective in analyzing problems concerning the input and output
of an economic system. The model discussed here, developed by the American economist
Wassily W. Leontief (1906–1999), was first published in 1936. In 1973, Leontief was
awarded a Nobel prize for his work in economics.
Suppose that an economic system has different industries . . . , each of which
has input needs (raw materials, utilities, etc.) and an output (finished product). In producing each unit of output, an industry may use the outputs of other industries, including itself.
For example, an electric utility uses outputs from other industries, such as coal and water,
and even uses its own electricity.
Let be the amount of output the th industry needs from the th industry to produce
one unit of output per year. The matrix of these coefficients is called the input-output
matrix.
User (Output)
. . .
Supplier (Input)
To understand how to use this matrix, imagine This means that 0.4 unit of
Industry 1’s product must be used to produce one unit of Industry 2’s product. If
then 0.2 unit of Industry 3’s product is needed to produce one unit of its own product. For
this model to work, the values of must satisfy and the sum of the entries in
any column must be less than or equal to 1.
dij 0  dij  1
d33  0.2,
d12  0.4.
I1
I2
.
.
.
In
D

d11
d21
.
.
.
dn1
d12
d22
.
.
.
dn2
...
...
...
d1n
d2n
.
.
.
dnn

In I2 I1
d j i ij
In I , 2 I , 1 n ,
13
M
5
E
5
E
20
T
0
__
13
M
5
E
0
__
13
M
15
O
14
N
4
D
1
A
25
Y
0.
__
13 5 5 20 0 13 5 0 13 15 14 4 1 25 0
24 23 77

1
1
0
10
6
1
8
5
1
  1 25 0
A1
Section 2.5 Applications of Matrix Operations 105
Consider a simple economic system consisting of three industries: electricity, water, and
coal. Production, or output, of one unit of electricity requires 0.5 unit of itself, 0.25 unit of
water, and 0.25 unit of coal. Production, or output, of one unit of water requires 0.1 unit of
electricity, 0.6 unit of itself, and 0 units of coal. Production, or output, of one unit of coal
requires 0.2 unit of electricity, 0.15 unit of water, and 0.5 unit of itself. Find the input-output
matrix for this system.
SOLUTION The column entries show the amounts each industry requires from the others, as well as from
itself, in order to produce one unit of output.
User (Output)
EWC
Supplier (Input)
The row entries show the amounts each industry supplies to the other industries, as well
as to itself, in order for that particular industry to produce one unit of output. For instance,
the electricity industry supplies 0.5 unit to itself, 0.1 unit to water, and 0.2 unit to coal.
To develop the Leontief input-output model further, let the total output of the th industry be denoted by If the economic system is closed (meaning that it sells its products
only to industries within the system, as in the example above), then the total output of the
th industry is given by the linear equation
Closed system
On the other hand, if the industries within the system sell products to nonproducing
groups (such as governments or charitable organizations) outside the system, then the
system is called open and the total output of the th industry is given by
Open system
where represents the external demand for the th industry’s product. The collection
of total outputs for an open system is represented by the following system of linear
equations.
The matrix form of this system is
where is called the X output matrix and is called the E external demand matrix.
X  DX  E,
x1
x2
xn

.
.
.

d11x1
d21x1
dn1x1



d12x 2
d22x 2
dn2x 2
 ... 
 ... 
 ... 
d1n xn
d2nxn
dnn xn



e1
e2
en
n
e i i
xi  di1x1  di2x2  . . .  dinxn  ei ,
i
xi  di1x1  di2x2  . . .  dinxn.
i
xi
.
i
E
W
C 
0.5
0.25
0.25
 0.1
 0.6
 0
0.2
0.15
0.5 
EXAMPLE 7 Forming an Input-Output Matrix
106 Chapter 2 Matrices
An economic system composed of three industries has the input-output matrix shown below.
User (Output)
ABC
Supplier (Input)
Find the output matrix if the external demands are
(The answers in this example have been rounded to the nearest unit.)
SOLUTION Letting be the identity matrix, write the equation as which
means
Using the matrix above produces
Finally, applying Gauss-Jordan elimination to the system of linear equations represented by
produces
So, the output matrix is
To produce the given external demands, the outputs of the three industries must be as
follows.
Output for Industry A: 46,616 units
Output for Industry B: 51,058 units
Output for Industry C: 38,014 units
X  
46,616
51,058
38,014
.

1
0
0
0
1
0
0
0
1
46,616
51,058
38,014
.  0.9
0.15
0.23
0.43
1
0.03
0
0.37
0.98
20,000
30,000
25,000
I  DX  E
I  D   0.9
0.15
0.23
0.43
1
0.03
0
0.37
0.98
.
D
I  DX  E.
I X  DX  E IX  DX  E,
E  
20,000
30,000
25,000
.
X
A
B
C
D  
0.1
0.15
0.23
0.43
0
0.03
0
0.37
0.02 
EXAMPLE 8 Solving for the Output Matrix of an Open System
A
B
C
Section 2.5 Applications of Matrix Operations 107
A
B
C
108 Chapter 2 Matrices
The economic systems described in Examples 7 and 8 are, of course, simple ones. In the
real world, an economic system would include many industries or industrial groups. For
example, an economic analysis of some of the producing groups in the United States would
include the products listed below (taken from the Statistical Abstract of the United States).
1. Farm products (grains, livestock, poultry, bulk milk)
2. Processed foods and feeds (beverages, dairy products)
3. Textile products and apparel (yarns, threads, clothing)
4. Hides, skins, and leather (shoes, upholstery)
5. Fuels and power (coal, gasoline, electricity)
6. Chemicals and allied products (drugs, plastic resins)
7. Rubber and plastic products (tires, plastic containers)
8. Lumber and wood products (plywood, pencils)
9. Pulp, paper, and allied products (cardboard, newsprint)
10. Metals and metal products (plumbing fixtures, cans)
11. Machinery and equipment (tractors, drills, computers)
12. Furniture and household durables (carpets, appliances)
13. Nonmetallic mineral products (glass, concrete, bricks)
14. Transportation equipment (automobiles, trucks, planes)
15. Miscellaneous products (toys, cameras, linear algebra texts)
A matrix of order would be required to represent even these broad industrial
groupings using the Leontief input-output model. A more detailed analysis could easily
require an input-output matrix of order greater than Clearly, this type of analysis could be done only with the aid of a computer.
Least Squares Regression Analysis
You will now look at a procedure that is used in statistics to develop linear models. The next
example demonstrates a visual method for approximating a line of best fit for a given set of
data points.
Determine the straight line that best fits the points.
and
SOLUTION Plot the points, as shown in Figure 2.2. It appears that a good choice would be the line whose
slope is 1 and whose -intercept is 0.5. The equation of this line is
An examination of the line shown in Figure 2.2 reveals that you can improve the fit
by rotating the line counterclockwise slightly, as shown in Figure 2.3. It seems clear that
this new line, the equation of which is fits the given points better than the
original line.
y  1.2x,
y  0.5  x.
y
1, 1, 2, 2, 3, 4, 4, 4, 5, 6
EXAMPLE 9 A Visual Straight-Line Approximation
100  100.
15  15
Figure 2.2 Figure 2.3
One way of measuring how well a function fits a set of points
is to compute the differences between the values from the function and the actual
values , as shown in Figure 2.4. By squaring these differences and summing the results,
you obtain a measure of error that is called the sum of squared error. The sums of squared
errors for our two linear models are shown in Table 2.1 below.
TABLE 2.1
Model 1: Model 2:
1 1 1.5 1 1 1.2
2 2 2.5 2 2 2.4
3 4 3.5 3 4 3.6
4 4 4.5 4 4 4.8
5 6 5.5 5 6 6.0
Total 1.25 Total 1.00
The sums of squared errors confirm that the second model fits the given points better than
the first.
Of all possible linear models for a given set of points, the model that has the best fit is
defined to be the one that minimizes the sum of squared error. This model is called the least
squares regression line, and the procedure for finding it is called the method of least
squares.
0.02 0.52
0.82 0.52
0.42 0.52
0.42 0.52
0.22 0.52
[yi  f (xi)]2 f (xi y ) i xi [yi  f (xi)]2 f (xi y ) i xi
f(x)  0.5  x f(x)  1.2x
yi
fxi

x1, y1, x2, y2, . . . , xn, yn
y  f(x)
x
1
1
2
3
4
5
6
2 3 4 5 6
(1, 1)
(2, 2)
(3, 4) (4, 4)
(5, 6)
y = 0.5 + x
y = 1.2x
y y
x
1
1
2
3
4
5
6
2 3 4 5 6
(1, 1)
(2, 2)
(3, 4) (4, 4)
(5, 6)
y = 0.5 + x
Section 2.5 Applications of Matrix Operations 109
x
1
1
2
3
4
5
6
2 3 4 5 6
(1, 1)
(2, 2)
(3, 4) (4, 4)
(5, 6)
y = 0.5 + x
Model 1
y
Figure 2.4
x
1
1
2
3
4
5
6
2 3 4 5 6
(1, 1)
(2, 2)
(3, 4) (4, 4)
(5, 6)
y = 1.2x
Model 2
y
110 Chapter 2 Matrices
To find the least squares regression line for a set of points, begin by forming the system
of linear equations
where the right-hand term, of each equation is thought of as the error in the
approximation of by Then write this error as
so that the system of equations takes the form
Now, if you define and as
the linear equations may be replaced by the matrix equation
Note that the matrix has two columns, a column of 1’s (corresponding to ) and a
column containing the ’s. This matrix equation can be used to determine the coefficients
of the least squares regression line, as follows.
xi
a X 0
Y  XA  E.
n
E

e1
e2
.
.
.
en
 A  
a0
a1
 X  ,

1
1
.
.
.
1
x1
x2.
.
.
xn
 Y  ,

y1
y2
.
.
.
yn

,
Y, X, A, E
yn  a0  a1xn  en.
.
.
.
y2  a0  a1x2  e2
y1  a0  a1x1  e1
ei  yi  fxi

fxi y . i
yi  fxi
,
yn  fxn  yn  fxn
.
.
.
y2  fx2  y2  fx2
y1  fx1  y1  fx1
For a set of points the least squares regression line is
given by the linear function
that minimizes the sum of squared error
y1  fx12  y2  fx22  . . .  yn  fxn2.
fx  a0  a1x
x1, y1, x2, y2, . . . , xn, yn Definition of Least ,
Squares Regression Line
Figure 2.5
x
1
1
2
3
4
5
6
2 3 4 5 6
(1, 1)
(2, 2)
(3, 4) (4, 4)
(5, 6)
y = − 0.2 + 1.2x
Least Squares Regression Line
y
Section 2.5 Applications of Matrix Operations 111
REMARK : You will learn more about this procedure in Section 5.4.
Example 10 demonstrates the use of this procedure to find the least squares regression
line for the set of points from Example 9.
Find the least squares regression line for the points and
(see Figure 2.5). Then find the sum of squared error for this regression line.
SOLUTION Using the five points below, the matrices and are
and
This means that
and
XTY  
1
1
1
2
1
3
1
4
1
5

1
2
4
4
6
  
17
63.
XTX  
1
1
1
2
1
3
1
4
1
5

1
1
1
1
1
1
2
3
4
5
  
5
15
15
55
Y

1
2
4
4
6
 X  .

1
1
1
1
1
1
2
3
4
5

YX
1, 1, 2, 2, 3, 4, 4, 4, 5, 6
EXAMPLE 10 Finding the Least Squares Regression Line
For the regression model the coefficients of the least squares regression line
are given by the matrix equation
and the sum of squared error is
ETE.
A  XTX1XT Y
Matrix Form for Y  XA  E,
Linear Regression
Exercises
112 Chapter 2 Matrices
Now, using to find the coefficient matrix you have
The least squares regression line is
(See Figure 2.5.) The sum of squared error for this line can be shown to be 0.8, which
means that this line fits the data better than either of the two experimental linear models
determined earlier.
y  0.2  1.2x.
A  XTX1XTY  1
50 55
15
15
517
63  
0.2
1.2.
X A, TX1
SECTION 2.5
Stochastic Matrices
In Exercises 1–6, determine whether the matrix is stochastic.
1. 2.
3. 4.
5. 6.
7. The market research department at a manufacturing plant
determines that 20% of the people who purchase the plant’s
product during any month will not purchase it the next month.
On the other hand, 30% of the people who do not purchase the
product during any month will purchase it the next month. In a
population of 1000 people, 100 people purchased the product
this month. How many will purchase the product next month? In
2 months?
8. A medical researcher is studying the spread of a virus in a
population of 1000 laboratory mice. During any week there is an
80% probability that an infected mouse will overcome the virus,
and during the same week there is a 10% probability that a
noninfected mouse will become infected. One hundred mice are
currently infected with the virus. How many will be infected next
week? In 2 weeks?
9. A population of 10,000 is grouped as follows: 5000 nonsmokers,
2500 smokers of one pack or less per day, and 2500 smokers of
more than one pack per day. During any month there is a 5%
probability that a nonsmoker will begin smoking a pack or
less per day, and a 2% probability that a nonsmoker will begin
smoking more than a pack per day. For smokers who smoke a
pack or less per day, there is a 10% probability of quitting and a
10% probability of increasing to more than a pack per day. For
smokers who smoke more than a pack per day, there is a 5%
probability of quitting and a 10% probability of dropping to a
pack or less per day. How many people will be in each of the
3 groups in 1 month? In 2 months?
10. A population of 100,000 consumers is grouped as follows:
20,000 users of Brand A, 30,000 users of Brand B, and 50,000
who use neither brand. During any month a Brand A user has a
20% probability of switching to Brand B and a 5% probability
of not using either brand. A Brand B user has a 15% probability of switching to Brand A and a 10% probability of not
using either brand. A nonuser has a 10% probability of purchasing Brand A and a 15% probability of purchasing Brand B. How
many people will be in each group in 1 month? In 2 months? In
3 months?
11. A college dormitory houses 200 students. Those who watch an
hour or more of television on any day always watch for less than
an hour the next day. One-fourth of those who watch television
for less than an hour one day will watch an hour or more the next
day. Half of the students watched television for an hour or more
today. How many will watch television for an hour or more
tomorrow? In 2 days? In 30 days?

1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1   1
3
1
3
1
3
1
6
2
3
1
6
1
4
1
4
1
2


0.3
0.5
0.2
0.1
0.2
0.7
0.8
0.1
0.1   0
0
1
1
0
0
0
1
0
 2
2
2
2
2
2
2
2
  2
5
3
5
2
5
7
5

Section 2.5 Applications of Matrix Operations 113
12. For the matrix of transition probabilities
find and for the state matrix
Then find the steady state matrix for
13. Prove that the product of two stochastic matrices is
stochastic.
14. Let be a stochastic matrix. Prove that there exists a
state matrix with nonnegative entries such that
Cryptography
In Exercises 15–18, find the uncoded row matrices of the indicated
size for the given messages. Then encode the message using the
matrix
15. Message: SELL CONSOLIDATED
Row Matrix Size:
Encoding Matrix:
16. Message: PLEASE SEND MONEY
Row Matrix Size:
Encoding Matrix:
17. Message: COME HOME SOON
Row Matrix Size:
Encoding Matrix:
18. Message: HELP IS COMING
Row Matrix Size:
Encoding Matrix:
In Exercises 19–22, find and use it to decode the cryptogram.
19.
11 21 64 112 25 50 29 53 23 46 40 75 55 92
20.
85 120 6 8 10 15 84 117 42 56 90 125 60 80 30
45 19 26
21.
13 19 10 3 4 1
4 1
22.
112 83 19 13 72 61 95 71 20 21
38 35 36 42 32
23. The cryptogram below was encoded with a matrix.
8 21 5 10 5 25 5 19 6 20 40
1 16
The last word of the message is __RON. What is the message?
24. The cryptogram below was encoded with a matrix.
5 2 25 11 32 14 38 19
37 16
The last word of the message is __SUE. What is the message?
25. Use a graphing utility or computer software program with
matrix capabilities to find Then decode the cryptogram.
38 29 56 62 17 3 38 18 20 76 18 21 29
32 32 9 77 36 48 33 51 41 3 79 12 1 26
58 49 63 69 28 8 67 31 27 41 28 22 19 11 18
7 8 5
14 15 5
A  
1
2
0
0
1
1
2
1
2
A1
.
19
2 7 15 15 8 13 19
2  2
18 18
15 10 13 13 1
2  2
23 48
140 25 76 118
A  
3
0
4
4
2
5
2
1
3

,
47 9
1 33 77 2 14 9 5 25
A   1
3
1
2
7
4
2
9
7

,
A  
2
3
3
4,
A  
1
3
2
5,
A1
A

2
1
1
3
3
1
1
1
1
1
1
2
1
1
2
4

1  4
A  
1
3
2
5
1  2
A   4
3
3
2
3
2
1
1
1

1  3
A   1
1
6
1
0
2
0
1
3

1  3
A.
2  1 X PX  X.
P 2  2
2  2
P.
X  
100
100
800
.
P3 P X 2X
P  
0.6
0.2
0.2
0.1
0.7
0.2
0.1
0.1
0.8
,
114 Chapter 2 Matrices
26. A code breaker intercepted the encoded message below.
45 38 18 35 81 42 75
2 22 15
Let
(a) You know that and
where is the inverse of
the encoding matrix Write and solve two systems of
equations to find and
(b) Decode the message.
Leontief Input-Output Models
27. A system composed of two industries, coal and steel, has the
following input requirements.
(a) To produce $1.00 worth of output, the coal industry
requires $0.10 of its own product and $0.80 of steel.
(b) To produce $1.00 worth of output, the steel industry
requires $0.10 of its own product and $0.20 of coal.
Find the input-output matrix for this system. Then solve for
the output matrix in the equation where the
external demand is
28. An industrial system has two industries with the following input
requirements.
(a) To produce $1.00 worth of output, Industry A requires
$0.30 of its own product and $0.40 of Industry B’s
product.
(b) To produce $1.00 worth of output, Industry B requires
$0.20 of its own product and $0.40 of Industry A’s
product.
Find the input-output matrix for this system. Then solve for
the output matrix in the equation where the
external demand is
29. A small community includes a farmer, a baker, and a grocer and
has the input-output matrix and external demand matrix
shown below.
Farmer Baker Grocer
and
Solve for the output matrix in the equation
30. An industrial system has three industries and the input-output
matrix and external demand matrix shown below.
and
Solve for the output matrix in the equation
Least Squares Regression Analysis
In Exercises 31–34, (a) sketch the line that appears to be the best fit
for the given points, (b) use the method of least squares to find the
least squares regression line, and (c) calculate the sum of the
squared error.
31. 32.
33. 34.
In Exercises 35–42, find the least squares regression line.
35.
36.
37.
38.
39.
40.
41.
42. 0, 6, 4, 3, 5, 0, 8, 4, 10, 5
5, 10, 1, 8, 3, 6, 7, 4, 5, 5
3, 4, 1, 2, 1, 1, 3, 0
5, 1, 1, 3, 2, 3, 2, 5
4, 1, 2, 0, 2, 4, 4, 5
2, 0, 1, 1, 0, 1, 1, 2
1, 0, 3, 3, 5, 6
0, 0, 1, 1, 2, 4
5 6
2
1
−2
3
4
x
(4, 2)
(5, 2)
(6, 2)
(4, 1)
(3, 1)
(1, 0)
(2, 0) (3, 0)
−1
y
x
1 2
1
−1
2
3
4 (0, 4)
(1, 3)
(1, 1) (2, 0)
y
3
2
−1 1 2 3
−2
3
4
x
(3, 2)
(1, 1) (−1, 1)
(−3, 0)
y
−2 −1 21
2
1
3
4
x
(−2, 0) (0, 1)
(2, 3)
y
X X  DX  E.
E  
5000
2000
8000 D  
0.2
0.4
0.0
0.4
0.2
0.2
0.4
0.2
0.2
D E
X X  DX  E.
E  
1000
1000
1000 Farmer
Baker
Grocer
D  
0.40
0.30
0.20
0.50
0.00
0.20
0.50
0.30
0.00
D E
E  
50,000
30,000.
X X  DX  E,
D,
E  
10,000
20,000.
X X  DX  E,
D,
w, x, y, z.
A.
A  1 38  30A 8 14, 1
45  35A1  10 15
A1  
w
y
x
z.
55 2 21 10
35 30 18 30 60 28
Review Exercises
Chapter 2 Review Exercises 115
43. A fuel refiner wants to know the demand for a certain grade of
gasoline as a function of the price. The daily sales (in gallons)
for three different prices of the product are shown in the table.
Price (x) $3.00 $3.25 $3.50
Demand (y) 4500 3750 3300
(a) Find the least squares regression line for these data.
(b) Estimate the demand when the price is $3.40.
44. A hardware retailer wants to know the demand for a rechargeable power drill as a function of the price. The monthly sales for
four different prices of the drill are shown in the table.
Price (x) $25 $30 $35 $40
Demand (y) 82 75 67 55
(a) Find the least squares regression line for these data.
(b) Estimate the demand when the price is $32.95.
45. The table shows the numbers of motor vehicle registrations
(in millions) in the United States for the years 2000 through
2004. (Source: U.S. Federal Highway Administration)
Year 2000 2001 2002 2003 2004
Number (y) 221.5 230.4 229.6 231.4 237.2
(a) Use the method of least squares to find the least squares
regression line for the data. Let represent the year,
with corresponding to 2000.
(b) Use the linear regression capabilities of a graphing
utility to find a linear model for the data. Let represent
the year, with corresponding to 2000.
46. A wildlife management team studied the reproduction rates of
deer in 3 tracts of a wildlife preserve. Each tract contained
5 acres. In each tract the number of females and the percent of
females that had offspring the following year were recorded.
The results are shown in the table.
Number (x) 100 120 140
Percent (y) 75 68 55
(a) Use the method of least squares to find the least squares
regression line that models the data.
(b) Use a graphing utility to graph the model and the data
in the same viewing window.
(c) Use the model to create a table of estimated values of
Compare the estimated values with the actual data.
(d) Use the model to estimate the percent of females that
had offspring when there were 170 females.
(e) Use the model to estimate the number of females when
40% of the females had offspring.



In Theorem 4.9, you saw that if is a basis for a vector space then every vector in
can be expressed in one and only one way as a linear combination of vectors in The
coefficients in the linear combination are the coordinates of relative to In the context
of coordinates, the order of the vectors in this basis is important, and this will sometimes
be emphasized by referring to the basis as an ordered basis.
Coordinate Representation in Rn
In the notation for coordinate matrices conforms to the usual component notation,
except that column notation is used for the coordinate matrix. In other words, writing a
Rn
,
B
x B.
B.
B V, x V
4.7
Let be an ordered basis for a vector space and let be a vector
in such that
The scalars are called the coordinates of relative to the basis The
coordinate matrix (or coordinate vector) of relative to is the column matrix in
whose components are the coordinates of
xB

c1
c2
.
.
.
cn

x.
R B n x
c x B. 1, c2, . . . , cn
x  c1v1  c2v2  . . .  cnvn.
V
Coordinate Representation B  v1, v2, . . . , vn
 V x
Relative to a Basis
250 Chapter 4 Vector Spaces
vector in as means that the ’s are the coordinates of relative to
the standard basis in So, you have
Find the coordinate matrix of in relative to the standard basis
SOLUTION Because can be written as
you can see that the coordinate matrix of relative to the standard basis is simply
So, the components of are the same as its coordinates relative to the standard basis.
The coordinate matrix of in relative to the (nonstandard) ordered basis
is
Find the coordinates of relative to the standard basis
SOLUTION Because you can write
Moreover, because it follows that the coordinates of relative
to are
Figure 4.19 compares these two coordinate representations.
xB  
5
4.
B
5, 4  51, 0  40, 1, x
x  3v1  2v2  31, 0  21, 2  5, 4.
xB  
3
2,
B  u1, u2 x   1, 0, 0, 1.
xB  
3
2.
1, 0, 1, 2
B  v1, v2 x R2 
EXAMPLE 2 Finding a Coordinate Matrix Relative to a Standard Basis
x
xS  
2
1
3

.
x
x  2, 1, 3  21, 0, 0  10, 1, 0  30, 0, 1,
x
S  1, 0, 0, 0, 1, 0, 0, 0, 1.
R3 x  2, 1, 3
EXAMPLE 1 Coordinates and Components in Rn
x S

x1
x2
.
.
.
xn

.
Rn S .
x x i x  x1, x2, . . . , xn R  n
Section 4.7 Coordinates and Change of Basis 251
Figure 4.19
Example 2 shows that the procedure for finding the coordinate matrix relative to a
standard basis is straightforward. The problem becomes a bit tougher, however, when you
must find the coordinate matrix relative to a nonstandard basis. Here is an example.
Find the coordinate matrix of in relative to the (nonstandard) basis
SOLUTION Begin by writing as a linear combination of and
Equating corresponding components produces the following system of linear equations.
The solution of this system is and So,
x  51, 0, 1  80, 1, 2  22, 3, 5,
c c 3  2. c 2  8, 1  5,

1
0
1
0
1
2
2
3
5
 c1
c2
c3
   1
2
1

c1
c1 
c2
2c2



2c3
3c3
5c3



1
2
1
1, 2, 1  c11, 0, 1  c20, 1, 2  c32, 3, 5
x  c1u1  c2u2  c3u3
u3 u . 1, u2 x ,
B  u1, u2, u3
  1, 0, 1, 0, 1, 2, 2, 3, 5.
R3 x  1, 2, 1
EXAMPLE 3 Finding a Coordinate Matrix Relative to a Nonstandard Basis
Standard basis:
B' = {(1, 0), (0, 1)}
x
(5, 4)
4u2
5u1
u2
u1
[x]
B'
= 5
4[ [
y x = 5(1, 0) + 4(0, 1)
x'
y'
Nonstandard basis:
B = {(1, 0), (1, 2)}
[x]
B = 3
2[ [
x = 3(1, 0) + 2(1, 2)
(3, 2)
2v2
3v1
v2
v1
252 Chapter 4 Vector Spaces
and the coordinate matrix of relative to is
REMARK : Note that the solution in Example 3 is written as
It would be incorrect to write the solution as
Do you see why?
Change of Basis in Rn
The procedure demonstrated in Examples 2 and 3 is called a change of basis. That is, you
were provided with the coordinates of a vector relative to one basis and were asked to
find the coordinates relative to another basis
For instance, if in Example 3 you let be the standard basis, then the problem of finding the coordinate matrix of relative to the basis becomes one of solving
for and in the matrix equation
The matrix is called the transition matrix from to where is the coordinate
matrix of relative to and is the coordinate matrix of relative to Multiplication
by the transition matrix changes a coordinate matrix relative to into a coordinate
matrix relative to That is,
Change of basis from to
To perform a change of basis from to use the matrix (the transition matrix from
to ) and write
x Change of basis from to B B
 B  P1
xB.
B B
P1 B B,
PxB  xB. 
 BB
B.
P B x x B. B B x ,
x B B P B ,
xB x P B

1
0
1
0
1
2
2
3
5
 c1
c2
c3
   1
2
1

.
c3 c1, c2,
x  1, 2, 1 B B
B.
B
x   5
8
2

.
xB   5
8
2

.
xB   5
8
2

.
x B
Section 4.7 Coordinates and Change of Basis 253
This means that the change of basis problem in Example 3 can be represented by the matrix
equation
This discussion generalizes as follows. Suppose that
and
are two ordered bases for If is a vector in and
and
are the coordinate matrices of relative to and then the transition matrix from
to is the matrix such that
The next theorem tells you that the transition matrix is invertible and its inverse is the
transition matrix from to That is,
Before proving Theorem 4.20, you will prove a preliminary lemma.
Coordinate
matrix of x
relative to B
Transition
matrix
from B to B
Coordinate
matrix of x
relative to B
xB  P1
xB.
B B.
P
xB  PxB
.
B P
x B B, P B
xB

d1
d2
.
.
.
dn
 xB

c1
c2
.
.
.
cn

Rn R x n.
B  u1, u2, . . . , un B  v  1, v2, . . . , vn

xB x P B 1

c1
c2
c3
  
1
3
1
4
7
2
2
3
1
  1
2
1
   5
8
2

.
If is the transition matrix from a basis to a basis in then is invertible and the
transition matrix from to is given by P1 B B .
PRn P B B , THEOREM 4.20
The Inverse of a
Transition Matrix
254 Chapter 4 Vector Spaces
PROOF (OF LEMMA ) Let
be an arbitrary vector in The coordinate matrix of with respect to the basis is
Then you have
On the other hand, you can write
which implies
So, and you can conclude that is the transition matrix from to Qv Q B B. B  vB
vB

c11d1  c12d2  . . .  c1ndn
c21d1  c22d2  . . .  c2ndn .
.
. .
.
. .
.
. cn1d1  cn2d2  . . .  cnndn

.
 d1c11  . . .  dnc1nu1  . . .  d1cn1  . . .  dncnnun,
 d1c11u1  . . .  cn1un  . . .  dnc1nu1  . . .  cnnun
v  d1v1  d2v2  . . .  dnvn


c11d1
c21d1 .
.
. cn1d1



c12d2
c22d2 .
.
. cn2d2



...
...
...



c1ndn
c2ndn .
.
. cnndn
 Qv . B

c11
c21
.
.
.
cn1
c12
c22
.
.
.
cn2
...
...
...
c1n
c2n
.
.
.
cnn d1
d2
.
.
.
dn

vB

d1
d2
.
.
.
dn

.
V. v B
v  d1v1  d2v2  . . .  dnvn
Let and be two bases for a vector
space If
then the transition matrix from to is
Q

c11
c21
.
.
.
cn1
c12
c22
.
.
.
cn2
...
...
...
c1n
c2n
.
.
.
cnn

.
B B
vn  c1nu1  c2nu2  . . .  cnnun,
.
.
.
v2  c12u1  c22u2  . . .  cn2un
v1  c11u1  c21u2  . . .  cn1un
V.
B  u1, u2, . . . , un B  v  1, v2, . . . , vn LEMMA 
Section 4.7 Coordinates and Change of Basis 255
PROOF (OF THEOREM 4.20) From the preceding lemma, let be the transition matrix from to Then
and
which implies that for every vector in From this it follows that
So, is invertible and is equal to the transition matrix from to
There is a nice way to use Gauss-Jordan elimination to find the transition matrix
First define two matrices and whose columns correspond to the vectors in and
That is,
and
Then, by reducing the matrix so that the identity matrix occurs in place
of you obtain the matrix This procedure is stated formally in the next
theorem.
PROOF To begin, let
which implies that
for From these vector equations you can write the systems of linear
equations
i  1, 2, . . . , n. n
c1i

u11
u12
.
.
.
u1n
  c2i

u21
u22
.
.
.
u2n
  ...  cni
un1
un2
.
.
.
unn


vi1
vi 2
.
.
.
vin

vn  c1nu1  c2nu2  . . .  cnnun,
.
.
.
v2  c12u1  c22u2  . . .  cn2un
v1  c11u1  c21u2  . . .  cn1un
In  P1 B, .
In n  2n B  B
v1 v2 vn u1 u2 un
B

u11
u21
.
.
.
un1
u12
u22
.
.
.
un2
...
...
...
u1n
u2n
.
.
.
unn
 B  .

v11
v21
.
.
.
vn1
v12
v22
.
.
.
vn2
...
...
. . .
v1n
v2n
.
.
.
vnn
B B B B.
P1
.
P Q, B B. 1 PQ  I. P
Rn v v . B  PQvB
vB  QvB v , B  PvB
Q B B.
Let and be two bases for Then the
transition matrix from to can be found by using Gauss-Jordan elimination on the
matrix , as follows.
In  P1 B  B 
n  2n B  B
BP B 1
Rn B  u . 1, u2, . . . , un B  v  1, v2, . . . , vn THEOREM 4.21 
Transition Matrix
from to B B
for Because each of the systems has the same coefficient matrix, you
can reduce all systems simultaneously using the augmented matrix below.
Applying Gauss-Jordan elimination to this matrix produces
By the lemma following Theorem 4.20, however, the right-hand side of this matrix is
which implies that the matrix has the form
which proves the theorem.
In the next example, you will apply this procedure to the change of basis problem from
Example 3.
Find the transition matrix from to for the following bases in .
and
SOLUTION First use the vectors in the two bases to form the matrices and
and
Then form the matrix and use Gauss-Jordan elimination to rewrite as

1
0
0
0
1
0
0
0
1
.
.
.
.
.
.
.
.
.
1
3
1
4
7
2
2
3
1   1
0
1
0
1
2
2
3
5
.
.
.
.
.
.
.
.
.
1
0
0
0
1
0
0
0
1

I3  P1
.
B  B B  B
B  
1
0
1
0
1
2
2
3
5
 B  
1
0
0
0
1
0
0
0
1

B B.
B  1, 0, 0, 0, 1, 0, 0, 0, 1 B  1, 0, 1, 0, 1, 2, 2, 3, 5
R3 B B
EXAMPLE 4 Finding a Transition Matrix
I  P1
,
Q  P1
,

1
0
.
.
.
0
0
1
.
.
.
0
. . .
...
...
0
0
.
.
.
1
.
.
.
.
.
.
.
.
.
.
.
.
c11
c21
.
.
.
cn1
c12
c22
.
.
.
cn2
. . .
...
...
c1n
c2n
.
.
.
cnn

.
B B

u11
u12
.
.
.
u1n
u21
u22
.
.
.
u2n
...
...
...
un1
un2
.
.
.
unn
.
.
.
.
.
.
.
.
.
.
.
.
v11
v12
.
.
.
v1n
v21
v22
.
.
.
v2n
...
...
...
vn1
vn2
.
.
.
vnn

n
i  1, 2, . . . , n. n
u1nc1i  u2nc2i  . . .  unncni  vin
.
.
.
u12c1i  u22c2i  . . .  un2cni  vi2
u11c1i  u21c2i  . . .  un1cni  vi1
256 Chapter 4 Vector Spaces
Section 4.7 Coordinates and Change of Basis 257
From this you can conclude that the transition matrix from to is
Try multiplying by the coordinate matrix of
to see that the result is the same as the one obtained in Example 3.
Note that when is the standard basis, as in Example 4, the process of changing
to becomes
But this is the same process that was used to find inverse matrices in Section 2.3. In other
words, if is the standard basis in then the transition matrix from to is
Standard basis to nonstandard basis
The process is even simpler if is the standard basis, because the matrix is
already in the form
In this case, the transition matrix is simply
Nonstandard basis to standard basis
For instance, the transition matrix in Example 2 from to
is
Find the transition matrix from to for the following bases for .
B  3, 2, 4, 2 and B  1, 2, 2, 2
R2 B B
EXAMPLE 5 Finding a Transition Matrix
P1  B  
1
0
1
2.
0, 1
B  1, 0, 1, 2 B  1, 0,
P1  B.
In  B  In  P1
.
B B  B
P1  B1
.
R B B n B ,
In  P1 B  I . n
In  P1 B  B 
B
x   1
2
1

P1
P1  
1
3
1
4
7
2
2
3
1

.
B B
Let and Form the matrix Make a conjecture
about the necessity of using Gauss-Jordan elimination to obtain the transition matrix if the
change of basis is from a nonstandard basis to a standard basis.
P1
Discovery B  1, 0, 1, 2 B  1, 0, 0, 1. B  B.
258 Chapter 4 Vector Spaces
SOLUTION Begin by forming the matrix
and use Gauss-Jordan elimination to obtain the transition matrix from to
So, you have
In Example 5, if you had found the transition matrix from to (rather than from
to ), you would have obtained
B .
.
. B  
3
2
4
2
.
.
.
.
.
. 1
2
2
2,
B B
 BB
P1  
1
2
2
3.
I2
.
.
. P1
  
1
0
0
1
.
.
.
.
.
.
1
2
2
3.
BP B: 1
B .
.
. B  
1
2
2
2
.
.
.
.
.
.
3
2
4
2
Most graphing utilities and computer software programs have the capability to augment two
matrices. After this has been done, you can use the reduced row-echelon form command to find
the transition matrix from to For example, to find the transition matrix from to in
Example 5 using a graphing utility, your screen may look like:
Use a graphing utility or a computer software program with matrix capabilities to find the transition
matrix from to Keystrokes and programming syntax for these utilities/programs applicable to
Example 5 are provided in the Online Technology Guide, available at college.hmco.com/pic/
larsonELA6e.
P B B.
BP B 1 BP B. 1
Technology
Note
Section 4.7 Coordinates and Change of Basis 259
which reduces to
The transition matrix from to is
You can verify that this is the inverse of the transition matrix found in Example 5 by
multiplication:
Coordinate Representation in General n-Dimensional Spaces
One benefit of coordinate representation is that it enables you to represent vectors in an
arbitrary -dimensional space using the same notation used in For instance, in Example
6, note that the coordinate matrix of a vector in is a vector in
Find the coordinate matrix of relative to the standard basis in
SOLUTION First write as a linear combination of the basis vectors (in the order provided).
This tells you that the coordinate matrix of relative to is
In the preceding section, you saw that it is sometimes convenient to represent
matrices as -tuples. The next example presents some justification for this practice.
Find the coordinate matrix of
X  
1
4
3

EXAMPLE 7 Coordinate Representation in M3,1
n
n  1
 p S

4
0
2
3

.
p S
p  41  0x  2x2  3x3
p
S  1, x, x2, x3.
P3 p  3x , 3  2x2  4
EXAMPLE 6 Coordinate Representation in P3
R4 P . 3
Rn n .
PP1  
3
2
2
11
2
2
3  
1
0
0
1  I2.
P  
3
2
2
1.
 BB
I2
.
.
. P  
1
0
0
1
.
.
.
.
.
. 3
2
2
1.
260 Chapter 4 Vector Spaces
relative to the standard basis in
SOLUTION Because can be written as
the coordinate matrix of relative to is
Theorems 4.20 and 4.21 can be generalized to cover arbitrary -dimensional spaces. This
text, however, does not cover this.
n
XS  
1
4
3

.
X S
 1

1
0
0
  4

0
1
0
  3

0
0
1
 X  , 
1
4
3

X
S  
1
0
0

, 
0
1
0

,

0
0
1
.
M3,1,
SECTION 4.7 Exercises
In Exercises 1–6, you are provided with the coordinate matrix of
relative to a (nonstandard) basis Find the coordinate vector of
relative to the standard basis in
1.
2.
3.
4.
5.
6.
In Exercises 7–12, find the coordinate matrix of in relative to
the basis
7.
8.
9.
10.
11.
12.
In Exercises 13–18, find the transition matrix from to by hand.
13.
14.
15.
16.
17.
18.
In Exercises 19–28, use a graphing utility or computer software
program with matrix capabilities to find the transition matrix from
to
19.
20. B  2, 1, 3, 2, B  1, 2, 1, 0
B  2, 5, 1, 2, B  2, 1, 1, 2
B B.
B  1, 3, 1, 2, 7, 4, 2, 9, 7
B  1, 0, 0, 0, 1, 0, 0, 0, 1,
B  1, 0, 0, 0, 2, 8, 6, 0, 12
B  1, 0, 0, 0, 1, 0, 0, 0, 1,
B  1, 1, 1, 0, B  1, 0, 0, 1
B  2, 4, 1, 3, B  1, 0, 0, 1
B  1, 0, 0, 1, B  1, 1, 5, 6
B  1, 0, 0, 1, B  2, 4, 1, 3
B B
x  0, 20, 7, 15
B  9, 3, 15, 4, 3, 0, 0, 1, 0, 5, 6, 8, 3, 4, 2, 3,
B  4, 3, 3, 11, 0, 11, 0, 9, 2, x  11, 18, 7
x  3, 1
2 B   , 8 3
2, 4, 1,
3
4, 5
2, 0, 1, 1
2, 2,
B  8, 11, 0, 7, 0, 10, 1, 4, 6, x  3, 19, 2
B  6, 7, 4, 3, x  26, 32
B  4, 0, 0, 3, x  12, 6
B.
Rn x
xB  2, 3, 4, 1T
B  4, 0, 7, 3, 0, 5, 1, 1, 3, 4, 2, 1, 0, 1, 5, 0,
xB  1, 2, 3, 1T
B  0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
xB  2, 0, 4T B  
3
4, 5
2, 3
2, 3, 4, 7
2, 3
2, 6, 2,
xB  2, 3, 1 T B  1, 0, 1, 1, 1, 0, 0, 1, 1,
xB  2, 3T B  1, 4, 4, 1,
xB  4, 1T B  2, 1, 0, 1,
Rn.
B. x
x
REMARK : In Section 6.2 you
will learn more about the use
of to represent an arbitrary
n-dimensional vector space.
Rn
Section 4.7 Coordinates and Change of Basis 261
21.
22.
23.
24.
25.
26.
27.
28.
In Exercises 29–32, use Theorem 4.21 to (a) find the transition
matrix from to (b) find the transition matrix from to
(c) verify that the two transition matrices are inverses of each other,
and (d) find when provided with
29.
30.
31.
32.
In Exercises 33 and 34, use a graphing utility with matrix capabilities to (a) find the transition matrix from B to (b) find the
transition matrix from to (c) verify that the two transition
matrices are inverses of one another, and (d) find when
provided with .
33.
34.
In Exercises 35–38, find the coordinate matrix of relative to the
standard basis in
35. 36.
37. 38.
In Exercises 39–42, find the coordinate matrix of relative to the
standard basis in
39. 40.
41. 42.
True or False? In Exercises 43 and 44, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false,
provide an example that shows the statement is not true in all cases
or cite an appropriate statement from the text.
43. (a) If is the transition matrix from a basis to then the
equation represents the change of basis from
B to B.
PxB  x
P B B,
X  
1
0
4
 X   1
2
1

X   2
1
4
 X  
0
3
2

M3,1.
X
p  4x p  2x 2  3x  2 2  5x  1
p  3x p  x 2  114x  13 2  11x  4
P2.
p
xB  
1
0
2

B  1, 2, 2, 4, 1, 4, 2, 5, 8,
B  1, 3, 4, 2, 5, 2, 4, 2, 6,
xB  
1
1
2

B  1, 0, 4, 4, 2, 8, 2, 5, 2,
B  4, 2, 4, 6, 5, 6, 2, 1, 8,
xB
xB
B B,
B,
xB  
2
3
1

B  2, 2, 0, 0, 1, 1, 1, 0, 1,
B  1, 1, 1, 1, 1, 1, 0, 0, 1,
xB   1
2
1

B  2, 1, 1, 1, 0, 0, 0, 2, 1,
B  1, 0, 2, 0, 1, 3, 1, 1, 1,
xB   2
1
B  2, 2, 6, 3, B  1, 1, 32, 31,
xB  
1
3
B  1, 3, 2, 2, B  12, 0, 4, 4,
xB x . B
B B, B B,
2, 1, 2, 1, 1, 0, 1, 2, 3, 1
B  2, 4, 2, 1, 0, 3, 1, 0, 1, 2, 0, 0, 2, 4, 5,
0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
B  1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
0, 1, 2, 2, 1, 1, 1, 0, 1, 2
B  1, 2, 4, 1, 2, 2, 3, 4, 2, 1, 0, 1, 2, 2, 1,
0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
B  1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
B  1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1
B  1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
2, 3, 5, 11
B  1, 3, 2, 1, 2, 5, 5, 4, 1, 2, 2, 4,
B  1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
B  1, 1, 1, 0, 1, 2, 1, 4, 0
B  3, 2, 1, 1, 1, 2, 1, 2, 0,
B  0, 2, 1, 2, 1, 0, 1, 1, 1
B  1, 2, 4, 1, 2, 0, 2, 4, 0,
B  1, 0, 0, 0, 1, 0, 0, 0, 1
B  2, 1, 4, 0, 2, 1, 3, 2, 1,
B  1, 0, 0, 0, 1, 0, 0, 0, 1
B  1, 3, 3, 1, 5, 6, 1, 4, 5,
262 Chapter 4 Vector Spaces
(b) If is the standard basis in then the transition matrix
from to is
44. (a) If is the transition matrix used to perform a change of
basis from to then is the transition matrix from
to
(b) To perform the change of basis from a nonstandard basis
to the standard basis the transition matrix is
simply
45. Let be the transition matrix from to and let be the
transition matrix from to What is the transition matrix
from to
46. Let be the transition matrix from to and let be the
transition matrix from to What is the transition matrix
from to
47. Writing Let and be two bases for the vector space
Discuss the nature of the transition matrix from to if one
of the bases is the standard basis.
48. Writing Is it possible for a transition matrix to equal the identity matrix? Illustrate your answer with appropriate examples.



Linear Differential Equations (Calculus)
A linear differential equation of order is of the form
where and are functions of with a common domain. If the
equation is homogeneous. Otherwise it is nonhomogeneous. A function is called a
solution of the linear differential equation if the equation is satisfied when and its first
derivatives are substituted into the equation.
Show that both and are solutions of the second-order linear differential
equation
SOLUTION For the function you have and So,
and is a solution of the differential equation. Similarly, for you have
and
This implies that
So, is also a solution of the linear differential equation.
There are two important observations you can make about Example 1. The first is that
in the vector space of all twice differentiable functions defined on the entire C, 
y2  ex
y2  y2  ex  ex  0.
y2  ex y . 2  ex
y2  ex y , 1  ex
y1  y1  ex  ex  0,
y1  ex y . 1
  ex y1  ex
,
y  y  0.
y2  ex y1  ex
EXAMPLE 1 A Second-Order Linear Differential Equation
n
y
y
g f x fx  0, 0, g1, . . . , gn1,
yn  gn1xyn1  . . .  g1xy  g0xy  fx,
n
4.8
Section 4.8 Applications of Vector Spaces 263
real line, the two solutions and are linearly independent. This means that
the only solution of
that is valid for all is The second observation is that every linear combination of and is also a solution of the linear differential equation. To see this, let
Then
Substituting into the differential equation produces
So, is a solution.
These two observations are generalized in the next theorem, which is stated without
proof.
In light of the preceding theorem, you can see the importance of being able to determine
whether a set of solutions is linearly independent. Before describing a way of testing for
linear independence, you are provided with a preliminary definition.
y  C1ex  C2ex
y  y  C1ex  C2ex
  C1ex  C2ex
  0.
y  y  0
y  C1ex  C2ex
.
y  C1ex  C2ex
y  C1ex  C2ex
y  C1y1  C2y2.
y2 y1
x C1  C2  0.
C1y1  C2y2  0
y2  ex y1  ex
Every th-order linear homogeneous differential equation
has linearly independent solutions. Moreover, if is a set of linearly
independent solutions, then every solution is of the form
where and are real numbers. C1 Cn , C2, . . . ,
y  C1y1  C2 y2  . . .  Cn yn,
y1, y2, . . . , yn n 
yn  gn1xyn1  . . .  g1xy  g0xy  0
Solutions of a Linear n
Homogeneous Differential
Equation
Let be a set of functions, each of which has derivatives on an
interval The determinant
is called the Wronskian of the given set of functions.
W y1, y2, . . . , yn
 y1
y1

.
.
. y1
n1
y2
y2

.
.
. y2
n1
...
...
...
yn
yn

.
.
. yn
n1

I.
y n  1 1, y2, . . . , yn Definition of the 
Wronskian of a Set
of Functions
REMARK : The solution
is
called the general solution of
the given differential equation.
C1y1  C2 y2  . . .  Cn yn
y
264 Chapter 4 Vector Spaces
REMARK : The Wronskian of a set of functions is named after the Polish mathematician
Josef Maria Wronski (1778–1853).
(a) The Wronskian of the set is
(b) The Wronskian of the set is
The Wronskian in part (a) of Example 2 is said to be identically equal to zero, because
it is zero for any value of The Wronskian in part (b) is not identically equal to zero
because values of exist for which this Wronskian is nonzero.
The next theorem shows how the Wronskian of a set of functions can be used to test for
linear independence.
REMARK : This test does not apply to an arbitrary set of functions. Each of the functions
and must be a solution of the same linear homogeneous differential equation of order
Determine whether is a set of linearly independent solutions of the linear
homogeneous differential equation
SOLUTION Begin by observing that each of the functions is a solution of (Try checking
this.) Next, testing for linear independence produces the Wronskian of the three functions, as
follows.
y  y  0.
y  y  0.
1, cos x, sin x
EXAMPLE 3 Testing a Set of Solutions for Linear Independence
n.
yn y1, y2, . . . ,
x
x.
W

x
1
0
x2
2x
2
x3
3x2
6x  2x3.
x, x2
, x3
W

1  x
1
0
1  x
1
0
2  x
1
0  0.
1  x, 1  x, 2  x
EXAMPLE 2 Finding the Wronskian of a Set of Functions
Let be a set of solutions of an th-order linear homogeneous
differential equation. This set is linearly independent if and only if the Wronskian is not
identically equal to zero.
y n n 1, y2, . . . , yn Wronskian Test for 
Linear Independence
Section 4.8 Applications of Vector Spaces 265
Because is not identically equal to zero, you can conclude that the set is
linearly independent. Moreover, because this set consists of three linearly independent
solutions of a third-order linear homogeneous differential equation, you can conclude that
the general solution is
Determine whether is a set of linearly independent solutions of the
linear homogeneous differential equation
SOLUTION As in Example 3, begin by verifying that each of the functions is actually a solution of
(This verification is left to you.) Testing for linear independence
produces the Wronskian of the three functions as follows.
So, the set is linearly dependent.
In Example 4, the Wronskian was used to determine that the set
is linearly dependent. Another way to determine the linear dependence of this set is to
observe that the third function is a linear combination of the first two. That is,
Try showing that the different set forms a linearly independent set of solutions of the differential equation
Conic Sections and Rotation
Every conic section in the -plane has an equation of the form
ax2  bxy  cy2  dx  ey  f  0.
xy
y  3y  3y  y  0.
ex
, xex
, x2
ex
x  1ex  ex  xex
.
ex
, xex
, x  1ex
ex
, xex
, x  1ex
W   0

ex
ex
ex
xex
x  1ex
x  2ex
x  1ex
x  2ex
x  3ex

y  3y  3y  y  0.
y  3y  3y  y  0.
ex
, xex
, x  1ex
EXAMPLE 4 Testing a Set of Solutions for Linear Independence
y  C1  C2 cos x  C3 sin x.
W 1, cos x, sin x
 sin  1 2 x  cos2 x
W

1
0
0
cos x
sin x
cos x
sin x
cos x
sin x
266 Chapter 4 Vector Spaces
Identifying the graph of this equation is fairly simple as long as the coefficient of the
-term, is zero. In such cases the conic axes are parallel to the coordinate axes, and the
identification is accomplished by writing the equation in standard (completed square) form.
The standard forms of the equations of the four basic conics are provided in the next summary. For circles, ellipses, and hyperbolas, the point is the center. For parabolas, the
point is the vertex. h, k
h, k
xy
b,
Circle
Ellipse
Hyperbola
(y − k)
2 (x − h)
2
− = 1
x
y
2
2
(h, k)
2 α 2 β
α
β
(x − h)
2 (y − k)
2
− = 1
x
y
2
2
2
(h, k)
α
β
α 2 β
2
  transverse axis length, 2  minor axis length:
(x − h)
2 (y − k)
2
+ = 1
x
y
2
(h, k) 2
2
β 2 α
β
α
x
(x − h)
2 (y − k)
2
2 2 + =1 α β
y
(h, k)
2
2
α
β
2
  major axis length, 2  minor axis length:
x  h2  y  k2  r 2 Standard Forms of r  radius:
Equations of Conics
Section 4.8 Applications of Vector Spaces 267
(a) The standard form of is The graph
of this equation is a parabola with the vertex at The axis of the parabola
is vertical. Because the focus is the point Finally, because the focus lies
below the vertex, the parabola opens downward, as shown in Figure 4.20(a).
(b) The standard form of is
The graph of this equation is an ellipse with its center at The major
axis is horizontal, and its length is The length of the minor axis is The
vertices of this ellipse occur at and and the endpoints of the minor axis
occur at and as shown in Figure 4.20(b).
(a) (b)
Figure 4.20
−5 −4 −3 −2 −1
1
2
3
x
(−3, 2)
(−3, 1)
(−5, 1) (−1, 1)
(−3, 0)
y
(x + 3)2
4
(y − 1)2
1 + = 1
−2 234
−3
−2
−1
1
x
(1, 1)
(1, 0)
Focus
(x − 1)2 = 4(−1)(y − 1)
y
3, 2 3, 0,
5, 1 1, 1,
2
  4. 2  2.
h, k  3, 1.
x  32
4
 y  12
1  1.
x2  4y2  6x  8y  9  0
p  1, 1, 0.
h, k  1, 1.
x  1 x 2  41y  1. 2  2x  4y  3  0
EXAMPLE 5 Identifying Conic Sections
Parabola
x
Focus
(+ ) h p, k
Vertex
(, ) h k
p > 0
( ) = y − k 4px h ( ) 2
y
−
Focus
(h, k + p)
Vertex
(h, k)
p > 0
(x − h)2 = 4p(y − k) x
y
Standard Forms of p  directed distance from vertex to focus):
Equations of Conics (cont.)
268 Chapter 4 Vector Spaces
The equations of the conics in Example 5 have no -term. Consequently, the axes of the
corresponding conics are parallel to the coordinate axes. For second-degree polynomial
equations that have an -term, the axes of the corresponding conics are not parallel
to the coordinate axes. In such cases it is helpful to rotate the standard axes to form a
new -axis and -axis. The required rotation angle (measured counterclockwise) is
With this rotation, the standard basis in the plane
is rotated to form the new basis
as shown in Figure 4.21.
To find the coordinates of a point relative to this new basis, you can use a transition matrix, as demonstrated in Example 6.
Find the coordinates of a point in relative to the basis
SOLUTION By Theorem 4.21 you have
Because is the standard basis in is represented by You can use Theorem
3.10 to find This results in
By letting be the coordinates of relative to you can use the transition
matrix as follows.
The - and -coordinates are
The last two equations in Example 6 give the -coordinates in terms of the
-coordinates. To perform a rotation of axes for a second-degree polynomial equation, it is
helpful to express the -coordinates in terms of the -coordinates. To do this, solve the
last two equations in Example 6 for and to obtain
x  x cos   y sin  and y  x sin   y cos .
yx
xy xy xy
xy
y  x sin   y cos .
x  x cos   y sin 
x y
 cos 
sin 
sin 
cos  
x
y  
x
y

P1
x, y x, y B, T
I .
.
. P1
  
1
0
0
1
.
.
.
.
.
. cos 
sin 
sin 
cos .
B1
.
B1 P . 1 R2 B ,
B .
.
. B  
cos 
sin 
sin 
cos 
.
.
.
.
.
. 1
0
0
1.
B  cos , sin , sin , cos .
R2 x, y
EXAMPLE 6 A Transition Matrix for Rotation in the Plane
x, y
B  cos , sin , sin , cos ,
B  1, 0, 0, 1
cot 2  a  c
b.
x y 
xy
xy
Figure 4.21
y'
x
x'
(cos , sin )
(−sin , cos )
(0, 1)
(1, 0)
θ
θ
θ
θ
θ
y
Substituting these expressions for and into the given second-degree equation produces
a second-degree polynomial equation in and that has no -term.
REMARK : When you solve for and the trigonometric identity
is often useful.
Example 7 demonstrates how to identify the graph of a second-degree polynomial by
rotating the coordinate axes.
Perform a rotation of axes to eliminate the -term in
and sketch the graph of the resulting equation in the -plane.
SOLUTION The angle of rotation is represented by
This implies that So,
and
By substituting
and
y  x sin   y cos   1
2
x  y
x  x cos   y sin   1
2
x  y
cos   1
2
sin   . 1
2
  
4.
cot 2  a  c
b  5  5
6  0.
xy
5x2  6xy  5y2  142x  22y  18  0,
xy
EXAMPLE 7 Rotation of a Conic Section
cot 2   1
2 cot 
cot 2
sin  cos ,
x y xy yx
Section 4.8 Applications of Vector Spaces 269
The second-degree equation can be written in the
form
by rotating the coordinate axes counterclockwise through the angle where is defined by
The coefficients of the new equation are obtained from the substitutions
y  x sin   y cos .
x  x cos   y sin 
cot 2  a  c
b .
, 
ax2  cy2  dx  ey  f  0
ax2  bxy  cy Rotation of Axes 2  dx  ey  f  0
Figure 4.22
−5 −4
−4
−3
−2
1
2 y' x'
x
( 2, 0) −
(−3 2, −2 2)
θ = 45°
y
(x' + 3)2
4
(y' − 1)2
1 + = 1
270 Chapter 4 Vector Spaces
into the original equation and simplifying, you obtain
Finally, by completing the square, you find the standard form of this equation to be
which is the equation of an ellipse, as shown in Figure 4.22.
In Example 7 the new (rotated) basis for is
and the coordinates of the vertices of the ellipse relative to are and
To find the coordinates of the vertices relative to the standard basis
use the equations
and
to obtain and as shown in Figure 4.22. 32, 22 2, 0,
y  1
2
x  y
x  1
2
x  y
B  1, 0, 0, 1,
1, 1T 5, 1 . T B
B  	
1
2
, 1
2
, 	 1
2
, 1
2
,
R2
x  32
22  y  12
12  x  32
4
 y  12
1  1,
x2  4y2  6x  8y  9  0.
Linear Differential Equations (Calculus)
In Exercises 1–8, determine which functions are solutions of the
linear differential equation.
1.
(a) (b) (c) (d)
2.
(a) x (b) (c) (d)
3.
(a) (b) (c) (d)
4.
(a) 1 (b) x (c) (d)
5.
(a) (b) (c) (d)
6.
(a) (b) (c) (d)
7.
(a) (b) (c) (d)
8.
(a) (b) (c) (d) y  xex y  x2
ex y  xex2
y  3ex2
y  2xy  0
y  xex y  2e2x y  2e2x y  xe2x
y  y  2y  0
y  xex y  xex y  1
x y  x
xy  2y  0
y  ex2
y  ex2
y  x2 y  1
x2
x2 y  2y  0
ex x2
y  2y  y  0
x  2e2x x2
e2x xe2x e2x
y  4y  4y  0
xex ex e x
y  3y  3y  y  0
e sin x cos x sin x  cos x x
y  y  0
SECTION 4.8 Exercises
Section 4.8 Applications of Vector Spaces 271
In Exercises 9–16, find the Wronskian for the set of functions.
9. 10.
11. 12.
13. 14.
15. 16.
In Exercises 17–24, test the given set of solutions for linear
independence.
Differential Equation Solutions
17.
18.
19.
20.
21.
22.
23.
24.
25. Find the general solution of the differential equation from
Exercise 17.
26. Find the general solution of the differential equation from
Exercise 18.
27. Find the general solution of the differential equation from
Exercise 20.
28. Find the general solution of the differential equation from
Exercise 24.
29. Prove that is the general solution of
30. Prove that the set is linearly independent if and only if
31. Prove that the set is linearly independent.
32. Prove that the set where is
linearly independent.
33. Writing Is the sum of two solutions of a nonhomogeneous linear differential equation also a solution? Explain your answer.
34. Writing Is the scalar multiple of a solution of a nonhomogeneous linear differential equation also a solution? Explain your
answer.
Conic Sections and Rotation
In Exercises 35–52, identify and sketch the graph.
35. 36.
37. 38.
39. 40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
In Exercises 53–62, perform a rotation of axes to eliminate the
-term, and sketch the graph of the conic.
53. 54.
55.
56.
57.
58.
59.
60.
61.
62.
In Exercises 63–66, perform a rotation of axes to eliminate the
-term, and sketch the graph of the “degenerate” conic.
63. 64.
65. 66. x2  10xy  y x 2  0 2  2xy  y2  1  0
5x2  2xy  5y x 2  0 2  2xy  y2  0
xy
7x2  23xy  5y2  16
x2  23xy  3y2  23x  2y  16  0
3x2  23xy  y2  2x  23 y  0
13x2  63xy  7y2  16  0
5x2  6xy  5y2  12  0
5x2  2xy  5y2  24  0
x2  2xy  y2  8x  8y  0
4x2  2xy  4y2  15  0
xy  1  0 xy  2  0
xy
y2  8x  6y  25  0
x2  4x  6y  2  0
4x2  y2  4x  2y  1  0
2x2  y2  4x  10y  22  0
4y2  4x2  24x  35  0
x2  4y2  4x  32y  64  0
4y2  2x2  4y  8x  15  0
9x2  y2  54x  10y  55  0
4x2  y2  8x  3  0
9x2  25y2  36x  50y  61  0
y2  6y  4x  21  0
x2  2x  8y  17  0
x2
16  y2
25  1 x2
9  y2
16  1  0
5x2  3y x 2  15  0 2  4y2  16  0
y y 2  8x  0 2  x  0
e b  0, ax cos bx, eax sin bx,
eax, xeax
a  b.
eax, ebx
y  a a  0. 2y  0,
y  C1 cos ax  C2 sin ax
1, x, ex
, xex y 2y y  0 
ex
, xex
, ex  xex y 3y  3y  y  0 
ex
, xex
, x2ex y 3y  3y  y  0 
y y  0 2, 1  2 sin x, 1  sin x
y y  0 1, sin x, cos x
e2x
, xe2x
, 2x  1e2x y 4y  4y  0 
e2x
, xe2x y  4y  4y  0 
y  y  0 sin x, cos x
x2, ex2
, x2ex 1, e  x
, e2x
x, ex
, ex e  x
, xex
, x  3ex
x, sin x, cos x x, sin x, cos x
ex2
, ex2
e  x
, ex
272 Chapter 4 Vector Spaces
67. Prove that a rotation of will eliminate the xy-term from
the equation
68. Prove that a rotation of where will eliminate the xy-term from the equation
69. For the equation define the matrix as
Prove that if then the graph of is
two intersecting lines.
70. For the equation in Exercise 69, define the matrix as
and describe the graph of ax2  bxy  cy2  0.




Section 4.1 mentioned that vectors in the plane can be characterized as directed line
segments having a certain length and direction. In this section, will be used as a model
for defining these and other geometric properties (such as distance and angle) of vectors in
In the next section, these ideas will be extended to general vector spaces.
You will begin by reviewing the definition of the length of a vector in If
is a vector in the plane, then the length, or magnitude, of denoted by is defined as
v  v1
2  v2
2
.
v, v,
v  v1, v2 R  2.
Rn.
R2
5.1
278 Chapter 5 Inner Product Spaces
This definition corresponds to the usual notion of length in Euclidean geometry. That is, the
vector is thought of as the hypotenuse of a right triangle whose sides have lengths of
and as shown in Figure 5.1. Applying the Pythagorean Theorem produces
Figure 5.1
Using as a model, the length of a vector in is defined as follows.
REMARK : The length of a vector is also called its norm. If then the vector is
called a unit vector.
This definition shows that the length of a vector cannot be negative. That is,
Moreover, if and only if is the zero vector v  0 v 0.
v ≥ 0.
v  1, v
Rn R2
⎥⎪v 2⎥⎪
⎥⎪v 1⎥⎪
⎥⎪v⎥⎪
⎥⎪v⎥⎪= v1
2 + v2
2
(v1, v2)
v2  v1
2  v2
2  v1
2  v2
2
.
v2, v v 1
The length, or magnitude, of a vector in is given by
v  v1
2  v2
2  . . .  vn
2
.
Rn v  v1, v2, . . . , vn Definition of Length
of a Vector in Rn
You can use a graphing utility or computer software program to find the length, or norm, of a vector.
For example, using a graphing utility, the length of the vector can be found and
may appear as follows.
Verify the length of in Example 1(a) on the next page. v
v  2, 1, 2 Technology
Note
Section 5.1 Length and Dot Product in R 279 n
(a) In the length of is
(b) In the length of is
Because its length is 1, is a unit vector, as shown in Figure 5.2.
Each vector in the standard basis for has length 1 and is called a standard unit
vector in In physics and engineering it is common to denote the standard unit vectors
in and as follows.
and
Two nonzero vectors and in are parallel if one is a scalar multiple of the other—
that is, Moreover, if then and have the same direction, and if
and have opposite directions. The next theorem gives a formula for finding the length
of a scalar multiple of a vector.
PROOF Because it follows that
 c v.
 cv1
2  v2
2  . . .  vn
2
 c2v1
2  v2
2  . . .  vn
2

 cv12  cv22  . . .  cvn2
cv  cv1, cv2, . . . , cvn
cv  cv1, cv2, . . . , cvn,
vu
u  cv. c > 0, vu c < 0,
Rn vu
i, j, k  1, 0, 0, 0, 1, 0, 0, 0, 1
i, j  1, 0, 0, 1
R3 R2
Rn.
Rn
v
v  	 2
17

2
 	 2
17

2
 	 3
17

2
 17
17  1.
R v  2
17, 2
17, 3
17 3
,
v  02  22  12  42  22  25  5.
R v  0, 2, 1, 4, 2 5
,
EXAMPLE 1 The Length of a Vector in Rn
Let be a vector in and let be a scalar. Then
where is the absolute value of c. c
cv  c v,
cRn THEOREM 5.1 v
Length of a
Scalar Multiple
Figure 5.2
v
v = , − , ( 17 17 17) 2 23
One important use of Theorem 5.1 is in finding a unit vector having the same direction
as a given vector. The theorem below provides a procedure for doing this.
PROOF Because is nonzero, you know is positive, and you can write as a
positive scalar multiple of
So, it follows that has the same direction as Finally, has length 1 because
The process of finding the unit vector in the direction of is called normalizing the
vector This procedure is demonstrated in the next example.
Find the unit vector in the direction of and verify that this vector has length 1.
SOLUTION The unit vector in the direction of is
which is a unit vector because
(See Figure 5.3.)
	 3
14

2
 	 1
14

2
 	 2
14

2
 14
14  1.
 	 3
14,  1
14, 2
14
,
 1
14 3, 1, 2
v
v  3, 1, 2
32  12  22
v
v  3, 1, 2,
EXAMPLE 2 Finding a Unit Vector
v.
v
u   v
v   1
v
v  1.
u . uv
u  	
1
v
v
v.
v v  0. 1
v u
280 Chapter 5 Inner Product Spaces
If is a nonzero vector in then the vector
has length 1 and has the same direction as . This vector is called the unit vector in the
direction of v.
v u
u  v
v
Rn THEOREM 5.2 v ,
Unit Vector in the
Direction of v
Figure 5.3
z
2
−4
4
4
2
x
y
(3, −1, 2)
, , 213 ( 14 14 14)
v
v
−
v
Section 5.1 Length and Dot Product in R 281 n
Distance Between Two Vectors in Rn
To define the distance between two vectors in will be used as the model. The
Distance Formula from analytic geometry tells you that the distance between two points
in the plane, and is
In vector terminology, this distance can be viewed as the length of where
and as shown in Figure 5.4. That is,
which leads to the next definition.
Figure 5.4
v u
(v1, v 2) (u1, u2 d(u, v) )
d(u, v) =⎟⎟ u − v⎟⎟ = (u1 − v1) 2 + (u2 − v2) 2
u  v  u1  v12  u2  v22
,
v  v1, v2 u , 1, u2
u  v, u
d  u1  v12  u2  v22.
v1, v2 u , 1, u2
d
R2 Rn,
You can use a graphing utility or computer software program to find the unit vector for a given
vector. For example, you can use a graphing utility to find the unit vector for , which
may appear as:
v  3, 4 Technology
Note
HISTORICAL NOTE
Olga Taussky-Todd
(1906–1995)
became interested in mathematics at an early age. She heavily
studied algebraic number theory
and wrote a paper on the sum
of squares, which earned her
the Ford Prize from the
Mathematical Association of
America. To read about her work,
visit college.hmco.com/pic/
larsonELA6e.
282 Chapter 5 Inner Product Spaces
You can easily verify the three properties of distance listed below.
1.
2. if and only if
3.
The distance between and is
Dot Product and the Angle Between Two Vectors
To find the angle between two nonzero vectors and
in the Law of Cosines can be applied to the triangle shown in Figure 5.5 to obtain
Expanding and solving for yields
The numerator of the quotient above is defined as the dot product of and and is
denoted by
.
This definition is generalized to as follows.
REMARK : Notice that the dot product of two vectors is a scalar, not another vector.
Rn
u  v  u1v1  u2v2
vu
cos   u1v1  u2v2
u v .
cos 
v  u2  u2  v2  2u v cos .
R2,
v  v1, v2 u  u  1, u2  0    
 22  22  12  3.
du, v  u  v  0  2, 2  0, 2  1
u  0, 2, 2 v  2, 0, 1
EXAMPLE 3 Finding the Distance Between Two Vectors
du, v  dv, u
du, v  0 u  v.
du, v 
 0
The distance between two vectors and in is
du, v  u  v.
Rn Definition of Distance vu
Between Two Vectors
Figure 5.5
||u||
||v||
||v − u||
θ
Angle Between Two Vectors
The dot product of and is the scalar quantity
u  v  u1v1  u2v2  . . .  unvn.
v  v1, v2 , . . . , vn u  u  1, u2, . . . , un Definition of
Dot Product in Rn
Section 5.1 Length and Dot Product in R 283 n
The dot product of and is
PROOF The proofs of these properties follow easily from the definition of dot product. For example,
to prove the first property, you can write
In Section 4.1, was defined as the set of all ordered -tuples of real numbers. When
is combined with the standard operations of vector addition, scalar multiplication, vector
length, and the dot product, the resulting vector space is called Euclidean -space. In the
remainder of this text, unless stated otherwise, you may assume to have the standard
Euclidean operations.
Rn
n
Rn
R n n
 v  u.
 v1u1  v2u2  . . .  vnun
u  v  u1v1  u2v2  . . .  unvn
u  v  13  22  04  32  7.
u  1, 2, 0, 3 v  3, 2, 4, 2
EXAMPLE 4 Finding the Dot Product of Two Vectors
You can use a graphing utility or computer software program to find the dot product of two vectors. Using
a graphing utility, you can verify Example 4, and it may appear as follows.
Keystrokes and programming syntax for these utilities/programs applicable to Example 4 are provided in
the Online Technology Guide, available at college.hmco.com/pic/larsonELA6e.
Technology
Note
If , and are vectors in and is a scalar, then the following properties are true.
1.
2.
3.
4.
5. and if and only if v  v 
 0, v  v  0 v  0.
v  v  v2
cu  v  cu  v  u  cv
u  v  w  u  v  u  w
u  v  v  u
cR wv n THEOREM 5.3 u ,
Properties of the
Dot Product
284 Chapter 5 Inner Product Spaces
Given and find
(a) (b) (c) (d) (e)
SOLUTION (a) By definition, you have
(b) Using the result in part (a), you have
(c) By Property 3 of Theorem 5.3, you have
(d) By Property 4 of Theorem 5.3, you have
(e) Because you have
Consequently,
Provided with two vectors and in such that and
evaluate
SOLUTION Using Theorem 5.3, rewrite the dot product as
 339  73  279  254.
 3u  u  7u  v  2v  v
 3u  u  u  v  6v  u  2v  v
 u  3u  u  v  2v  3u  2v  v
u  2v  3u  v  u  3u  v  2v  3u  v
v  v  79, u  2v  3u  v.
R u  u  39, u  v  3, n vu
EXAMPLE 6 Using Properties of the Dot Product
u  v  2w  213  22  26  4  22.
v  2w  5  8, 8  6  13, 2.
2w  8, 6,
w2  w  w  44  33  25.
u  2v  2u  v  26  12.
u  vw  6w  64, 3  24, 18.
u  v  25  28  6.
w u  v  2w. 2 u  v. u  vw. u  2v. .
u  2, 2, v  5, 8, w  4, 3,
EXAMPLE 5 Finding Dot Products
How does the dot product of two vectors compare with the product of their lengths? For instance,
let and . Calculate and Repeat this experiment with other
choices for and . Formulate a conjecture about the relationship between and vu u  v u v.
u  1, 1 v  4, 3 u  v u v.
Discovery
Section 5.1 Length and Dot Product in R 285 n
To define the angle between two vectors and in you can use the formula in
For such a definition to make sense, however, the value of the right-hand side of this
formula cannot exceed 1 in absolute value. This fact comes from a famous theorem named
after the French mathematician Augustin-Louis Cauchy (1789–1857) and the German
mathematician Hermann Schwarz (1843–1921).
PROOF Case 1. If then it follows that
and
So, the theorem is true if
Case 2. If let be any real number and consider the vector Because
it follows that
Now, let and to obtain the quadratic inequality
Because this quadratic is never negative, it has either no real roots or a
single repeated real root. But by the Quadratic Formula, this implies that the discriminant,
is less than or equal to zero.
Taking the square root of both sides produces
Verify the Cauchy-Schwarz Inequality for u  1, 1, 3 and v  2, 0, 1.
EXAMPLE 7 An Example of the Cauchy-Schwarz Inequality
u  v  u  u v  v  u v.
u  v2  u  uv  v
 4u  v2  4u  uv  v
b2  4ac
b2  4ac  0
b2  4ac,
at 2  bt  c 
 0.
a  u  u, b  2u  v, c  v  v
tu  v  tu  v  t 2u  u  2tu  v  v  v 
 0.
t u  v  t u  v 
 0,
u  0, t tu  v.
u  0.
u v  0v  0. u  v  0  v  0
u  0,
cos   u  v
u v
.
R2 Rn  vu ,
If and are vectors in then
where denotes the absolute value of u  v. u  v
u  v  u v,
Rn THEOREM 5.4 vu ,
The Cauchy-Schwarz
Inequality
286 Chapter 5 Inner Product Spaces
SOLUTION Because and you have
and
The inequality holds, and you have
The Cauchy-Schwarz Inequality leads to the definition of the angle between two nonzero
vectors in
REMARK : The angle between the zero vector and another vector is not defined.
The angle between and is
Consequently, It makes sense that and should have opposite directions, because
Note that because and are always positive, and will always have the
same sign. Moreover, because the cosine is positive in the first quadrant and negative in the
second quadrant, the sign of the dot product of two vectors can be used to determine
whether the angle between them is acute or obtuse, as shown in Figure 5.6.
u v u  v cos 
u  2v.
  . vu
cos   u  v
u v
 12
24 6   12
144  1.
u  4, 0, 2, 2 v  2, 0, 1, 1
EXAMPLE 8 Finding the Angle Between Two Vectors
Rn
.
u  v  u v.
u v  u  u v  v  115  55.
u  v  1  1
u  v  1, u  u  11, v  v  5,
The angle between two nonzero vectors in is given by
cos   0    . u  v
u v
,
Rn Definition of the Angle 
Between Two Vectors in Rn
Figure 5.6
θ
θ
θ
= π
cos = −1
u v
Opposite
direction
u
v
θ
θ
π π
θ
<<
cos < 0
2
u • v < 0
u
v
θ
θ
π
θ
=
cos = 0
2
u • v = 0
u
v
θ
θ
π
θ
<<
cos > 0
0 2
u • v > 0
u
v
θ
θ
direction
= 0
cos = 1
Same
Section 5.1 Length and Dot Product in R 287 n
Figure 5.6 shows that two nonzero vectors meet at a right angle if and only if their dot
product is zero. Two such vectors are said to be orthogonal (or perpendicular).
REMARK : Even though the angle between the zero vector and another vector is not
defined, it is convenient to extend the definition of orthogonality to include the zero vector.
In other words, the vector is said to be orthogonal to every vector.
(a) The vectors and are orthogonal because
(b) The vectors and are orthogonal because
Determine all vectors in that are orthogonal to
SOLUTION Let be orthogonal to Then
which implies that and So, every vector that is orthogonal to
is of the form
where is a real number. (See Figure 5.7.)
The Cauchy-Schwarz Inequality can be used to prove another well-known inequality
called the Triangle Inequality (Theorem 5.5, page 288). The name “Triangle Inequality”
is derived from the interpretation of the theorem in illustrated for the vectors and
in Figure 5.8(a). If you consider and to be the lengths of two sides of a triangle,
you can see that the length of the third side is Moreover, because the length
of any side of a triangle cannot be greater than the sum of the lengths of the other two sides,
you have
Figure 5.8(b) illustrates the Triangle Inequality for the vectors and in R3 vu .
u  v  u  v.
u  v.
u v
R vu 2
,
t
v  t, 2t  t1, 2,
v 4, 2 2  2v1 2v . 2  4v1
u  v  4, 2  v1, v2  4v1  2v2  0,
v  v u. 1, v2
R u  4, 2. 2
EXAMPLE 10 Finding Orthogonal Vectors
u  v  31  21  11  40  0.
u  3, 2, 1, 4 v  1, 1, 1, 0
u  v  10  01  00  0.
u  1, 0, 0 v  0, 1, 0
EXAMPLE 9 Orthogonal Vectors in Rn
0
Two vectors and in are orthogonal if
u  v  0.
Rn Definition of vu
Orthogonal Vectors
Figure 5.7
u
x
(4, 2)
1
2
y
1234
−1
−2
v = (1, −2)
288 Chapter 5 Inner Product Spaces
(a) (b)
Figure 5.8
These results can be generalized to in the following theorem.
PROOF Using the properties of the dot product, you have
Now, by the Cauchy-Schwarz Inequality, and you can write
Because both and are nonnegative, taking the square root of both
sides yields
REMARK : Equality occurs in the Triangle Inequality if and only if the vectors and
have the same direction. (See Exercise 117.)
vu
u  v ≤ u  v.
u  v u  v
 u  v2
.
 u2  2u v  v2
u  v2  u2  2u  v  v2
u  v  u v,
 u2  2u  v  v2
.
 u2  2u  v  v2
 u  u  2u  v  v  v
 u  u  v  v  u  v
u  v2  u  v  u  v
Rn
x y
z
⏐ ⏐ u + v⏐ ⏐
⏐ ⏐ u⏐ ⏐
⏐ ⏐ v⏐ ⏐
⏐ ⏐ v⏐ ⏐
⏐ ⏐ u + v⏐ ⏐ ⏐ < + ⏐ u⏐ ⏐ ⏐ ⏐ v⏐ ⏐
⏐ ⏐ u + v⏐ ⏐
⏐ ⏐ u + v⏐ ⏐
⏐ ⏐ u⏐ ⏐
⏐ ⏐ u⏐ ⏐
⏐ ⏐ v⏐ ⏐
⏐ ⏐ v⏐ ⏐
⏐ ⏐ v⏐ ⏐
x
< +
y
If and are vectors in then
u  v  u  v.
Rn THEOREM 5.5 vu ,
The Triangle Inequality
Section 5.1 Length and Dot Product in R 289 n
From the proof of the Triangle Inequality, you have
If and are orthogonal, then and you have the extension of the Pythagorean
Theorem to shown below.
This relationship is illustrated graphically for and in Figure 5.9.
The Dot Product and Matrix Multiplication
It is often useful to represent a vector in as an column
matrix. In this notation, the dot product of two vectors
and
can be represented as the matrix product of the transpose of multiplied by .
For example, the dot product of the vectors
and
is
In this light, many of the properties of the dot product are direct consequences of the
corresponding properties of matrix multiplication.
u  v  uTv  1 2 1 
3
2
4
  13  22  14  5.
v   3
2
4
 u   1
2
1

u  v  uTv  u1 u2 . . . un

v1
v2
.
.
.
vn
  u1v1  u2v2  . . .  unvn
u v
v

v1
v2
.
.
.
vn
 u

u1
u2
.
.
.
un

R n  1 n u  u1, u2, . . . , un
R3 R2
Rn
vu u  v  0,
u  v2  u2  2u  v  v2.
If and are vectors in then and are orthogonal if and only if
u  v2  u2  v2
.
R vu n THEOREM 5.6 vu ,
The Pythagorean Theorem
Figure 5.9
⏐ ⏐ u + v⏐ ⏐
⏐ ⏐ u⏐ ⏐
⏐ ⏐ u⏐ ⏐
⏐ ⏐ v⏐ ⏐
x
y
x y
z
⏐ ⏐ u + v⏐ ⏐
⏐ ⏐ u⏐ ⏐
⏐ ⏐ v⏐ ⏐
⏐ ⏐ v⏐ ⏐
290 Chapter 5 Inner Product Spaces
SECTION 5.1 Exercises
In Exercises 1–6, find the length of the vector.
1. 2.
3. 4.
5. 6.
In Exercises 7–12, find (a) (b) and (c)
7.
8.
9.
10.
11.
12.
In Exercises 13–18, find a unit vector (a) in the direction of and
(b) in the direction opposite that of .
13. 14.
15. 16.
17. 18.
19. For what values of c is
20. For what values of c is
In Exercises 21–26, find the vector with the given length that has
the same direction as the vector .
21. 22.
23. 24.
25.
26.
27. Given the vector find such that
(a) has the same direction as and one-half its length.
(b) has the direction opposite that of and one-fourth its
length.
(c) has the direction opposite that of and twice its length.
28. Given the vector find such that
(a) has the same direction as and one-half its length.
(b) has the direction opposite that of and one-fourth its
length.
(c) has the direction opposite that of and twice its length.
In Exercises 29–34, find the distance between and .
29.
30.
31.
32.
33.
34.
In Exercises 35– 40, find (a) (b) (c) (d)
and (e)
35.
36.
37.
38.
39.
40.
41. Find given that
and
42. Find given that
and
In Exercises 43–58, use a graphing utility or computer software
program with vector capabilities to find (a)–(f).
(a) Norm of and
(b) A unit vector in the direction of
(c) A unit vector in the direction opposite that of
(d)
(e)
(f)
43.
44.
45.
46.
47.
48.
49.
50.
51.
52. v  0, 1
4, 1
2 u  1,  1
2, 1
4,
v  0, 1
4,
1
5 u  1,  1
8,
2
5,
u  3, 0, 4, v  3, 4, 0
u  0, 5, 12, v  0, 5, 12
u  9, 12, v  7, 24
u  10, 24, v  5, 12
u  3, 4, v  4, 3
u  5, 12, v  12, 5
u  3, 4, v  5, 12
u  5, 12, v  8, 15
v  v
u  u
u  v
u
v
vu
v  v  6.
3u  v  u  3v, u  u  8, u  v  7,
v  v  10.
u  v  2u  v, u  u  4, u  v  5,
u  0, 4, 3, 4, 4, v  6, 8, 3, 3, 5
u  4, 0, 3, 5, v  0, 2, 5, 4
u  2, 1, 1, v  0, 2, 1
u  1, 1, 2, v  1, 3, 2
u  1, 2, v  2, 2
u  3, 4, v  2, 3
u  5v.
u u  vv, 2 u  v, u  u, ,
u  0, 1, 1, 2, v  1, 1, 2, 2
u  0, 1, 2, 3, v  1, 0, 4, 1
u  1, 2, 0, v  1, 4, 1
u  1, 1, 2, v  1, 3, 0
u  3, 4, v  7, 1
u  1, 1, v  1, 1
vu
u v
u v
u v
v  1, 3, 0, 4, u
u v
u v
u v
v  8, 8, 6, u
v  2, u  1, 1, 4, 0
v  3, u  0, 2, 1, 1
v  2, u  3, 3, 0 v  4, u  1, 2, 1
v  4, u  1, 1 v  4, u  1, 1
u
v
 c2, 2, 1  3?
 c1, 2, 3  1?
u  1, 0, 2, 2 u  1, 1, 2, 0
u  3, 2, 5 u  1, 3, 4
u  5, 12 u  1, 1
u
u
u  1, 0, 0, 0, v  0, 1, 0, 0
u  0, 1, 1, 2, v  1, 1, 3, 0
u  1, 2, 1, v  0, 2, 2
u  0, 4, 3, v  1, 2, 1
v  2, 1
2 u  1,  1
2,
v  4, 1
8 u  1,  1
4,
u, v, u  v.
v  2, 0, 5, 5 v  2, 4, 5, 1, 1
v  1, 2, 2 v  2, 0, 6
v  4, 3 v  0, 1
Section 5.1 Length and Dot Product in R 291 n
53.
54.
55.
56.
57.
58.
In Exercises 59–62, verify the Cauchy-Schwarz Inequality for the
given vectors.
59.
60.
61.
62.
In Exercises 63–72, find the angle between the vectors.
63.
64.
65.
66.
67.
68.
69.
70.
71.
72.
In Exercises 73–80, determine all vectors that are orthogonal to .
73. 74.
75. 76.
77. 78.
79. 80.
In Exercises 81–88, determine whether and are orthogonal,
parallel, or neither.
81.
82.
83.
84.
85.
86.
87.
88.
In Exercises 89–92, use a graphing utility or computer software
program with vector capabilities to determine whether and are
orthogonal, parallel, or neither.
89.
90.
91.
92.
Writing In Exercises 93 and 94, determine if the vectors are
orthogonal, parallel, or neither. Then explain your reasoning.
93.
94.
True or False? In Exercises 95 and 96, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
95. (a) The length or norm of a vector is
(b) The dot product of two vectors and is another vector
represented by
96. (a) If is a nonzero vector in the unit vector is
(b) If then the angle between and is acute.
Writing In Exercises 97 and 98, write a short paragraph explaining
why each expression is meaningless. Assume that and are
vectors in and that is a scalar.
97. (a) (b)
98. (a) u  v  u (b) c  u  v
u  v u  u  v
R c n
,
vu
u  v < 0,  vu
u  v
v.
Rn v ,
u  v  u1v1, u2v2, u3v3, . . . , unvn .
vu
v  v1  v2  v3  . . .  vn.
u  sin , cos , 1, v  sin , cos , 0
u  cos , sin , 1, v  sin , cos , 0
v  16
3 , 2, 4
3, 2
3 u    4
3,
8
3, 4, 32
3 ,
v 
3
8, 3
4, 9
8 u   , 3 3
4,
3
2, 9
2, 6,
v  0, 6, 21
2 , 9
2 u    21
2 ,
43
2 , 12, 3
2,
v 
3
2, 1, 5
2 u  2, , 0 1
2, 1, 3,
vu
v  2, 3
4, 1
2, 1
4 u  4,  3
2, 1, 1
2,
v 
1
4, 5
4 u  2, 5, 1, 0, , 0, 1
u  0, 1, 6, v  1, 2, 1
u  0, 1, 0, v  1, 2, 0
u  1, 1, v  0, 1
u   v  2, 4 1
3,
2
3,
v   1
2, 2
3 u  4, 3,
v   3
2, 1
6 u  2, 18,
vu
u  0, 1, 0, 0, 0 u  0, 0, 1, 0
u  4, 1, 0 u  2, 1, 1
u  3, 2 u  0, 0
u  0, 5 u  2, 7
v u
u  1, 1, 1, 0, 1, v  1, 0, 1, 0, 1
u  1, 3, 1, 2, 0, v  1, 4, 5, 3, 2
u  1, 1, 0, 1, v  1, 2, 1, 0
u  0, 1, 0, 1, v  3, 3, 3, 3
u  2, 3, 1, v  3, 2, 0
u  1, 1, 1, v  2, 1, 1
v  	cos 
4
, sin 
4 
 u  	cos 
3
, sin 
3 
,
v  	cos 3
4 , sin
3
4 
 u  	cos 
6
, sin 
6 
,
u  2, 1, v  2, 0
u  3, 1, v  2, 4

u  1, 1, 0, v  0, 1, 1
u  1, 1, 2, v  1, 3, 2
u  1, 0, v  1, 1
u  3, 4, v  2, 3
v  1, 2, 0, 1, 2, 2, 1, 0
u  3, 1, 2, 1, 0, 1, 2, 1,
v  1, 0, 1, 2, 2, 1, 1, 2
u  1, 1, 2, 1, 1, 1, 2, 1,
u  1, 2, 3, 2, 1, 3, v  1, 0, 2, 1, 2, 3
u  0, 2, 2, 1, 1, 2, v  2, 0, 1, 1, 2, 2
u  1, 3, 2, v  2, 1, 2
u  0, 1, 2, v  1, 2, 1
292 Chapter 5 Inner Product Spaces
In Exercises 99–102, verify the Triangle Inequality for the vectors
and .
99. 100.
101.
102.
In Exercises 103–106, verify the Pythagorean Theorem for the
vectors and .
103.
104.
105.
106.
107. Writing Explain what is known about the angle between
and , if
(a) (b) (c)
In Exercises 108 and 109, let be a vector in Show
that is orthogonal to , and use this fact to find two unit
vectors orthogonal to the given vector.
108. 109.
110. Find the angle between the diagonal of a cube and one of its
edges.
111. Find the angle between the diagonal of a cube and the diagonal
of one of its sides.
112. Prove that if , , and are vectors in then
113. Guided Proof Prove that if is orthogonal to and , then
is orthogonal to for any scalars and
Getting Started: To prove that is orthogonal to
you need to show that the dot product of and is 0.
(i) Rewrite the dot product of and as a linear
combination of and using Properties 2
and 3 of Theorem 5.3.
(ii) Use the fact that is orthogonal to v and w, and the
result of part (i) to lead to the conclusion that is
orthogonal to
114. Prove that if and are vectors in then
115. Prove that if and are vectors in then
116. Prove that the vectors and
are orthogonal unit vectors for any value of
Graph and for
117. Prove that if and only if and have
the same direction.
118. Writing The vector gives the numbers of bushels of corn, oats, and wheat raised by a farmer in a
certain year. The vector gives the prices
in dollars per bushel of the three crops. Find the dot product
and explain what information it gives.
119. Writing Let be a solution to the homogeneous linear
system of equations Explain why is orthogonal to
the row vectors of
120. The vector gives the numbers of units of
two models of mountain bikes produced by a company. The
vector gives the prices in dollars of the two
models, respectively. Find the dot product and explain
what information it gives.
121. Use the matrix multiplication interpretation of the dot
product, to prove the first three properties of
Theorem 5.3.
u  v  uTv,
u  v
v  225, 275
u  1245, 2600
A.
Ax  0. x
x m  n
u  v,
v  2.22, 1.85, 3.25
u  3240, 1450, 2235
u  v  u  v vu
. vu   
3.
v  sin , cos 
u  cos , sin 
u  v2  u  v2  2u2  2v2.
Rn vu ,
u  v  1
4 u  v2  1
4 u  v2.
Rn vu ,
cv  dw.
u
u
u  v u  w
u cv  dw
u cv  dw
u cv  dw,
u cv  dw c d.
u wv
u  w  v  w.
wvu Rn
, u  v  w
v  12, 5 v  8, 15
v v 2, v1
R2 v  v . 1, v2
u  v  0. u  v > 0. u  v < 0.
v
, u
u  4, 1, 5, v  2, 3, 1
u  3, 4, 2, v  4, 3, 0
u  3, 2, v  4, 6
u  1, 1, v  1, 1
vu
u  1, 1, 0, v  0, 1, 2
u  1, 1, v  2, 0
u  4, 0, v  1, 1 u  1, 1, 1, v  0, 1, 2
v
u
Inner Product Spaces
In Section 5.1, the concepts of length, distance, and angle were extended from to
This section extends these concepts one step further—to general vector spaces. This is
accomplished by using the notion of an inner product of two vectors.
You already have one example of an inner product: the dot product in The dot
product, called the Euclidean inner product, is only one of several inner products that can
be defined on To distinguish between the standard inner product and other possible
inner products, use the following notation.
Rn.
Rn.
Rn R . 2
5.2
dot product Euclidean inner product for
general inner product for vector space
A general inner product is defined in much the same way that a general vector space
is defined—that is, in order for a function to qualify as an inner product, it must satisfy a
set of axioms. The axioms parallel Properties 1, 2, 3, and 5 of the dot product given in
Theorem 5.3.
REMARK : A vector space with an inner product is called an inner product space.
Whenever an inner product space is referred to, assume that the set of scalars is the set of
real numbers.
Show that the dot product in satisfies the four axioms of an inner product.
SOLUTION In the dot product of two vectors and is
By Theorem 5.3, you know that this dot product satisfies the required four axioms, which
verifies that it is an inner product on
The Euclidean inner product is not the only inner product that can be defined on
A different inner product is illustrated in Example 2. To show that a function is an inner
product, you must show that the four inner product axioms are satisfied.
Show that the following function defines an inner product on where and
SOLUTION 1. Because the product of real numbers is commutative,
u, v  u1v1  2u2v2  v1u1  2v2u2  v, u.
u, v  u1v1  2u2v2
v  v1, v2.
u  u1, u2 R  2
,
EXAMPLE 2 A Different Inner Product for R2
Rn.
Rn
.
u  v  u1v1  u2v2  . . .  unvn.
v  v1, v2, . . . , vn u  u  1, u2, . . . , un R  n,
Rn
EXAMPLE 1 The Euclidean Inner Product for Rn
V
u, v  V
Rn u  v  
Section 5.2 Inner Product Spaces 293
Let , , and be vectors in a vector space and let be any scalar. An inner product
on is a function that associates a real number with each pair of vectors and
and satisfies the following axioms.
1.
2.
3.
4. and if and only if v, v 
 0, v, v  0 v  0.
cu, v  cu, v
u, v  w  u, v  u, w
u, v  v, u
V u, v vu
Definition of wvu , cV
Inner Product
294 Chapter 5 Inner Product Spaces
2. Let Then
3. If is any scalar, then
4. Because the square of a real number is nonnegative,
Moreover, this expression is equal to zero if and only if (that is, if and only if
Example 2 can be generalized to show that
is an inner product on The positive constants are called weights. If any
is negative or 0, then this function does not define an inner product.
Show that the following function is not an inner product on where and
SOLUTION Observe that Axiom 4 is not satisfied. For example, let Then
which is less than zero.
Let
and
be matrices in the vector space The function
is an inner product on The verification of the four inner product axioms is left to you. M2,2.
A, B  a11b11  a21b21  a12b12  a22b22
M2,2.
B  
b11
b21
b12
b22
A    a11
a21
a12
a22 
EXAMPLE 4 An Inner Product on M2,2
11  222  11  6,
v  1, 2, 1. v, v
u, v  u1v1  2u2v2  u3v3
v  v1, v2, v3.
u  u1, u2, u3 R  3
,
EXAMPLE 3 A Function That Is Not an Inner Product
ci c1, . . . , c R n n
.
c u, v  c1u1v1  c2u2v2  i > 0 . . .  cnunvn,
v1  v2  0.
v  0
v, v  v1
2  2v2
2 
 0.
cu, v  cu1v1  2u2v2  cu1v1  2cu2v2  cu, v.
c
 u, v  u, w.
 u1v1  2u2v2  u1w1  2u2w2
 u1v1  u1w1  2u2v2  2u2w2
u, v  w  u1v1  w1  2u2v2  w2
w  w1, w2.
Section 5.2 Inner Product Spaces 295
You obtain the inner product described in the next example from calculus. The verification of the inner product properties depends on the properties of the definite integral.
Let and be real-valued continuous functions in the vector space Show that
defines an inner product on
SOLUTION You can use familiar properties from calculus to verify the four parts of the definition.
1.
2.
3.
4. Because for all you know from calculus that
with
if and only if is the zero function in or if
The next theorem lists some easily verified properties of inner products.
f Ca, b, a  b.
 f, f   
b
a
 fx2 dx  0
 f, f   
b
a
 fx 2 dx 
 0
 fx x, 2 
 0
c f , g  c
b
a
fxgx dx  
b
a
cfxgx dx  cf, g
    f, g   f, h
b
a
fxgx dx  
b
a
fxhx dx
 
b
a
 f, g  h    fxgx  fxhx dx
b
a
fxgx  hx dx
 f, g  
b
a
fxgx dx  
b
a
gxfx dx  g, f 
Ca, b.
 f, g  
b
a
fxgx dx
gf Ca, b.
EXAMPLE 5 An Inner Product Defined by a Definite Integral (Calculus)
Let , , and be vectors in an inner product space and let be any real number.
1.
2.
3. u, cv  cu, v
u  v, w  u, w  v, w
0, v  v, 0  0
THEOREM 5.7 wvu , cV
Properties of
Inner Products
296 Chapter 5 Inner Product Spaces
PROOF The proof of the first property follows. The proofs of the other two properties are left as
exercises. (See Exercises 85 and 86.) From the definition of an inner product, you know
so you only need to show one of these to be zero. Using the fact that
The definitions of norm (or length), distance, and angle for general inner product spaces
closely parallel those for Euclidean -space. Note that the definition of the angle between
and presumes that
for a general inner product, which follows from the Cauchy-Schwarz Inequality shown later
in Theorem 5.8.
REMARK : If then is called a unit vector. Moreover, if is any nonzero vector
in an inner product space then the vector is a unit vector and is called the
unit vector in the direction of .
For polynomials and in the
vector space the function
is an inner product. Let and be
polynomials in and determine
(a) (b) (c) (d)  p, q. q, r. q. dp, q.
P2,
rx  x  2x2 px  1  2x2, qx  4  2x  x2
,
p, q  a0b0  a1b1  . . .  anbn
Pn,
q  b0  b1x  . . .  bn x n p  a0  a1x  . . .  an x n
EXAMPLE 6 Finding Inner Products
v
V, u  v
v
v  1, v v
1  u, v
u v  1
vu
n 
 0.
 0v, v
0, v  0v, v
0v  0,
0, v  v, 0,
Let and be vectors in an inner product space
1. The norm (or length) of is
2. The distance between and is
3. The angle between two nonzero vectors and is given by
4. and are vu orthogonal if u, v  0.
cos   0    . u, v
u v
,
vu
vu du, v  u  v.
u u  u, u.
Definitions of Norm, vu V.
Distance, and Angle
Section 5.2 Inner Product Spaces 297
SOLUTION (a) The inner product of and is
(b) The inner product of and is
(c) The norm of is
(d) The distance between and is
Notice that the vectors and are orthogonal.
Orthogonality depends on the particular inner product used. That is, two vectors may be
orthogonal with respect to one inner product but not to another. Try reworking Example 6
using the inner product With this inner product the only
orthogonal pair is and
Use the inner product defined in Example 5 and the functions and in
to find
(a) (b)
SOLUTION (a) Because you have
So,
(b) To find write
So, df, g  1
30.
 1
30  . 
x3
3  x4
2

x5
5 
1
0
 
1
0
x2  2x3  x4
 dx
 
1
0
x  x2
2   dx
1
0
fx  gx2 dx
d f, g2   f  g, f  g
df, g,
 f   1
3  . 
x3
3 
1
0
 1
3  f  . 2   f, f   
1
0
xx dx  
1
0
x2 dx
fx  x,
 f . df, g.
C0, 1
gx  x2 fx  x
EXAMPLE 7 Using the Inner Product on C[0, 1] (Calculus)
p q.
p, q  a0b0  a1b1  2a2b2.
rq
 32  22  32  22.
 3  2x  3x2 
dp, q   p  q   1  2x2
  4  2x  x2
qp
q  q, q  42  22  12  21.
q
q, r  40  21  12  0.
rq
 14  02  21  2.
p, q  a0b0  a1b1  a2b2
qp
298 Chapter 5 Inner Product Spaces
In Example 7, you found that the distance between the functions and
in is In practice, the actual distance between a pair of vectors is
not as useful as the relative distance between several pairs. For instance, the distance
between and in is 1. From Figure 5.10, this seems reasonable. That is, whatever norm is defined on it seems reasonable that you would want
to say that and are closer than and
Figure 5.10
The properties of length and distance listed for in the preceding section also hold for
general inner product spaces. For instance, if and are vectors in an inner product space,
then the following three properties are true.
vu
Rn
d(h, g) = 1
1 2
1
2
x
y
g(x) = x 2
h(x) = x 2 + 1
1
30
1 2
1
2
x
y
d(f, g) =
g(x) = x 2
f(x) = x
gf g h.
C0, 1,
hx  x C0, 1 gx  x 2  1 2
C0, 1 1
30  0.183.
gx  x2 fx  x
Many graphing utilities and computer software programs have built-in routines for approximating
definite integrals. For example, on some graphing utilities, you can use the fnInt command to verify
Example 7(b). It may look like:
The result should be approximately
Keystrokes and programming syntax for these utilities/programs applicable to Example 7(b) are
provided in the Online Technology Guide, available at college.hmco.com/pic/larsonELA6e.
0.182574  1
30.
Technology
Note
Section 5.2 Inner Product Spaces 299
Properties of Norm Properties of Distance
1. 1.
2. if and only if 2. if and only if
3. 3.
Theorem 5.8 lists the general inner product space versions of the Cauchy-Schwarz
Inequality, the Triangle Inequality, and the general Pythagorean Theorem.
The proofs of these three axioms parallel those for Theorems 5.4, 5.5, and 5.6. Simply
substitute for the Euclidean inner product
Let and be functions in the vector space with the inner product
defined in Example 5,
Verify that
SOLUTION For the left side of this inequality you have
For the right side of the inequality you have
and
So,
and  f, g  f  g  1    f  g. 	
1
3
  1
3  0.577,
g2  
1
0
gxgx dx  
1
0
x2 dx  x3
3 
1
0
 1
3
.
 f 2  
1
0
fxfx dx  
1
0
dx  x
1
0
 1
 f, g  
1
0
fxgx dx  
1
0
x dx  x2
2 
1
0
 1
2
.
 f, g   f  g.
 f, g  
b
a
fxgx dx.
fx  1 gx  x C0, 1,
EXAMPLE 8 An Example of the Cauchy-Schwarz Inequality (Calculus)
u, v u  v.
cu  du, v  dv, u c u
u  0 u  0. du, v  0 u  v.
u 
 0 du, v 
 0
Let and be vectors in an inner product space
1. Cauchy-Schwarz Inequality:
2. Triangle Inequality:
3. Pythagorean Theorem: and are orthogonal if and only if
u  v2  u2  v2
.
vu
u  v  u  v
u, v  u v
THEOREM 5.8 vu V.
300 Chapter 5 Inner Product Spaces
Orthogonal Projections in Inner Product Spaces
Let and be vectors in the plane. If is nonzero, then can be orthogonally projected
onto , as shown in Figure 5.11. This projection is denoted by Because is a
scalar multiple of , you can write
If as shown in Figure 5.11(a), then and the length of is
which implies that So,
If as shown in Figure 5.11(b), then it can be shown that the orthogonal projection
of onto is the same formula.
(a) (b)
Figure 5.11
In the orthogonal projection of onto is
as shown in Figure 5.12.
The notion of orthogonal projection extends naturally to a general inner product space.
 	
12
5 ,
16
5 
,
 20
253, 4
 projvu  u  v
v  v
 v  4, 2  3, 4
3, 4  3, 4
3, 4
R u  4, 2 v  3, 4 2,
EXAMPLE 9 Finding the Orthogonal Projection of u onto v
u
v
θ
projvu = av, a < 0
u
v θ
projvu = av, a > 0
vu
a < 0,
projvu  u  v
v  v
v.
a  u  v
v2  u  v
v  v.
av  a v  av  u cos   u v cos 
v  u  v
v ,
proj a > 0, cos  > 0 vu
projvu  av.
v
proj v projvu. vu
vu v u
u
(3, 4)
( ) ,
(4, 2)
v
3
1
4
2
1234
projvu
12
5
16
5
Figure 5.12
Section 5.2 Inner Product Spaces 301
REMARK : If is a unit vector, then and the formula for the orthogonal projection of onto takes the simpler form
Use the Euclidean inner product in to find the orthogonal projection of
onto
SOLUTION Because and the orthogonal projection of onto is
as shown in Figure 5.13.
REMARK : Notice in Example 10 that is
orthogonal to This is true in general: if and are nonzero vectors in an
inner product space, then is orthogonal to . (See Exercise 84.)
An important property of orthogonal projections used in approximation problems (see
Section 5.4) is shown in the next theorem. It states that, of all possible scalar multiples of
a vector , the orthogonal projection of onto is the one closest to , as shown in Figure
5.14. For instance, in Example 10, this theorem implies that, of all the scalar multiples of
the vector the vector is closest to You are
asked to prove this explicitly in Exercise 90.
proj u  6, 2, 4. v  1, 2, 0, vu  2, 4, 0
v vu u
u  proj v vu
v  1, 2, 0. vu
u  projvu  6, 2, 4  2, 4, 0  4, 2, 4
 2, 4, 0,
 21, 2, 0
 10
5 1, 2, 0
 projvu  u  v
v  v
 v
v vu u  v  10 2  v  v  5,
v  1, 2, 0.
R u  6, 2, 4 3
EXAMPLE 10 Finding an Orthogonal Projection in R3
projvu  u, vv.
vu
v, v  v v 2  1,
Let and be vectors in an inner product space such that Then the orthogonal projection of unto is given by
projvu  u, v
v, v
 v.
vu
Definition of vu V, v  0.
Orthogonal Projection
2 2
6
2
4
x
y
z
4
(6, 2, 4) u
(2, 4, 0)
projvu
v
(1, 2, 0)
Figure 5.13
302 Chapter 5 Inner Product Spaces
Figure 5.14
PROOF Let Then you can write
where and are orthogonal. You can verify this by using the inner product axioms to show that
Now, by the Pythagorean Theorem you can write
which implies that
Because and you know that So,
and it follows that
The next example discusses a type of orthogonal projection in the inner product space
Ca, b.
du, bv < du, cv.
u  bv2 < u  cv2,
b  c2v2 b  c v  0, > 0.
u  cv2  u  bv2  b  c2
v2
.
u  bv  b  cv2  u  bv2  b  cv2,
u  bv, b  cv  0.
u  bv b  cv
u  cv2  u  bv  b  cv2,
b  u, v
v, v.
u
v
cv
d(u, cv) u
v
d( , proj ) u uv
projvu
Let and be two vectors in an inner product space such that Then
c  u, v
v, v du, proj . vu < du, cv,
THEOREM 5.9 vu V, v  0.
Orthogonal Projection
and Distance
Section 5.2 Inner Product Spaces 303
Let and be functions in Use the inner product defined in
Example 5,
to find the orthogonal projection of onto
SOLUTION From Example 8 you know that
and
So, the orthogonal projection of onto is
 3
2
x.
 1
2
1
3
x
 projg f   f, g
g, g
g
gf
g2  g, g  1
3  f, g  . 1
2
f g.
 f, g  
a
b
fxgx dx,
fx  1 gx  x C0, 1.
EXAMPLE 11 Finding an Orthogonal Projection in C[a, b] (Calculus)
SECTION 5.2 Exercises
In Exercises 1–10, find (a) , (b) , (c) , and (d)
for the given inner product defined in
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
Calculus In Exercises 11–16, use the functions and in
to find (a) (b) (c) and (d) for the inner product
11.
12.
13.
14.
15.
16. gx  1  2x2 fx  1,
gx  3x fx  1, 2  1
gx  ex fx  x,
gx  ex fx  x,
gx  x fx  x, 2  x  2
gx  x fx  x 2  1 2,
 f, g  
1
1
fxgx dx.
 f, g, f , g , df, g
gf C1, 1
u, v  u  v
u  1, 1, 2, 0, v  2, 1, 0, 1,
u, v  u  v
u  2, 0, 1, 1, v  2, 2, 0, 1,
u, v  u1v1  2u2v2  u3v3
u  1, 1, 1, v  2, 5, 2,
u, v  2u1v1  3u2v2  u3v3
u  8, 0, 8, v  8, 3, 16,
u  0, 1, 2, v  1, 2, 0, u, v  u  v
u  0, 9, 4, v  9, 2, 4, u, v  u  v
u, v  u1v1  2u2v2 u  0, 6, v  1, 1,
u, v  3u1v1  u2v v  2 u  4, 3, 0, 5,
u  1, 1, v  7, 9, u, v  u  v
u  3, 4, v  5, 12, u, v  u  v
Rn
.
u, v u  v  du, v
304 Chapter 5 Inner Product Spaces
In Exercises 17–20, use the inner product
to find (a) (b) (c) and
(d) for the matrices in
17.
18.
19.
20.
In Exercises 21–24, use the inner product
to find (a) (b) (c) and (d) for
the polynomials in
21.
22.
23.
24.
In Exercises 25–28, prove that the indicated function is an inner
product.
25. as shown in Exercise 3
26. as shown in Exercise 7
27. as shown in Exercises 17 and 18
28. as shown in Exercises 21 and 23
Writing In Exercises 29–36, state why is not an inner
product for and in
29. 30.
31. 32.
33. 34.
35. 36.
In Exercises 37–46, find the angle between the vectors.
37.
38.
39.
40.
41.
42.
43.
44.
45. Calculus
46. Calculus
In Exercises 47–58, verify (a) the Cauchy-Schwarz Inequality and
(b) the Triangle Inequality.
47.
48.
49.
50.
51.
52.
53.
54.
55. Calculus
56. Calculus
57. Calculus
 f, g  
1
0
fxgx dx
gx  ex fx  x, ,
 f, g  
2
0
fxgx dx
fx  x, gx  cos x,
 f, g  


fxgx dx
fx  sin x, gx  cos x,
A, B  a11b11  a12b12  a21b21  a22b22
B  
1
2
1
2 A  , 
0
2
1
1,
A, B  a11b11  a12b12  a21b21  a22b22
B  
3
4
1
3 A  , 
0
2
3
1,
 p, q  a0b0  2a1b1  a2b2 qx  1  x2 px  x, ,
 p, q  a q 0b0  a1b1  a2b2 x  3x px  2x, 2  1,
u  1, 0, 2, v  1, 2, 0, u, v  u  v
u  1, 0, 4, v  5, 4, 1, u, v  u  v
u  1, 1, v  1, 1, u, v  u  v
u  5, 12, v  3, 4, u, v  u  v
 f, g  
1
1
fxgx dx
gx  x2 fx  1, ,
 f, g  
1
1
fxgx dx
gx  x2 fx  x, ,
 p, q  a0b0  2a1b1  a2b2
qx  x  x2 px  1  x , 2,
 p, q  a0b0  a1b1  a2b2
qx  1  x  x2 px  1  x  x , 2,
u  0, 1, 1, v  1, 2, 3, u, v  u  v
u, v  u1v1  2u2v2  u3v3
u  1, 1, 1, v  2, 2, 2,
u, v  2u1v1  u2v2 u   v  2, 1, 1
4, 1,
u, v  3u1v1  u2v v  2 u  4, 3, 0, 5,
v   u, v  u  v 1
2 u  2, 1, , 1,
u  3, 4, v  5, 12, u, v  u  v
u, v  u1u2  v1v2 u, v  3u1v2  u2v1
u, v  u 2
1 v 2
1  u 2
2 v 2
2 u, v  u 2
1 v 2
1  u 2
2 v 2
2
u, v  u1v1  2u2v2 u, v  u1v1  u2v2
u, v  u2v2 u, v  u1v1
R2 v  v . 1, v2 u  u  1, u2
u, v
 p, q
A, B
u, v
u, v
qx  x  x2 px  1  2x  x2,
qx  1  x2 px  1  x2
,
qx  1  2x2 px  1  x  1
2x2,
qx  x  x2 px  1  x  3x2,
P2.
a  p, q,  p, q, d p, q 1b1  a2b2
 p, q  a0b0 
B  
1
0
1
1
A    1
0
0
1,
B   0
2
1
0
A    1
2
1
4,
B  
0
1
1
0
A    1
0
0
1,
B  
0
1
2
1
A    1
4
3
2,
M2,2 dA,B .
a A, B, A, B, 12b12  a21b21  2a22b22
A, B  2a11b11 
Section 5.2 Inner Product Spaces 305
58. Calculus
Calculus In Exercises 59–62, show that and are orthogonal in
the inner product space with the inner product
59.
60.
61.
62.
In Exercises 63–66, (a) find (b) find and (c) sketch
a graph of both and
63.
64.
65.
66.
In Exercises 67–70, find (a) and (b)
67.
68.
69.
70.
Calculus In Exercises 71–78, find the orthogonal projection of
onto Use the inner product in
71.
72.
73.
74.
75.
76.
77.
78.
True or False? In Exercises 79 and 80, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
79. (a) The dot product is the only inner product that can be
defined in
(b) Of all the possible scalar multiples of a vector , the
orthogonal projection of onto is the vector closest to .
80. (a) The norm of the vector is defined as the angle between
the vector and the positive -axis.
(b) The angle between a vector and the projection of onto
is obtuse if the scalar and acute if where
81. Let and be vectors in with the inner
product
(a) Show that and are orthogonal.
(b) Sketch the vectors and . Are they orthogonal in the
Euclidean sense?
82. Prove that for any
vectors and in an inner product space
83. Prove that the function is an inner product for
84. Let and be nonzero vectors in an inner product space
Prove that is orthogonal to .
85. Prove Property 2 of Theorem 5.7: If , , and are vectors in
an inner product space, then
86. Prove Property 3 of Theorem 5.7: If and are vectors in an
inner product space and is a scalar, then
87. Guided Proof Let be a subspace of the inner product space
Prove that the set is a subspace of
for all
Getting Started: To prove that is a subspace of you
must show that is nonempty and that the closure conditions
for a subspace hold (Theorem 4.5).
(i) Find an obvious vector in to conclude that it is
nonempty.
(ii) To show the closure of under addition, you need to
show that for all and for any
Use the properties of inner products and
the fact that and are both zero to show
this.
v2 v , w 1, w
v1, v2 	 W.
v1  v2, w  0 w 	 W
W
W
W
W V, 
W  v 	 V : v, w  0 w 	 W
W V.  V.
W
c u, cv  cu, v.
vu
u  v, w  u, w  v, w.
wvu
u  proj v vu
vu V.
c u, v  c i > 0 1u1v1  c2u2v2  . . .  cnunvn,
Rn.
vu V.
u  v2  u  v2  2u2  2v2
vu
vu
u, v  u1v1  2u2v2.
R2 u  4, 2 v  2, 2
av  projvu.
v a < 0 a > 0,
 v u
u x
u
vu u
v
Rn
.
C, , fx  x, gx  cos 2x
C, , fx  x, gx  sin 2x
C, , fx  sin 2x, gx  cos 2x
C, , fx  sin x, gx  cos x
gx  ex C0, 1, fx  x,
gx  ex C0, 1, fx  x,
fx  x gx  2x  1 C1, 1, 3  x,
C1, 1, fx  x, gx  1
 f, g  
b
a
fxgx dx.
f g. Ca, b
u  1, 4, 2, 3, v  2, 1, 2, 1
u  0, 1, 3, 6, v  1, 1, 2, 2
u  1, 2, 1, v  1, 2, 1
u  1, 3, 2, v  0, 1, 1
proj proj uv. vu
u  2, 2, v  3, 1
u  1, 3, v  4, 4
u  1, 2, v  4, 2
u  1, 2, v  2, 1
proj proj uv. vu
proju proj v, vu,
C0, , fx  1, gx  cos2nx, n  1, 2, 3, . . .
gx  1
25x C1, 1, fx  x, 3  3x
gx  1
23x C1, 1, fx  x, 2  1
C, , fx  cos x, gx  sin x
 f, g  
b
a
fxgx dx.
Ca, b
gf
 f, g  
1
0
fxgx dx
gx  ex fx  x, ,
Orthonormal Bases: Gram-Schmidt Process
You saw in Section 4.7 that a vector space can have many different bases. While studying
that section, you should have noticed that certain bases are more convenient than others. For
example, has the convenient standard basis This set
is the standard basis for because it has special characteristics that are particularly useful.
One important characteristic is that the three vectors in the basis are mutually orthogonal.
That is,
A second important characteristic is that each vector in the basis is a unit vector.
This section identifies some advantages of bases consisting of mutually orthogonal unit
vectors and develops a procedure for constructing such bases, known as the Gram-Schmidt
orthonormalization process.
For this definition has the form shown below.
Orthogonal Orthonormal
1. 1.
2. v i  1, 2, . . . , n i  1,
v i  j i
, vj v   0, i
, vj
  0, i  j
S  v1, v2, . . . , vn
,
0, 1, 0  0, 0, 1  0.
1, 0, 0  0, 0, 1  0
1, 0, 0  0, 1, 0  0
R3
R B  1, 0, 0, 0, 1, 0, 0, 0, 1. 3
5.3
A set of vectors in an inner product space is called orthogonal if every pair of
vectors in is orthogonal. If, in addition, each vector in the set is a unit vector,
then is called S orthonormal.
S
Definitions of Orthogonal S V
and Orthonormal Sets
306 Chapter 5 Inner Product Spaces
(iii) To show closure under multiplication by a scalar,
proceed as in part (ii). You need to use the properties of
inner products and the condition of belonging to
88. Use the result of Exercise 87 to find if is a span of
(1, 2, 3) in
89. Guided Proof Let be the Euclidean inner product on
Use the fact that to prove that for any
matrix
(a) and (b)
Getting Started: To prove (a) and (b), you can make use of
both the properties of transposes (Theorem 2.6) and the properties of dot products (Theorem 5.3).
(i) To prove part (a), you can make repeated use of the
property and Property 4 of Theorem 2.6.
(ii) To prove part (b), you can make use of the property
Property 4 of Theorem 2.6, and Property
4 of Theorem 5.3.
90. The two vectors from Example 10 are and
. Without using Theorem 5.9, show that among all
the scalar multiples of the vector , the projection of
onto is the vector closest to —that is, show that
is a minimum.
du, proj v u vu
cv v u
v  1, 2, 0
u  6, 2, 4
u, v  uTv,
u, v  uTv
ATAu, u  Au2 A . Tu, v  u, Av
A
u, v  u n  n T R v n
.
u, v
V  R3.
 WW
W.
If is a basis, then it is called an orthogonal basis or an orthonormal basis, respectively.
The standard basis for is orthonormal, but it is not the only orthonormal basis for
For instance, a nonstandard orthonormal basis for can be formed by rotating the
standard basis about the -axis to form
as shown in Figure 5.15. Try verifying that the dot product of any two distinct vectors in
is zero, and that each vector in is a unit vector. Example 1 describes another nonstandard
orthonormal basis for
Show that the set is an orthonormal basis for
SOLUTION First show that the three vectors are mutually orthogonal.
Now, each vector is of length 1 because
So, is an orthonormal set. Because the three vectors do not lie in the same plane (see
Figure 5.16), you know that they span By Theorem 4.12, they form a (nonstandard)
orthonormal basis for R3.
R3.
S
v3   v3  v3  4
9  4
9  1
9  1.
v2   v2  v2   2
36  2
36  8
9  1
v1   v1  v1  1
2  1
2  0  1
v2  v3  2
9  2
9

22
9  0
v1  v3  2
32  2
32
 0  0
v1  v2  1
6

1
6
 0  0
	
2
3
, 2
3
,
1
3 	 
 2
6 ,
2
6 ,
22
3 
 S  , 	
1
2
, 1
2
, 0
,
v3 v2 v1
R3.
EXAMPLE 1 A Nonstandard Orthonormal Basis for R3
R3.
B
B
B  cos , sin , 0, sin , cos , 0, 0, 0, 1,
z
R3 Rn
.
Rn
S
Figure 5.15
k
j
i
x y
θ
z
v2
v3
Section 5.3 Orthonormal Bases: Gram-Schmidt Process 307
Figure 5.16
k
j
i
x y ,
− , , , − ,
, 0 1
3
1
2
2 2
( 2
( (
)
) )
z
2 2 2
3
2
3
1
3
v1
v2
v3
6 6
In with the inner product
the standard basis is orthonormal. The verification of this is left as an
exercise. (See Exercise 19.)
The orthogonal set in the next example is used to construct Fourier approximations of
continuous functions. (See Section 5.5.)
In with the inner product
show that the set is orthogonal.
SOLUTION To show that this set is orthogonal, you need to verify the inner products shown below, where
and are positive integers.
One of these products is verified, and the others are left to you. If then the
formula for rewriting a product of trigonometric functions as a sum can be used to obtain
 0.
 1
2 
cos(m  n)x
m  n  cos(m  n)x
m  n 
2
0

2
0
sin mx cos nx dx  1
2 
2
0
sinm  nx  sinm  nx dx
m  n,
cos mx, cos nx  
2
0
cos mx cos nx dx  0, m  n
sin mx, sin nx  
2
0
sin mx sin nx dx  0, m  n
sin mx, cos nx  
2
0
sin mx cos nx dx  0
1, cos nx  
2
0
cos nx dx  0
1, sin nx  
2
0
sin nx dx  0
nm
S  1, sin x, cos x, sin 2x, cos 2x, . . . , sin nx, cos nx
 f, g  
2
0
fxgx dx,
C[0, 2,
EXAMPLE 3 An Orthogonal Set in C[0, 2 ] (Calculus)
B  1, x, x2, x3
 p, q  a0b0  a1b1  a2b2  a3b3,
P3,
EXAMPLE 2 An Orthonormal Basis for P3
308 Chapter 5 Inner Product Spaces
HISTORICAL NOTE
Jean-Baptiste Joseph Fourier
(1768–1830)
is credited as a significant
contributor to the field of
education for scientists, mathematicians, and engineers. His
research led to important results
pertaining to eigenvalues,
differential equations, and
Fourier series (functions by
trigonometric series). His work
forced mathematicians of that
day to accept the definition of a
function, which at that time was
very narrow. To read about his
work, visit college.hmco.com/
pic/larsonELA6e.

If then
Note that Example 3 shows only that the set is orthogonal. This particular set is not
orthonormal. An orthonormal set can be formed, however, by normalizing each vector in
That is, because
it follows that the set
is orthonormal.
Each set in Examples 1, 2, and 3 is linearly independent. Linear independence is a
characteristic of any orthogonal set of nonzero vectors, as stated in the next theorem.
PROOF You need to show that the vector equation
implies To do this, form the inner product of the left side of the
equation with each vector in That is, for each
Now, because is orthogonal, for and now the equation reduces to
But because each vector in is nonzero, you know that
So every must be zero and the set must be linearly independent. ci
vi
, vi
  vi2  0.
S
ci
vi
, vi
  0.
v j  i, i , vj S   0
c1v1, vi
  c2v2, vi
  . . .  civi , vi
  . . .  cnvn, vi
  0.
c1v1  c2v2  . . .  ci
vi  . . .  cnvn, vi
  0, vi

S. i,
c1  c2  . . .  cn  0.
c1v1  c2v2  . . .  cnvn  0
 1
2
, 1

 sin x, 1

 cos x, . . . , 1

 sin nx, 1

 cos nx
cos nx2  
2
0
 cos2 nx dx  ,
sin nx2  
2
0
 sin2 nx dx  
12  
2
0
dx  2
S.
S
  0.
2
0
sin mx cos mx dx  1
2m sin2 mx
2
0
m  n,
Section 5.3 Orthonormal Bases: Gram-Schmidt Process 309
If is an orthogonal set of nonzero vectors in an inner product space
, then is linearly independent. SV
S  v1, v2, . . . , vn
 THEOREM 5.10
Orthogonal Sets Are
Linearly Independent
310 Chapter 5 Inner Product Spaces
As a consequence of Theorems 4.12 and 5.10, you have the result shown next.
Show that the following set is a basis for
SOLUTION The set has four nonzero vectors. By the corollary to Theorem 5.10, you can show that
is a basis for by showing that it is an orthogonal set, as follows.
is orthogonal, and by the corollary to Theorem 5.10, it is a basis for
Section 4.7 discussed a technique for finding a coordinate representation relative to a
nonstandard basis. If the basis is orthonormal, this procedure can be streamlined.
Before presenting this procedure, you will look at an example in Figure 5.17 shows
that and form an orthonormal basis for Any vector in can be
represented as where and Because and are
unit vectors, it follows that and Consequently,
which shows that the coefficients and are simply the dot products of with the
respective basis vectors. This is generalized in the next theorem.
c1 c2 w
 c1i  c2 j,
 w  ii  w  jj
w  w1  w2
w w2  w  jj. 1  w  ii
w ji 2  proj w j
w. 1  proj w  w1  w2, i
w
R w 2 R2 i  1, 0 j  0, 1 .
R2.
R4 S .
v3  v4  1  0  2  1  0
v2  v4  1  0  0  1  0
v2  v3  1  0  0  1  0
v1  v4  2  6  2  2  0
v1  v3  2  0  4  2  0
v1  v2  2  0  0  2  0
R4
S S
S  2, 3, 2, 2, 1, 0, 0, 1, 1, 0, 2, 1, 1, 2, 1, 1
v4 v3 v2 v1
R4
.
EXAMPLE 4 Using Orthogonality to Test for a Basis
If is an inner product space of dimension then any orthogonal set of nonzero
vectors is a basis for V.
COROLLARY TO V n, n
THEOREM 5.10
Figure 5.17
j
i
w = w1 + w2
w2 = projjw
w1 = proji
w
w = w1 + w2 = c1i + c2j
If is an orthonormal basis for an inner product space then the
coordinate representation of a vector with respect to is
w  w, v1v1  w, v2v2  . . .  w, vnvn.
w B
B  v V, 1, v2, . . . , vn THEOREM 5.11 
Coordinates Relative to an
Orthonormal Basis
Section 5.3 Orthonormal Bases: Gram-Schmidt Process 311
PROOF Because is a basis for there must exist unique scalars such that
Taking the inner product (with ) of both sides of this equation, you have
and by the orthogonality of this equation reduces to
Because is orthonormal, you have and it follows that
In Theorem 5.11 the coordinates of relative to the orthonormal basis are called the
Fourier coefficients of relative to after the French mathematician Jean-Baptiste
Joseph Fourier (1768–1830). The corresponding coordinate matrix of relative to is
Find the coordinates of relative to the orthonormal basis for shown
below.
SOLUTION Because is orthonormal, you can use Theorem 5.11 to find the required coordinates.
So, the coordinate matrix relative to is
wB  
1
7
2

.
B
w  v3  5, 5, 2  0, 0, 1  2
w  v2  5, 5, 2  4
5,
3
5, 0  7
w  v1  5, 5, 2 
3
5, 4
5, 0  1
B
B   3
5, 4
5, 0, 4
5, 3
5, 0, 0, 0, 1 v1 v2 v3
R w  3 5, 5, 2
EXAMPLE 5 Representing Vectors Relative to an Orthonormal Basis
 w, v1 w, v2 . . . w, vnT.
wB  c1 c2 . . . cnT
w B
w B,
w B
w, vi
  ci v . i
, vi
  vi B 2  1,
w, vi
  ci
vi
, vi
.
B
 c1v1, vi
  c2v2, vi
  . . .  cnvn, vi

w, vi
  c1v1  c2v2  . . .  cnvn, vi

vi
w  c1v1  c2v2  . . .  cnvn.
c1, c2, . . . , c V n B ,
312 Chapter 5 Inner Product Spaces
Gram-Schmidt Orthonormalization Process
Having seen one of the advantages of orthonormal bases (the straightforwardness of coordinate representation), you will now look at a procedure for finding such a basis. This
procedure is called the Gram-Schmidt orthonormalization process, after the Danish
mathematician Jorgen Pederson Gram (1850–1916) and the German mathematician Erhardt
Schmidt (1876–1959). It has three steps.
1. Begin with a basis for the inner product space. It need not be orthogonal nor consist of
unit vectors.
2. Convert the given basis to an orthogonal basis.
3. Normalize each vector in the orthogonal basis to form an orthonormal basis.
REMARK : The Gram-Schmidt orthonormalization process leads to a matrix factorization
similar to the LU-factorization you studied in Chapter 2. You are asked to investigate this
QR-factorization in Project 1 at the end of this chapter.
Rather than give a general proof of this theorem, it seems more instructive to discuss a
special case for which you can use a geometric model. Let be a basis for as
shown in Figure 5.18. To determine an orthogonal basis for first choose one of the
original vectors, say Now you want to find a second vector orthogonal to Figure 5.19
shows that has this property. v2  projv1
v2
v1 v . 1.
R2,
R2 v , 1, v2

1. Let be a basis for an inner product space
2. Let where is given by
Then is an orthogonal basis for
3. Let Then the set is an orthonormal basis for
Moreover, span for v k  1, 2, . . . , n. 1, v2, . . . , vk
  spanu1, u2, . . . , uk

B  u V. 1, u2, . . . , un u  i  wi
wi
.
B V.
wn  vn  vn, w1
w1, w1
w1  vn, w2
w2, w2
w2  . . .  vn, wn1
wn1, wn1
wn1.
.
.
.
w3  v3  v3, w1
w1, w1
w1  v3, w2
w2, w2
w2
w2  v2  v2, w1
w1, w1
w1
w1  v1
B wi   w1, w2, . . . , wn
,
B  v V. 1, v2, . . . , vn THEOREM 5.12 
Gram-Schmidt
Orthonormalization Process
Section 5.3 Orthonormal Bases: Gram-Schmidt Process 313
By letting
and
you can conclude that the set is orthogonal. By the corollary to Theorem 5.10, it
is a basis for Finally, by normalizing and you obtain the orthonormal basis for
shown below.
Apply the Gram-Schmidt orthonormalization process to the basis for shown below.
SOLUTION The Gram-Schmidt orthonormalization process produces
The set is an orthogonal basis for . By normalizing each vector in you
obtain
 	
2
2 ,
2
2 
  2 . 	
1
2
,
1
2
 u2  w2
w2 
 1
1
2 	
1
2
,
1
2

 	
2
2 ,
2  2 
 2
2 u 1, 1 1  w1
w1 
 1
2
1, 1
R B, 2 B  w1, w2

 1
2, 1
2  0, 1  . 1
21, 1
w2  v2  v2  w1
w1  w1
w1
w1  v1  1, 1
B  1, 1, 0, 1
v1 v2
R2
EXAMPLE 6 Applying the Gram-Schmidt Orthonormalization Process
u1, u2
  
w1
w1 
, w2
w2 
R2
w2 w , R 1 2
.
w1, w2

w2  v2  projv1
v2  v2  v2  w1
w1  w1
w1 w , 1  v1
Figure 5.18
v1
v2
{v1, v2} is a basis for R2.
Figure 5.19
w2
v2
v1 = w1
proj v1
v2
w2 = v2 − proj v1
v2
is orthogonal to w1 = v1.
314 Chapter 5 Inner Product Spaces
So, is an orthonormal basis for See Figure 5.20.
Figure 5.20
REMARK : An orthonormal set derived by the Gram-Schmidt orthonormalization process
depends on the order of the vectors in the basis. For instance, try reworking Example 6 with
the original basis ordered as rather than
Apply the Gram-Schmidt orthonormalization process to the basis for shown below.
SOLUTION Applying the Gram-Schmidt orthonormalization process produces
The set is an orthogonal basis for Normalizing each vector in
produces
u2  w2
w2 
 1
1
2 	1
2
,
1
2
, 0
  	2
2 ,
2
2 , 0

u1  w1
w1 
 1
2
1, 1, 0  	
2
2 ,
2
2 , 0

R B 3 B  w . 1, w2, w3

 0, 1, 2   0, 0, 2. 1
2
1, 1, 0  1
2
1
2 	1
2
,
1
2
, 0

w3  v3  v3  w1
w1  w1
w1  v3  w2
w2  w2
w2
 1, 2, 0  3
2
1, 1, 0  	1
2
,
1
2
, 0 w2  
 v2  v2  w1
w1  w1
w1
w1  v1  1, 1, 0
B  1, 1, 0, 1, 2, 0, 0, 1, 2
v1 v2 v3
R3
EXAMPLE 7 Applying the Gram-Schmidt Orthonormalization Process
v1, v2 v . 2, v1

1
x
− , , 2 2 2 2
2 2 2 2 ( ( ) )
y
−1
u2 u1
Orthonormal basis: B″ = {u1, u2}
(0, 1) (1, 1)
−1 1
x
1
y
v1 v2
Given basis: B = {v1, v2}
R2 B  u . 1, u2

Section 5.3 Orthonormal Bases: Gram-Schmidt Process 315
So, is an orthonormal basis for
Examples 6 and 7 applied the Gram-Schmidt orthonormalization process to bases for
and The process works equally well for a subspace of an inner product space. This
procedure is demonstrated in the next example.
The vectors and span a plane in Find an orthonormal basis
for this subspace.
SOLUTION Applying the Gram-Schmidt orthonormalization process produces
Normalizing and produces the orthonormal set
See Figure 5.21.
Apply the Gram-Schmidt orthonormalization process to the basis in
using the inner product
 p, q  
1
1
pxqx dx.
P2 B  1, x, x , 2
EXAMPLE 9 Applying the Gram-Schmidt Orthonormalization Process (Calculus)
 	
2
2 , 0, 2
2 
.
 1
2
1, 0, 1
u2  w2
w2 
u1  w1
w1 
 0, 1, 0
w1 w2
 1, 1, 1   1, 0, 1. 1
1
0, 1, 0
w2  v2  v2  w1
w1  w1
w1
w1  v1  0, 1, 0
R3 v . v 2  1, 1, 1 1  0, 1, 0
EXAMPLE 8 Applying the Gram-Schmidt Orthonormalization Process
R3.
R2
R3 B  u . 1, u2, u3

u3  w3
w3 
 1
2
0, 0, 2  0, 0, 1.
Figure 5.21
y
x
(0, 1, 0)
(1, 0, 1) 2
1
z
u1
u2
316 Chapter 5 Inner Product Spaces
SOLUTION Let Then you have
(In Exercises 43–46 you are asked to verify these calculations.) Now, by normalizing
you have
REMARK : The polynomials and in Example 9 are called the first three
normalized Legendre polynomials, after the French mathematician Adrien-Marie
Legendre (1752–1833).
The computations in the Gram-Schmidt orthonormalization process are sometimes
simpler when each vector is normalized before it is used to determine the next vector.
This alternative form of the Gram-Schmidt orthonormalization process has the steps
shown below.
un  wn
wn 
, where wn  vn  vn, u1u1  . . .  vn, un1un1
.
.
.
u3  w3
w3 
, where w3  v3  v3, u1u1  v3, u2u2
u2  w2
w2 
, where w2  v2  v2, u1u1
u1  w1
w1 
 v1
v1 
wi
u1 u3 , u2,
u3  w3
w3 
 1
8
45 	x2  1
3
  5
22
3x2  1.
u2  w2
w2 
 1
2
3
x  3
2
x
u1  w1
w1 
 1
2
1  1
2
B  w1, w2, w3
,
 x2  1
3
.
 x2  2
3
2 1  0
2
3
x
w3  v3  v3, w1
w1, w1
w1  v3, w2
w2, w2
w2
 x  0
2 w 1  x 2  v2  v2, w1
w1, w1
w1
w1  v1  1
B  1, x, x2  v1, v2, v3
.
Section 5.3 Orthonormal Bases: Gram-Schmidt Process 317
Find an orthonormal basis for the solution space of the homogeneous system of linear
equations.
SOLUTION The augmented matrix for this system reduces as follows.
If you let and each solution of the system has the form
So, one basis for the solution space is
To find an orthonormal basis use the alternative form of the
Gram-Schmidt orthonormalization process, as follows.
 	
 3
30,  4
30, 2
30, 1
30

 1
30 3, 4, 2, 1
u2  w2
w2 
 3, 4, 2, 1
 1, 8, 0, 1  1, 8, 0, 1  	2
3
,
2
3
,
1
3
, 0
 	2
3
,
2
3
,
1
3
, 0

w2  v2  v2, u1u1
 	
2
3
,
2
3
,
1
3
, 0

 1
3
2, 2, 1, 0
u1  v1
v1 
B  u1, u2
,
B  v1, v2
  2, 2, 1, 0, 1, 8, 0, 1.

x1
x2
x3
x4


2s  t
2s  8t
s
t   s

2
2
1
0
  t

1
8
0
1

.
x x 4  t, 3  s

1
0
0
1
2
2
1
8
0
0   1
2
1
1
0
2
7
6
0
0
x1
2x1


x2
x2  2x3


7x4
6x4


0
0
EXAMPLE 10 Alternative Form of Gram-Schmidt Orthonormalization Process
318 Chapter 5 Inner Product Spaces
SECTION 5.3 Exercises
In Exercises 1–14, determine whether the set of vectors in is
orthogonal, orthonormal, or neither.
1. 2.
3. 4.
5. 6.
7.
8.
9.
10.
11.
12.
13.
14.
In Exercises 15–18, determine if the set of vectors in is
orthogonal and orthonormal. If the set is only orthogonal, normalize the set to produce an orthonormal set.
15. 16.
17.
18.
19. Complete Example 2 by verifying that is an orthonormal basis for with the inner product
20. Verify that is an orthonormal
basis for
In Exercises 21–26, find the coordinates of relative to the orthonormal basis in
21.
22.
23.
24.
25.
26.
In Exercises 27–36, use the Gram-Schmidt orthonormalization
process to transform the given basis for into an orthonormal
basis. Use the Euclidean inner product for and use the vectors in
the order in which they are shown.
27. 28.
29. 30.
31.
32.
33.
34.
35.
36.
In Exercises 37–42, use the Gram-Schmidt orthonormalization
process to transform the given basis for a subspace of into an
orthonormal basis for the subspace. Use the Euclidean inner product for and use the vectors in the order in which they are shown.
37. 38.
39. 40.
41.
42.
Calculus In Exercises 43–46, let be a basis for
with the inner product
Complete Example 9 by verifying the indicated inner products.
43. 44.
45. 46. x2 x , x  0 2
, 1  2
3
x, 1  0 1, 1  2
 p, q  
1
1
pxqx dx.
B  P2 1, x, x2
B  7, 24, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2
B  1, 2, 1, 0, 2, 2, 0, 1, 1, 1, 1, 0
B  (3, 4, 0, 1, 0, 0 B  1, 2, 0, 2, 0, 2
B  8, 3, 5 B  4, 7, 6
Rn
Rn
B  3, 4, 0, 0, 1, 1, 0, 0, 2, 1, 0, 1, 0, 1, 1, 0
B  0, 1, 1, 1, 1, 0, 1, 0, 1
B  0, 1, 2, 2, 0, 0, 1, 1, 1
B  4, 3, 0, 1, 2, 0, 0, 0, 4
B  1, 0, 0, 1, 1, 1, 1, 1, 1
B  1, 2, 2, 2, 2, 1, 2, 1, 2
B  0, 1, 2, 5 B  4, 3, 3, 2
B  3, 4, 1, 0 B  1, 2, 1, 0
Rn
Rn
x  2, 1, 4, 3
B   5
13, 0, 12
13, 0, 0, 1, 0, 0, 12
13, 0, 5
13, 0, 0, 0, 0, 1,
B   x  5, 10, 15 3
5, 4
5, 0, 4
5, 3
5, 0, 0, 0, 1,
B  1, 0, 0, 0, 1, 0, 0, 0, 1, x  3, 5, 11
x  2, 2, 1
B  	
10
10 , 0,
310
10 
, 0, 1, 0, 	310
10 , 0, 10
10 
,
B  	
5
5 ,
25
5 
, 	25
5 ,
5
5 
, x  3, 4
B  	213
13 ,
313
13 
, 	
313
13 ,
213
13 
, x  1, 2
Rn B .
x
R2.
sin , cos , cos , sin 
a0b0  a1b1  a2b2  a3b3.
P3  p, q
1, x, x2, x3
 2
15, 1
15, 2
15,  1
15, 2
15, 0
3, 3, 3 , 2, 0, 2 
1, 4, 8, 2 2, 5, 10, 4
Rn
	310
10 , 0, 0, 10
10 

	
10
10 , 0, 0,
310
10 
, 0, 0, 1, 0, 0, 1, 0, 0,
	
2
2 , 0, 0, 2
2 
, 	0, 2
2 ,
2
2 , 0
, 	1
2
,
1
2
, 1
2
,
1
2

6, 3, 2, 1, 2, 0, 6, 0
2, 5, 3, 4, 2, 6
	
2
3 , 0, 2
6 
, 	0,
25
5 , 5
5 
, 	
5
5 , 0,
1
2

	
2
2 , 0, 2
2 
, 	6
6 ,
6
3 ,
6
6 
, 	
3
3 ,
3
3 , 3
3 

2, 4, 2, 0, 2, 4, 10, 4, 2
4, 1, 1, 1, 0, 4, 4, 17, 1
1, 2, 2
5, 1
5   3
5, 4
5, 4
5, 3
5
4, 6, 5, 0 11, 4, 8, 3
2, 4, 2, 1 3, 2, 4, 6
Rn
Section 5.3 Orthonormal Bases: Gram-Schmidt Process 319
True or False? In Exercises 47 and 48, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
47. (a) A set of vectors in an inner product space is orthogonal
if every pair of vectors in is orthogonal.
(b) To show that a set of nonzero vectors is a basis for it is
sufficient to show that the set is an orthogonal set.
(c) An orthonormal basis derived by the Gram-Schmidt orthonormalization process does not depend on the order of the
vectors in the basis.
48. (a) A set of vectors in an inner product space is orthonormal if every vector is a unit vector and each pair of vectors
is orthogonal.
(b) If a set of nonzero vectors in an inner product space is
orthogonal, then is linearly independent.
(c) The Gram-Schmidt orthonormalization process is a procedure for finding an orthonormal basis for an inner product
space
In Exercises 49–54, find an orthonormal basis for the solution
space of the homogeneous system of linear equations.
49.
50.
51.
52.
53. 54.
In Exercises 55–60, let and
be vectors in with
Determine whether the given second-degree polynomials form an
orthonormal set, and if not, use the Gram-Schmidt orthonormalization process to form an orthonormal set.
55.
56.
57.
58.
59.
60.
61. Use the inner product in and
the Gram-Schmidt orthonormalization process to transform
into an orthonormal basis.
62. Writing Explain why the result of Exercise 61 is not an
orthonormal basis when the Euclidean inner product on is
used.
63. Let be an orthonormal basis for Prove
that for any
vector in This equation is called Parseval’s equality.
64. Guided Proof Prove that if is orthogonal to each vector in
then is orthogonal to every linear
combination of vectors in
Getting Started: To prove that is orthogonal to every linear
combination of vectors in you need to show that their dot
product is 0.
(i) Write as a linear combination of vectors, with arbitrary scalars in S.
(ii) Form the inner product of w and v.
(iii) Use the properties of inner products to rewrite the inner
product as a linear combination of the inner
products
(iv) Use the fact that w is orthogonal to each vector in to
lead to the conclusion that w is orthogonal to
65. Let be an matrix. Prove that the following conditions
are equivalent.
(a) (Such a matrix is called orthogonal.)
(b) The row vectors of form an orthonormal basis for
(c) The column vectors of form an orthonormal basis
for
66. Use each matrix to illustrate the result of Exercise 65.
(a)
(b)
67. Find an orthonormal basis for that includes the vectors
and v2  	0,  1
2
, 0,
1
2
 v . 1  	
1
2
, 0,
1
2
, 0

R4
P  
1
2
1
2
0
1
2
1
2
0
0
0
1

P  
1
0
0
0
0
1
0
1
0

Rn.
P
Rn P .
P1  PT.
P n  n
v.
S
w, vi
, i  1, . . . , n.
w, v
c1, . . . , cn,
v
S,
w
S.
S  v1, v2, . . . , vn
, w
w
Rn v .
v2  v  u1
2  v  u2
2  . . .  v  un
2
Rn u . 1, u2, . . . , un

R2
2, 1, 2, 10
R2 u, v  2u1v1  u2v2

3x2  4x
5 ,
4x2  3x
5 , 1
x2  1, x  1
1, x, x2
x2, x2  2x, x2  2x  1
2x2  1, 2x2  x  2

x2  1
2 ,
x2  x  1
3 
 p, q  a0b0  a1b1  a2b2.
b0  b1 P2 x  b2x2
px  a0  a1x  a2x2 qx
x1  2x2  x x1  3x2  3x3  0 3  0
x1  2x2  x3  x4  0
x1  x2  x3  x4  0
 2x1  x2  2x3  2x4  0
x1  x2  x3  x4  0
 3x1  x2  5x3  4x4  0
 2x1  x2  x4  0
x1  x2  3x3  2x4  0
x1  x2  3x3  2x4  0
x1  2x2  3x3  4x4  0
 2x1  x2  6x3  2x4  0
V.
S
S V
S V
Rn
,
S
S V
68. Let be a subspace of Prove that the set shown below is a
subspace of in
Then prove that the intersection of and is
In Exercises 69–72, find bases for the four fundamental subspaces
of the matrix shown below.
Then show that and
69. 70.
71.
72.
73. Let be an matrix.
(a) Explain why is the same as the row space of
(b) Prove that
(c) Prove that
(d) Prove that NAT   RA.
NA  RAT .
NA  RAT .
RA A. T
A m  n

0
1
1
0
0
2
2
0
1
0
1
1
2
2
0
2
0
0
0
1


1
1
0
1
1
1

0
0
0
1
2
1
1
2
1   1
0
1
1
2
3
1
1
0

NAT   RA NA  RA . T 
RAT   column space of AT RA  column space of A
NAT   nullspace of A N T A  nullspace of A
A
W 0. W 
R W. n
Rn W .
320 Chapter 5 Inner Product Spaces
Mathematical Models and Least Squares Analysis
In this section, you will study inconsistent systems of linear equations and learn how to
find the “best possible solution” of such a system. The necessity of “solving” inconsistent systems arises in the computation of least squares regression lines, as illustrated in
Example 1.
Let and be three points in the plane, as shown in Figure 5.22. How can
you find the line that “best fits” these points? One way is to note that if the
three points were collinear, then the following system of equations would be consistent.
This system can be written in the matrix form where
and
Because the points are not collinear, however, the system is inconsistent. Although it is
impossible to find such that you can look for an that minimizes the norm of
the error The solution Ax  b.
x Ax  b, x
x  
c0
c1
 b  . 
0
1
3
 A  , 
1
1
1
1
2
3

,
Ax  b,
c0  3c1  3
c0  2c1  1
c0  c1  0
y  c0  c1x
1, 0, 2, 1, 3, 3
EXAMPLE 1 Least Squares Regression Line
5.4
Figure 5.22
1
2
3
4
x
y
1 234
of this minimization problem is called the least squares regression line
In Section 2.5, you briefly studied the least squares regression line and how to calculate
it using matrices. Now you will combine the ideas of orthogonality and projection to
develop this concept in more generality. To begin, consider the linear system
where is an matrix and is a column vector in You already know how to use
Gaussian elimination with back-substitution to solve for if the system is consistent. If the
system is inconsistent, however, it is still useful to find the “best possible” solution; that is,
the value of for which the difference between and is smallest. One way to define
“best possible” is to require that the norm of be minimized. This definition is the
heart of the least squares problem.
REMARK : The term least squares comes from the fact that minimizing is
equivalent to minimizing which is a sum of squares.
Orthogonal Subspaces
To solve the least squares problem, you first need to develop the concept of orthogonal
subspaces. Two subspaces of are said to be orthogonal if the vectors in each subspace
are orthogonal to the vectors in the other subspace.
The subspaces
and
are orthogonal because the dot product of any vector in and any vector in is zero. S1 S2
S2  span	
1
1
1

 S1  span	
1
0
1

,

1
1
0


EXAMPLE 2 Orthogonal Subspaces
Rn
Ax  b2,
Ax  b
Ax  b
x Ax b
x
Rm A m  n b .
Ax  b,
y  c0  c1x.
x  
c0
c1

Given an matrix and a vector in the least squares problem is to find in
such that is minimized. Ax  b2 Rn
R x m Least Squares Problem m  n A b ,
The subspaces and of are orthogonal if for all in and all
in S2.
v R v1  v2  0 v1 S1 2 n Definition of S1 S2
Orthogonal Subspaces
Section 5.4 Mathematical Models and Least Squares Analysis 321
322 Chapter 5 Inner Product Spaces
Notice in Example 2 that the zero vector is the only vector common to both and
This is true in general. If and are orthogonal subspaces of then their intersection
consists only of the zero vector. You are asked to prove this fact in Exercise 45.
Provided with a subspace of the set of all vectors orthogonal to every vector in
is called the orthogonal complement of as shown in the next definition.
The orthogonal complement of the trivial subspace is all of and, conversely, the
orthogonal complement of is the trivial subspace In Example 2, the subspace is
the orthogonal complement of and the subspace is the orthogonal complement of
In general, the orthogonal complement of a subspace of is itself a subspace of (see
Exercise 46). You can find the orthogonal complement of a subspace of by finding the
nullspace of a matrix, as illustrated in the next example.
Find the orthogonal complement of the subspace of spanned by the two column
vectors and of the matrix
SOLUTION A vector will be in the orthogonal complement of if its dot product with the two
columns of and is zero. If you take the transpose of then you will see that the
orthogonal complement of consists of all the vectors such that
That is, the orthogonal complement of is the nullspace of the matrix
Using the techniques for solving homogeneous linear systems, you can find that a possible
basis for the orthogonal complement can consist of the two vectors
S  NAT .
AT S :

1
0
2
0
1
0
0
1 
x1
x2
x3
x4
  
0
0
ATu  0
AT S u u  0.
v A, 2 v , A 1 ,
u 	 R S 4
v1 v2
A

1
2
1
0
0
0
0
1

v A. v1 2
R4 S
EXAMPLE 3 Finding the Orthogonal Complement
Rn
Rn Rn
S1 S . 2 S2,
S1 R 0. n
Rn 0 ,
S,
R S n S ,
Rn S , 2 S1
S2 S . 1
If is a subspace of then the orthogonal complement of is the set
S for all vectors v 	 S.   u 	 Rn : v  u  0
R S n Definition of S ,
Orthogonal Complement
Section 5.4 Mathematical Models and Least Squares Analysis 323
and
Notice that in Example 3 is split into two subspaces, and
In fact, the four vectors and form a basis for Each
vector in can be uniquely written as a sum of a vector from and a vector from This
concept is generalized in the next definition.
(a) From Example 2, you can see that is the direct sum of the subspaces
and
(b) From Example 3, you can see that where
and
The next theorem collects some important facts about orthogonal complements and
direct sums.
S  span	
2
1
0
0

,

1
0
1
0

 S  span . 	
1
2
1
0

,

0
0
0
1


R4  S % S,
S2  span	
1
1
1

 S . 1  span	
1
0
1

,

1
1
0


R3
EXAMPLE 4 Direct Sum
S R S . 4
R4 u . u1 2 v , 2 v , 1 S ,   spanu1, u2.
S  spanv1, v2 R  4
u2

1
0
1
0
 u . 1

2
1
0
0

Let and be two subspaces of If each vector can be uniquely written as a
sum of a vector from and a vector from , then is the direct sum
of and and you can write Rn  S1 % S2 S . 2 S , 1
Rn x  s1  s S2 2 s , S1 2 s1
x 	 Rn Rn S . Definition of S1 2
Direct Sum
Let be a subspace of Then the following properties are true.
1.
2.
3. S  S
Rn  S % S
dimS  dimS  n
Rn THEOREM 5.13 S .
Properties of
Orthogonal Subspaces
324 Chapter 5 Inner Product Spaces
PROOF 1. If or then Property 1 is trivial. So let be a basis for
Let be the matrix whose columns are the basis vectors Then
which implies that where is a matrix of rank (see Section
5.3, Exercise 73). Because the dimension of is you have shown that
2. If or then Property 2 is trivial. So let be a basis for
and let be a basis for It can be shown that the set
is linearly independent and forms a basis for Let
If you write
and then you have expressed an arbitrary
vector as the sum of a vector from and a vector from
To show the uniqueness of this representation, assume (where
denotes a vector that has all zero entries except for one, which is 1). This implies that
So, the two vectors and are in both and Because
you must have and .
3. Let Then for all which implies that On the other
hand, if then, because you can write as the unique sum of
the vector from and a vector from Because is in
it is orthogonal to every vector in and in particular to So,
This implies that and
You studied the projection of one vector onto another in Section 5.3. This is now generalized to projections of a vector onto a subspace Because every vector
in can be uniquely written as a sum of a vector from and a vector from
The vector is called the projection of onto the subspace and is denoted by
So, which implies that the vector is
orthogonal to the subspace
Provided with a subspace of you can use the Gram-Schmidt orthonormalization
process to calculate an orthonormal basis for It is then an easy matter to compute the
projection of a vector onto using the next theorem. (You are asked to prove this
theorem in Exercise 47.)
v S
S.
Rn S ,
S.
v  proj v1  projSv. v2  v  v1  v  projSv, Sv
v v S, 1
v2 	 S v . v  v 1 	 S, 1  v2,
S R S : n v
Rn  S % S v S. ,
w  0 v  s  w  s 	 S.
 w  w.
 w  s  w  w
0  w  v  w  s  w
S, v.
S S w , , v  s  w, s 	 S, w 	 S S .
R v n  S % S v 	 S , ,
v 	 S u 	 S .  v 	 S. v  u  0 ,
S  S  0, vˆ  v  ˆww
S vˆ  v  w  ˆw. vˆ  v w  ˆw S .
ˆr
x  v  w  vˆ  ˆw
S x  v  w.  x S ,
w  ct1vt1  . . .  cnvn, . . .  ct
vt
v  c x 	 Rn, x  c1
v1  . . .  ct
vt  ct1vt1  . . .  cnvn. 1v1 
Rn v . 1, v2, . . . , vt
, vt1, . . . , vn

S v . t1, vt2, . . . , vn S 
v1, v2, . . . ,vt S  R S  0,  n
dim(S)  dim(S  t  n  t  n.
NA n  t, T
t  nA t T S  NAT S  R(A), ,
vi 0 < t < n. A n  t .
v S, 1, v2, . . . ,vt S  R S  0,  n
If is an orthonormal basis for the subspace of and then
projSv  v  u1u1  v  u2u2  . . .  v  ut
ut
.
v 	 Rn R , n u S , 1, u2, . . . , ut THEOREM 5.14 
Projection onto a
Subspace
Find the projection of the vector onto the subspace of spanned by the vectors
and
SOLUTION By normalizing and you obtain an orthonormal basis for
Use Theorem 5.14 to find the projection of onto
The projection of onto the plane is illustrated in Figure 5.23.
Theorem 5.9 said that among all the scalar multiples of a vector the orthogonal
projection of onto is the one closest to Example 5 suggests that this property is also
true for projections onto subspaces. That is, among all the vectors in the subspace the
vector is the closest vector to These two results are illustrated in Figure 5.24. proj v. Sv
S,
uv v.
u,
v S


1
9
5
3
5
  6
10 
0
3
10
1
10

 1

1
0
0

 projSv  v  u1u1  v  u2u2
v S.

 0
3
10
1
10

,

1
0
0

 u1, u2
   1
10
w1,
1
2
w2
w S. 2 w , 1
w2  
2
0
0
 w . 1  
0
3
1

R3 v  S 
1
1
3

EXAMPLE 5 Projection onto a Subspace
Section 5.4 Mathematical Models and Least Squares Analysis 325
Figure 5.24
v
u projuv
v − projuv
S
v
projsv
v − projsv
326 Chapter 5 Inner Product Spaces
PROOF Let By adding and subtracting the same quantity to and from the
vector you obtain
Observe that is in and is orthogonal to So, and
are orthogonal vectors, and you can use the Pythagorean Theorem (Theorem
5.6) to obtain
Because the second term on the right is positive, and you have
Fundamental Subspaces of a Matrix
You need to develop one more concept before solving the least squares problem. Recall
that if is an matrix, the column space of is a subspace of consisting of all
vectors of the form The four fundamental subspaces of the matrix are
defined as follows (see Exercises 69–73 in Section 5.3).
These subspaces play a crucial role in the solution of the least squares problem.
Find the four fundamental subspaces of the matrix
SOLUTION The column space of is simply the span of the first and third columns, because the second
column is a scalar multiple of the first.
A
A

1
0
0
0
2
0
0
0
0
1
0
0

.
EXAMPLE 6 Fundamental Subspaces
RAT   column space of AT RA  column space of A
NAT   nullspace of A N T A  nullspace of A
Ax, x 	 R A n.
Rm A m  n A
v  projSv < v  u.
u  projSv,
v  u2  v  projSv2   projSv  u2.
projSv  u
v  proj v  proj S. Sv proj S Sv Sv  u
v  u  v  projSv  projSv  u.
v  u,
projS u 	 S, u  proj v Sv.
Let be a subspace of and let Then, for all
v  projSv < v  u.
u 	 S, u  proj v 	 R Sv, n R . n S THEOREM 5.15
Orthogonal Projection
and Distance
Section 5.4 Mathematical Models and Least Squares Analysis 327
The column space of is equivalent to the row space of which is spanned by the first
two rows.
The nullspace of is a solution space of the homogeneous system
Finally, the nullspace of is a solution space of the homogeneous system whose coefficient matrix is
In Example 6, observe that and are orthogonal subspaces of and
and are orthogonal subspaces of These and other properties of these subspaces are
stated in the next theorem.
PROOF To prove Property 1, let and Because the column space of is equal
to the row space of you can see that implies Property 2 follows from
applying Property 1 to
To prove Property 3, observe that So,
A similar argument applied to proves Property 4. RAT
Rm  RA % RA  RA % NAT .
RA  NAT .
AT.
A u  v  0. T A u  0 T,
u 	 NA A T v 	 RA .
R3 NA .
RAT R  4 NA , T RA
NAT   span	
0
0
1
0

 , 
0
0
0
1


AT.
AT
NA  span	
2
1
0


A Ax  0.
RAT   span	
1
2
0

 , 
0
0
1


A A, T
RA  span	
1
0
0
0

 , 
0
1
0
0


If is an matrix, then
1. and are orthogonal subspaces of
2. and are orthogonal subspaces of
3.
4. RAT % NA  Rn.
RA % NAT   Rm.
Rn RA NA . T
Rm NA . T RA
THEOREM 5.16 A m  n
Fundamental Subspaces
of a Matrix
Figure 5.23
y
x
v
S
z
projsv
w1
w2
328 Chapter 5 Inner Product Spaces
Least Squares
You have now developed all the tools needed to solve the least squares problem. Recall that
you are attempting to find a vector that minimizes where is an matrix
and is a vector in Let be the column space of You can assume that
is not in because otherwise the system would be consistent. You are looking for
a vector in that is as close as possible to as indicated in Figure 5.25.
From Theorem 5.15 you know that the desired vector is the projection of onto
Letting be that projection, you can see that is orthogonal
to But this implies that is in which equals according to
Theorem 5.16. This is the crucial observation: is in the nullspace of So, you
have
The solution of the least squares problem comes down to solving the linear system
of equations These equations are called the normal equations of the least
squares problem
Find the solution of the least squares problem
presented in Example 1.
SOLUTION Begin by calculating the matrix products shown below.
The normal equations are

3
6
6
14c0
c1
  
4
11.
ATAx  AT b
AT b  
1
1
1
2
1
3 
0
1
3
  
4
11
ATA  
1
1
1
2
1
3 
1
1
1
1
2
3
  
3
6
6
14

1
1
1
1
2
3
 
c0
c1
  
0
1
3

Ax  b
EXAMPLE 7 Solving the Normal Equations
Ax  b.
ATAx  AT b.
n  n
ATAˆx  AT b.
ATAˆx  AT b  0
ATAˆx  b  0
AT Aˆx  b .
NAT RA   S  RA. Aˆx  b ,
Aˆx  b  proj Aˆx  projSb Sb  b
b S.
x SA b,
S, Ax  b
SR A: S  RA. b m b .
x Ax  b, A m  n
Figure 5.25
S
b
Ax
Figure 5.26
1
2
3
4
x
y
1234
y = x − 3
2
5
3
Section 5.4 Mathematical Models and Least Squares Analysis 329
The solution of this system of equations is which implies that the least squares
regression line for the data is as indicated in Figure 5.26.
REMARK : For an matrix the normal equations form an system of linear
equations. This system is always consistent, but it may have an infinite number of solutions.
It can be shown, however, that there is a unique solution if the rank of is
The next example illustrates how to solve the projection problem from Example 5 using
normal equations.
Find the orthogonal projection of the vector onto the column space of the matrix
SOLUTION To find the orthogonal projection of onto first solve the least squares problem
As in Example 7, calculate the matrix products and
AT b  
0
2
3
0
1
0 
1
1
3
  
6
2
ATA  
0
2
3
0
1
0 
0
3
1
2
0
0
  
10
0
0
4
AT A b. TA
b S, Ax  b.
A  
0
3
1
2
0
0

.
b  S 
1
1
3

EXAMPLE 8 Orthogonal Projection onto a Subspace
A n.
m  n A, n  n
y  3
2x  5
3,
x  
5
3
3
2
,
Many graphing utilities and computer software programs have built-in programs for finding the least
squares regression line for a set of data points. If you have access to such tools, try verifying the
result of Example 7. Keystrokes and programming syntax for these utilities/programs applicable to
Example 7 are provided in the Online Technology Guide, available at college.hmco.com/pic/
larsonELA6e.
Technology
Note
330 Chapter 5 Inner Product Spaces
The normal equations are
The solution of these equations is easily seen to be
Finally, the projection of onto is
which agrees with the solution obtained in Example 5.
Mathematical Modeling
Least squares problems play a fundamental role in mathematical modeling of real-life
phenomena. The next example shows how to model the world population using a least
squares quadratic polynomial.
Table 5.1 shows the world population (in billions) for six different years. (Source: U.S.
Census Bureau)
TABLE 5.1
Year 1980 1985 1990 1995 2000 2005
Population (y) 4.5 4.8 5.3 5.7 6.1 6.5
Let represent the year 1980. Find the least squares regression quadratic polynomial for these data and use the model to estimate the population for
the year 2010.
SOLUTION By substituting the data points and
into the quadratic polynomial you obtain the following system of
linear equations.
y  c0  c1x  c2x2,
0, 4.5, 5, 4.8, 10, 5.3, 15, 5.7, 20, 6.1, 25, 6.5
y  c0  c1x  c2x2
x  0
EXAMPLE 9 World Population
Ax  
0
3
1
2
0
0
 
3
5
1
2
  
1
9
5
3
5

,
b S
x  
x1
x2
  
3
5
1
2
.

10
0
0
4 x1
x2
  
6
2.
ATAx  AT b
This produces the least squares problem
The normal equations are
and their solution is
Note that So, the least squares polynomial for these data is the linear polynomial:
Evaluating this polynomial at gives the estimate of the world population for the year
2010:
billion.
Least squares models can arise in many other contexts. Section 5.5 explores some applications of least squares models to approximation of functions. In the final example of this
section, a nonlinear model is used to find a relationship between the period of a planet and
its mean distance from the sun.
y  4.5  0.0830  6.9
x  30
y  4.5  0.08x.
c2  0.
 
4.5
0.08
0 
.
x  
c0
c1
c2

 6
75
1375
75
1375
28,125
1375
28,125
611,875 c0
c1
c2
  
 32.9
447
8435
ATAx  AT b

4.5
4.8
5.3
5.7
6.1
6.5  .

1
1
1
1
1
1
0
5
10
15
20
25
0
25
100
225
400
625  
c0
c1
c2

Ax  b
c0
c0
c0
c0
c0
c0





5c1
10c1
15c1
20c1
25c1





25c2
100c2
225c2
400c2
625c2






4.5
4.8
5.3
5.7
6.1
6.5
Section 5.4 Mathematical Models and Least Squares Analysis 331
332 Chapter 5 Inner Product Spaces
Table 5.2 shows the mean distances and the periods of the six planets that are closest to
the sun. The mean distance is given in terms of astronomical units (where the Earth’s mean
distance is defined as 1.0), and the period is in years. Find a model for these data. (Source:
CRC Handbook of Chemistry and Physics)
TABLE 5.2
Planet Mercury Venus Earth Mars Jupiter Saturn
Distance, x 0.387 0.723 1.0 1.523 5.203 9.541
Period, y 0.241 0.615 1.0 1.881 11.861 29.457
If you plot the data as shown, they do not seem to lie in a straight line. By taking the
logarithm of each coordinate, however, you obtain points of the form as shown
in Table 5.3.
TABLE 5.3
Planet Mercury Venus Earth Mars Jupiter Saturn
ln x –0.949 –0.324 0.0 0.421 1.649 2.256
ln y –1.423 –0.486 0.0 0.632 2.473 3.383
Figure 5.27 shows a plot of the transformed points and suggests that the least squares
regression line would be a good fit. Using the techniques of this section, you can find that
the equation of the line is
or
Figure 5.27
Mercury
Mars
1
2
3
ln x
ln y
Ven 2 3 us Earth
Jupiter
Saturn
ln y = ln x 3
2
y  x3
2 ln y  . 3
2 ln x
ln x, ln y,
x y
EXAMPLE 10 Application to Astronomy
You can use a computer software
program or graphing utility with
a built-in power regression
program to verify the result of
Example 10. For example, using
the data in Table 5.2 and a
graphing utility, a power fit
program would result in an
answer of (or very similar to)
Keystrokes
and programming syntax for
these utilities/programs applicable
to Example 10 are provided in the
Online Technology Guide,
available at college.hmco.com/
pic/larsonELA6e.
y  1.00042x1.49954.
Technology
Note
Section 5.4 Mathematical Models and Least Squares Analysis 333
SECTION 5.4 Exercises
In Exercises 1–4, determine whether the sets are orthogonal.
1.
2.
3.
4.
In Exercises 5– 8, find the orthogonal complement .
5. is the subspace of consisting of the -plane.
6. is the subspace of consisting of all vectors whose third and
fourth components are zero.
7. 8.
9. Find the orthogonal complement of the solution of Exercise 7.
10. Find the orthogonal complement of the solution of Exercise 8.
In Exercises 11–14, find the projection of the vector onto the
subspace
11.
12.
13.
14.
In Exercises 15 and 16, find the orthogonal projection of
onto the column space of the matrix
15. 16.
In Exercises 17– 20, find bases for the four fundamental subspaces
of the matrix
17. 18.
19.
20.
In Exercises 21–26, find the least squares solution of the system
21.
22.
23. A

1
1
0
1
0
1
1
1
1
1
1
0
 b

4
1
0
1

A  
0
1
1
1
0
2
 b  
1
1
3

A  
2
1
1
1
2
1
 b   2
0
3

Ax  b.
A

1
0
1
1
0
1
1
0
1
1
0
1

A

1
0
1
1
0
1
1
2
0
1
1
2
1
1
2
3

A  
0
1
1
1
2
1
1
0
1
 A  
1
0
2
1
3
0
A.
A  
0
1
1
2
1
3
 A  
1
0
1
2
1
1

b  2 2 1 A. T
S  span
1
1
1
1

,

0
1
1
0

,

0
1
1
0

, v

1
2
3
4

S  span
1
0
1

,

 0
1
1
, v  
2
3
4

S  span
1
2
0
0

,

0
0
1
0

,

0
0
0
1

, v

1
1
1
1

S  span
0
0
1
1

,

0
1
1
1

, v

1
0
1
1

S.
v
S  span
0
1
1
1
 S  span
1
2
0
0

,

0
1
0
1

R5 S
R xz 3 S
S
S2  span
3
2
0
0

,

0
1
2
2
 S1  span
0
0
2
1

,

0
0
1
2

S2  span
1
1
1
1

,

0
2
2
0
 S1  span
1
1
1
1

S2  span
2
1
6

,

0
1
0
 S1  span
3
0
1

S2  span
1
2
0
 S1  span 2
1
1

,

0
1
1

334 Chapter 5 Inner Product Spaces
24.
25.
26.
In Exercises 27–32, find the least squares regression line for the
data points. Graph the points and the line on the same set of axes.
27.
28.
29.
30.
31.
32.
In Exercises 33–36, find the least squares quadratic polynomial for
the data points.
33.
34.
35.
36.
37. The table shows the annual sales (in millions of dollars) for
Advanced Auto Parts and Auto Zone for 2000 through 2007.
Find an appropriate regression line, quadratic regression
polynomial, or cubic regression polynomial for each company.
Then use the model to predict sales for the year 2010. Let
represent the year, with corresponding to 2000. (Source:
Advanced Auto Parts and Auto Zone)
Year 2000 2001 2002 2003
Advanced 2288 2518 3288 3494
Auto Parts Sales, y
Auto Zone Sales, y 4483 4818 5326 5457
Year 2004 2005 2006 2007
Advanced 3770 4265 4625 5050
Auto Parts Sales, y
Auto Zone Sales, y 5637 5711 5948 6230
38. The table shows the numbers of doctorate degrees awarded in
the education fields in the United States during the years 2001
to 2004. Find the least squares regression line for the data. Let
represent the year, with corresponding to 2001. (Source:
U.S. National Science Foundation)
Year 2001 2002 2003 2004
Doctorate 6337 6487 6627 6635 degrees, y
39. The table shows the world carbon dioxide emissions (in
millions of metric tons) during the years 1999 to 2004. Find the
least squares regression quadratic polynomial for the data. Let
represent the year, with corresponding to 1999.
(Source: U.S. Energy Information Administration)
Year 1999 2000 2001 2002 2003 2004
CO2, y 6325 6505 6578 6668 6999 7376
40. The table shows the sales (in millions of dollars) for Gateway,
Incorporated during the years 2000 to 2007. Find a least squares
regression quadratic polynomial that best fits the data. Let
represent the year, with corresponding to 2000. (Source:
Gateway Inc.)
Year 2000 2001 2002 2003
Sales, y 9600.6 6079.2 4171.3 3402.4
Year 2004 2005 2006 2007
Sales, y 3649.7 3854.1 4075.0 4310.0
t  0
t
y
t  1
t
y
t  1
t
y
t  0
t
2, 6, 1, 5, 0, 7
2, 1, 2, 2, 1
2, 0, 1, 0, 0, 1, 1, 2, 2, 5
0, 2, 1, 3
2, 2, 5
2, 3, 4
0, 0, 2, 2, 3, 6, 4, 12
2, 0, 1, 2, 0, 3, 1, 5, 2, 6
2, 1, 1, 2, 0, 1, 1, 2, 2, 1
3, 3, 2, 2, 0, 0, 1, 2
2, 1, 1, 0, 1, 0, 2, 2
1, 1, 2, 3, 4, 5
1, 1, 1, 0, 3, 3
A

0
1
2
1
0
2
1
1
1
2
1
1
0
1
1
 b
 1
0
1
1
0

A
 0
1
1
1
0
2
1
0
1
0
0
0
1
1
1
 b

0
1
0
0
1

A

1
1
0
1
1
1
1
0
1
1
1
1
 b

2
1
0
2

Section 5.4 Mathematical Models and Least Squares Analysis 335
41. The table shows the sales (in millions of dollars) for Dell
Incorporated during the years 1996 to 2007. Find the least
squares regression line and the least squares cubic regression
polynomial for the data. Let represent the year, with
corresponding to 1996. Which model is the better fit for the
data? Why? (Source: Dell Inc.)
Year 1996 1997 1998 1999
Sales, y 7759 12,327 18,243 25,265
Year 2000 2001 2002 2003
Sales, y 31,888 31,168 35,404 41,444
Year 2004 2005 2006 2007
Sales, y 49,205 55,908 58,200 61,000
42. The table shows the net profits (in millions of dollars) for Polo
Ralph Lauren during the years 1996 to 2007. Find the least
squares regression line and the least squares cubic regression
polynomial for the data. Let represent the year, with
corresponding to 1996. Which model is the better fit for the
data? Why? (Source: Polo Ralph Lauren)
Year 1996 1997 1998 1999
Net Profit, y 81.3 120.1 125.3 147.5
Year 2000 2001 2002 2003
Net Profit, y 166.3 168.6 183.7 184.4
Year 2004 2005 2006 2007
Net Profit, y 257.2 308.0 385.0 415.0
True or False? In Exercises 43 and 44, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
43. (a) If and are orthogonal subspaces of then their
intersection is an empty set.
(b) If each vector can be uniquely written as a sum of a
vector from and a vector from then is called
the direct sum of and
(c) The solution of the least squares problem consists essentially of solving the normal equations—that is, solving the
linear system of equations
44. (a) If is an matrix, then and are orthogonal subspaces of
(b) The set of all vectors orthogonal to every vector in a
subspace is called the orthogonal complement of
(c) Given an matrix and a vector in the least
squares problem is to find in such that is
minimized.
45. Prove that if and are orthogonal subspaces of then their
intersection consists only of the zero vector.
46. Prove that the orthogonal complement of a subspace of is
itself a subspace of
47. Prove Theorem 5.14.
48. Prove that if and are subspaces of and if
then
49. Writing Describe the normal equations for the least squares
problem if the matrix has orthonormal columns.



In this chapter you will learn about functions that map a vector space into a vector space
This type of function is denoted by
The standard function terminology is used for such functions. For instance, is called the
domain of and is called the codomain of If is in and is in such that
then is called the image of under The set of all images of vectors in is called the
range of and the set of all in such that is called the preimage of (See
Figure 6.1 on the next page.)
T, v V Tv  w w.
w v T. V
Tv  w,
, WT T. v V w W
V
T: V→W.
W.
V
6.1
362 Chapter 6 Linear Transformations
Figure 6.1
For any vector in let be defined by
(a) Find the image of
(b) Find the preimage of
SOLUTION (a) For you have
(b) If then
This system of equations has the unique solution and So, the preimage
of is the set in consisting of the single vector
This chapter centers on functions (from one vector space to another) that preserve the
operations of vector addition and scalar multiplication. Such functions are called linear
transformations.
A linear transformation is said to be operation preserving, because the same result occurs
whether the operations of addition and scalar multiplication are performed before or after
the linear transformation is applied. Although the same symbols are used to denote the
R 3, 4. 2 1, 11
v v 2  4. 1  3
v1
v1


v2
2v2


1
11.
Tv  v1  v2, v1  2v2  1, 11,
T1, 2  1  2, 1  22  3, 3.
v  1, 2
w  1, 11.
v  1, 2.
Tv1, v2  v1  v2, v1  2v2.
T: R2→R2 R2 v  v , 1, v2
EXAMPLE 1 A Function from R2 into R2
T: V → W W: Codomain
T
v
w
Range
REMARK : For a vector V: Domain
in it
would be technically correct to
use double parentheses to denote
as
For convenience, however, one
set of parentheses is dropped,
producing
Tv  Tv1, v2, . . . ,vn.
Tv Tv1, v2, . . . , vn Tv .
Rn v  v , 1, v2, . . . , vn
Let and be vector spaces. The function is called a linear transformation of
into if the following two properties are true for all and in and for any scalar
1.
2. Tcu  cTu
Tu  v  Tu  Tv
WV vu V c.
Definition of a WV T: V→W
Linear Transformation
Section 6.1 Introduction to Linear Transformations 363
vector operations in both and you should note that the operations may be different, as
indicated in the diagram below.
Show that the function given in Example 1 is a linear transformation from into
SOLUTION To show that the function is a linear transformation, you must show that it preserves
vector addition and scalar multiplication. To do this, let and be
vectors in and let be any real number. Then, using the properties of vector addition and
scalar multiplication, you have the two statements below.
1. Because you have
2. Because , you have
So, is a linear transformation.
Most of the common functions studied in calculus are not linear transformations.
(a) is not a linear transformation from into because, in general,
For instance, sin
2  
3  sin
2  sin
3.
sinx1  x2  sin x1  sin x2.
fx  sin x RR
EXAMPLE 3 Some Functions That Are Not Linear Transformations
T
 cTu.
 cu1  u2, u1  2u2
 cu1  cu2, cu1  2cu2
Tcu  Tcu1, cu2
cu  cu1, u2  cu1, cu2
 Tu  Tv.
 u1  u2, u1  2u2  v1  v2, v1  2v2
 u1  u2  v1  v2,u1  2u2  v1  2v2
 u1  v1  u2  v2, u1  v1  2u2  v2
Tu  v  Tu1  v1, u2  v2
u  v  u1, u2  v1, v2  u1  v1, u2  v2,
cR2
u  u1, u2 v  v  1, v2
T
Tv1, v2  v1  v2, v1  2v2
R2 R . 2
EXAMPLE 2 Verifying a Linear Transformation from R2 into R2
Tu  v  Tu  Tv Tcu  cTu
Scalar
multiplication
in W
Scalar
multiplication
in V
Addition
in W
Addition
in V
V W,
REMARK : A linear transformation from a vector
space into itself (as in Example
2) is called a linear operator.
T: V→V
364 Chapter 6 Linear Transformations
(b) is not a linear transformation from into because, in general,
For instance,
(c) is not a linear transformation from into because
whereas
So
REMARK : The function in Example 3(c) points out two uses of the term linear. In calculus, is called a linear function because its graph is a line. It is not a linear
transformation from the vector space into however, because it preserves neither vector
addition nor scalar multiplication.
Two simple linear transformations are the zero transformation and the identity
transformation, which are defined as follows.
1. for all Zero transformation
2. for all Identity transformation
The verifications of the linearity of these two transformations are left as exercises. (See
Exercises 68 and 69.)
Note that the linear transformation in Example 2 has the property that the zero vector is
mapped to itself. That is, . (Try checking this.) This property is true for all linear
transformations, as stated in the next theorem.
T0  0
Tv  v, v T: V→V
Tv  0, v T: V→W
R R,
fx  x  1
fx1  x2  fx1  fx2.
fx1  fx2  x1  1  x2  1  x1  x2  2.
fx1  x2  x1  x2  1
fx  x  1 RR
1  22  12  22
.
x1  x22  x1
2  x2
2
.
fx  x RR 2
Let be a linear transformation from into where and are in Then the
following properties are true.
1.
2.
3.
4. If
then
 c1Tv1  c2Tv2  . . .  cnTvn.
Tv  Tc1v1  c2v2  . . .  cnvn
v  c1v1  c2v2  . . .  cnvn,
Tu  v  Tu  Tv
Tv  Tv
T0  0
THEOREM 6.1 T V W, vu V.
Properties of
Linear Transformations
Section 6.1 Introduction to Linear Transformations 365
PROOF To prove the first property, note that . Then it follows that
The second property follows from which implies that
.
The third property follows from which implies that
.
The proof of the fourth property is left to you.
Property 4 of Theorem 6.1 tells you that a linear transformation is determined
completely by its action on a basis of In other words, if is a basis for
the vector space and if are given, then is determined for
any in The use of this property is demonstrated in Example 4.
Let be a linear transformation such that
Find
SOLUTION Because can be written as
,
you can use Property 4 of Theorem 6.1 to write
Another advantage of Theorem 6.1 is that it provides a quick way to spot functions that
are not linear transformations. That is, because all four conditions of the theorem must be
true of a linear transformation, it follows that if any one of the properties is not satisfied for
a function then the function is not a linear transformation. For example, the function
is not a linear transformation from to because R T0, 0  0, 0. 2 R2
Tx1, x2  x1  1, x2
T,
 7, 7, 0.
 22, 1, 4  31, 5, 2  20, 3, 1
T2, 3, 2  2T1, 0, 0  3T0, 1, 0  2T0, 0, 1
2, 3, 2  21, 0, 0  30, 1, 0  20, 0, 1
2, 3, 2
T2, 3, 2.
T0, 0, 1  0, 3, 1.
T0, 1, 0  1, 5, 2
T1, 0, 0  2, 1, 4
T: R3→R3
EXAMPLE 4 Linear Transformations and Bases
v V.
Tv Tv 1, Tv2, . . . , Tvn V
v1, v2, . . . , vn V. 
T: V→W
Tu  v  T u  1v  Tu  1Tv  Tu  Tv
u  v  u  v,
Tv  T 1v  1Tv  Tv
v  1v,
T0  T0v  0Tv  0.
0v  0
366 Chapter 6 Linear Transformations
In the next example, a matrix is used to define a linear transformation from into .
The vector is written in the matrix form
so it can be multiplied on the left by a matrix of order
The function is defined as follows.
(a) Find where
(b) Show that is a linear transformation from into
SOLUTION (a) Because you have
So, you have
(b) Begin by observing that does map a vector in to a vector in To show that is
a linear transformation, use the properties of matrix multiplication, as shown in
Theorem 2.3. For any vectors and in the distributive property of matrix
multiplication over addition produces
Similarly, for any vector in and any scalar the commutative property of scalar
multiplication with matrix multiplication produces
Example 5 illustrates an important result regarding the representation of linear transformations from into This result is presented in two stages. Theorem 6.2 on the next
page states that every matrix represents a linear transformation from into
Then, in Section 6.3, you will see the converse—that every linear transformation from
into can be represented by an matrix. R m  n m
Rn
Rm R . m  n n
Rm R . n
Tcu  Acu  cAu  cTu.
R c, 2 u
Tu  v  Au  v  Au  Av  Tu  Tv.
R2 vu ,
R T 3 R . 2 T
T2, 1  6, 3, 0.
Vector
in R3 Vector
in R2
Tv  Av   3
2
1
0
1
2
  2
1  
6
3
0

.
v  2, 1,
R3 R . 2 T
Tv, v  2, 1.
Tv  Av   3
2
1
0
1
2
 
v1
v2

T: R2→R3
EXAMPLE 5 A Linear Transformation Defined by a Matrix
3  2.
v  
v1
v2
,
v  v1, v2
R3 R2
Section 6.1 Introduction to Linear Transformations 367
Note in part (b) of Example 5 that no reference is made to the specific matrix This
verification serves as a general proof that the function defined by any matrix is a
linear transformation from into
REMARK : The zero matrix corresponds to the zero transformation from
into and the identity matrix corresponds to the identity transformation from
into
Be sure you see that an matrix defines a linear transformation from into
The linear transformation is defined by Find the dimensions of
and for the linear transformation represented by each matrix.
(a)
(b)
(c) A  
1
3
0
1
1
0
2
0
A   2
5
0
3
0
2

A  
0
2
4
1
3
2
1
0
1

Rm
Rn T: R Tv  Av. n→Rm
EXAMPLE 6 Linear Transformation Given by Matrices
Vector
in Rm Vector
in Rn

a11v1
a21v1 .
.
. am1v1



a12v2
a22v2 .
.
. am2v2
 ... 
 ... 
 ... 
a1nvn
a2nvn .
.
. amnvn
  v1
v2
.
.
.
vn
 Av 

a11
a21
.
.
.
am1
a12
a22
.
.
.
am2
. . .
. . .
. . .
a1n
a2n
.
.
.
amn

Rm R . n m  n A
Rn.
Rn I R n  n n m,
R m  n n
Rm R . n
m  n
A.
Let be an matrix. The function defined by
is a linear transformation from into In order to conform to matrix multiplication
with an matrix, the vectors in are represented by matrices and the vectors
in are represented by matrices. R m  1 m
R n  1 m  n n
Rm R . n
Tv  Av
THEOREM 6.2 A m  n T
The Linear Transformation
Given by a Matrix
368 Chapter 6 Linear Transformations
SOLUTION (a) Because the size of this matrix is it defines a linear transformation from
into
(b) Because the size of this matrix is it defines a linear transformation from
into
(c) Because the size of this matrix is it defines a linear transformation from
into
In the next example, a common type of linear transformation from into is discussed.
Show that the linear transformation represented by the matrix
has the property that it rotates every vector in counterclockwise about the origin through
the angle
SOLUTION From Theorem 6.2, you know that is a linear transformation. To show that it rotates every
vector in counterclockwise through the angle let be a vector in Using
polar coordinates, you can write as
where is the length of and is the angle from the positive -axis counterclockwise to
the vector Now, applying the linear transformation to produces
 
r cos(  
)
r sin(  
)
.
 
r cos  cos 
  r sin  sin 

r sin  cos 
  r cos  sin 

 
cos 
sin 
sin 
cos r cos 

r sin 

Tv  Av  
cos 
sin 
sin 
cos x
y
v. T v
r v 
 x
v  x, y  r cos 
, r sin 
,
v
R2 R , v  x, y . 2
T
.
R2
A  
cos 
sin 
sin 
cos 
T: R2→R2
EXAMPLE 7 Rotation in the Plane
R2 R2
R2.
R4 2  4,
R3
.
R2 3  2,
Av  
0
2
4
1
3
2
1
0
1
 v1
v2
v3
  
u1
u2
u3

R3.
R3 3  3,
Vector
in R3 Vector
in R3
Figure 6.2
Rotation in the Plane
x
θ
α
y
T(x, y)
(x, y)
Section 6.1 Introduction to Linear Transformations 369
So, the vector has the same length as Furthermore, because the angle from the
positive -axis to is is the vector that results from rotating the vector
counterclockwise through the angle as shown in Figure 6.2 on the previous page.
REMARK : The linear transformation in Example 7 is called a rotation in Rotations
in preserve both vector length and the angle between two vectors. That is, the angle
between and is equal to the angle between and
The linear transformation represented by
is called a projection in If is a vector into then In
other words, maps every vector in to its orthogonal projection in the -plane, as
shown in Figure 6.3.
So far only linear transformations from into or from into have been
discussed. In the remainder of this section, some linear transformations involving vector
spaces other than will be considered.
Let be the function that maps an matrix to its transpose. That is,
Show that is a linear transformation.
SOLUTION Let and be matrices. From Theorem 2.6 you have
and
So, is a linear transformation from into Mn,m M . T m,n
 cTA.
 cAT
TcA  (cAT
 TA  TB
 AT  BT
TA  B  A  BT
m  nBA
T
TA  AT.
T: Mm,n→Mn,m m  n A
EXAMPLE 9 A Linear Transformation from Mm,n into Mn,m
Rn
Rn Rn Rm Rn
R xy 3 T
R Tv  x, y, 0. 3 R v  x, y, z , 3
.
A  
1
0
0
0
1
0
0
0
0

T: R3→R3
EXAMPLE 8 A Projection in R3
vu Tu Tv.
R2
R2.
,
x Tv   
 , Tv v
Tv v.
Figure 6.3
x y
z
(x, y, z)
T(x, y, z) = (x, y, 0)
Projection onto xy-plane
370 Chapter 6 Linear Transformations
Let be the set of all functions whose derivatives are continuous on Show
that the differential operator defines a linear transformation from into
SOLUTION Using operator notation, you can write
where is in To show that is a linear transformation, you must use calculus.
Specifically, because the derivative of the sum of two functions is equal to the sum of their
derivatives and because the sum of two continuous functions is continuous, you have
Similarly, because the derivative of a scalar multiple of a function is equal to the scalar
multiple of the derivative and because the scalar multiple of a continuous function is
continuous, you have
So, is a linear transformation from into
The linear transformation in Example 10 is called the differential operator. For
polynomials, the differential operator is a linear transformation from into because
the derivative of a polynomial function of degree is a polynomial function of degree
or less. That is,
The next example describes a linear transformation from the vector space of polynomial
functions into the vector space of real numbers
Let be defined by
Show that is a linear transformation from the vector space of polynomial functions,
into the vector space of real numbers. R,
T P,
Tp  
b
a
px dx.
T: P→R
EXAMPLE 11 The Definite Integral as a Linear Transformation (Calculus)
P R.
Dxanxn  . . .  a1x  a0  nanxn1  . . .  a1.
n  1
n
Pn Pn1
Dx
D Ca, b Ca, b. x
 cDx f .
Dxcf  d
dx cf   c	
d
dx  f 

 Dx f   Dxg.
Dx f  g  d
dx  f  g  d
dx  f 
d
dx g
C Dx f  a, b.
Dx f  d
dx f,
D Ca, b Ca, b. x
C a, b a, b.
EXAMPLE 10 The Differential Operator (Calculus)
Section 6.1 Introduction to Linear Transformations 371
SOLUTION Using properties of definite integrals, you can write
and
So, is a linear transformation. T
Tcp  
b
a
c px dx  c
b
a
px dx  cT p.
 Tp  Tq
 
b
a
px dx  
b
a
qx dx
Tp  q  
b
a
 px  qx dx
SECTION 6.1 Exercises
In Exercises 1–8, use the function to find (a) the image of and
(b) the preimage of
1.
2.
3.
4.
5.
6.
7.
8.
In Exercises 9–22, determine whether the function is a linear
transformation.
9.
10.
11.
12.
13.
14.
15.
16. where
17.
18.
19.
20.
21.
22.
In Exercises 23–26, let be a linear transformation
such that and
Find
23. 24.
25. 26. T2, 4, 1. T2, 4, 1.
T0, 3, 1. T2, 1, 0.
T0, 0, 1  0, 2, 2.
T1, 0, 0  2, 4, 1, T0, 1, 0  1, 3, 2,
T: R3 → R3
T: P2→P2, Ta0  a1x  a2x2  a1  2a2x
a0  a1  a2  a1  a2x  a2 x2
T: P2→P2, Ta0  a1x  a2 x2
T: M2,2→M2,2, TA  A1
T: M2,2→M2,2, TA  AT
T: M3,3→M3,3, TA  
1
0
0
0
1
0
0
0
1A
T: M3,3→M3,3, TA  
0
0
1
0
1
0
1
0
0A
A  
a
c
b
d T: M . 2,2→R, TA  a  b  c  d,
T: M2,2→R, TA  A
T: R2→R3, Tx, y  x2
, xy, y2
T: R2→R3, Tx, y  x, xy, y
T: R3→R3
, Tx, y, z  x  1, y  1, z  1
T: R3→R3, Tx, y, z  x  y, x  y, z
T: R2→R2, Tx, y  x2, y
T: R2→R2
, Tx, y  x, 1
v  2, 4, w  3, 2, 0
Tv1, v2   	
3
2 v1  1
2
v2, v1  v2, v2
,
v  1, 1, w  52, 2, 16
Tv1, v2   	
2
2 v1  2
2 v2, v1  v2, 2v1  v2
,
w  1, 2
Tv1, v2, v3  2v1  v2, v1  v2 , v  2, 1, 4,
w  3, 9
Tv1, v2, v3  4v2  v1, 4v1  5v2 , v  2, 3, 1,
v  4, 5, 1, w  4, 1, 1
Tv1, v2, v3  2v1  v2, 2v2  3v1, v1  v3,
w  11, 1, 10
Tv1, v2, v3  v2  v1, v1  v2, 2v1, v  2, 3, 0,
Tv1, v2   2v2  v1, v1, v2 , v  0, 6, w  3, 1, 2
Tv1, v2   v1  v2, v1  v2 , v  3, 4, w  3, 19
w.
v
372 Chapter 6 Linear Transformations
In Exercises 27–30, let be a linear transformation
such that and
Find
27. 28.
29. 30.
In Exercises 31–35, the linear transformation is defined
by Find the dimensions of and
31.
32.
33.
34.
35.
36. For the linear transformation from Exercise 31, find
(a) and (b) the preimage of
37. Writing For the linear transformation from Exercise 32,
find (a) and (b) the preimage of (c) Then
explain why the vector has no preimage under this
transformation.
38. For the linear transformation from Exercise 33, find
(a) and (b) the preimage of
39. For the linear transformation from Exercise 34, find
(a) and (b) the preimage of
40. For the linear transformation from Exercise 35, find
(a) (b) the preimage of and (c) the preimage of
41. Let be the linear transformation from into represented
by
Find (a) for (b) for and
(c) for
42. For the linear transformation from Exercise 41, let and
find the preimage of
In Exercises 43–46, let be the linear transformation from
into from Example 10. Decide whether each
statement is true or false. Explain your reasoning.
43.
44.
45.
46.
Calculus In Exercises 47–50, for the linear transformation from
Example 10, find the preimage of each function.
47. 48.
49. 50.
51. Calculus Let be the linear transformation from into
shown by
Find (a) (b) and (c)
52. Calculus Let be the linear transformation from into
represented by the integral in Exercise 51. Find the preimage of
1. That is, find the polynomial function(s) of degree 2 or less
such that
53. Let be a linear transformation from into such that
and Find and
54. Let be a linear transformation from into such that
and Find and
55. Let be a linear transformation from into such
that and Find
56. Let be a linear transformation from into such that
Find T 	 1
1
3
4
.
T 	
0
0
0
1
  
3
1
1
0 T . 	
0
1
0
0
  
1
0
2
1,
T 	
0
0
1
0
  
0
1
2
1 T , 	
1
0
0
0
  
1
0
1
2,
T M2,2 M2,2
T2  6x  x2.
Tx2  1  x  x2 T1  x, Tx  1  x, .
T P2 P2
T1, 0  1, 1 T0, 1  1, 1. T1, 4 T2, 1.
R2 R2 T
T1, 1  1, 0 T1, 1  0, 1. T1, 0 T0, 2.
R2 R2 T
T p  1.
T 2 RP
Tx T4x  6. 3  x5 T3x , 2  2,
T p  
1
0
px dx.
T RP
fx  1
x fx  sin x
fx  ex fx  2x  1
Dx	cos x
2
  1
2
Dxcos x
Dxsin 2x  2Dxsin x
Dxx2  ln x  Dxx2  Dxln x
Dxex 2
 2x  Dxex 2
  2Dxx
Ca, b Ca, b
Dx
v  1, 1.
  45
T5, 0   120.
T4, 4   45, T4, 4   30,
Tx, y  x cos   y sin , x sin   y cos .
R2 R2 T
0, 0.
T1, 1, 1, 1,
T1, 1, 1, 1 1, 1, 1, 1.
T1, 0, 1, 3, 0 1, 8.
1, 1, 1
T2, 4 1, 2, 2.
T1, 0, 2, 3 0, 0, 0.
A   0
1
1
0
A

1
0
0
0
0
1
0
0
0
0
2
0
0
0
0
1

A  
1
0
2
0
1
2
3
1
4
0
A   1
2
2
2
4
2

A   0
1
0
1
4
1
2
5
3
1
0
1

Rm R . n Tv  Av.
T: Rn→Rm
T2, 1, 1. T2, 1, 0.
T2, 1, 0. T0, 2, 1.
T1, 0, 1  1, 1, 0.
T1, 1, 1  2, 0, 1, T0, 1, 2  3, 2, 1,
T: R3 → R3
Section 6.1 Introduction to Linear Transformations 373
True or False? In Exercises 57 and 58, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
57. (a) Linear transformations are functions from one vector space
to another that preserve the operations of vector addition and
scalar multiplication.
(b) The function is a linear transformation from
into
(c) For polynomials, the differential operator is a linear
transformation from into
58. (a) A linear transformation is operation preserving if the
same result occurs whether the operations of addition and
scalar multiplication are performed before or after the linear
transformation is applied.
(b) The function is a linear transformation from
into
(c) Any linear function of the form is a linear
transformation from into
59. Writing Suppose such that and
(a) Determine for in
(b) Give a geometric description of
60. Writing Suppose such that and
(a) Determine for in
(b) Give a geometric description of T.
61. Let be the function from into such that
where
(a) Find (b) Find
(c) Prove that for every and
in
(d) Prove that for every in This result
and the result in part (c) prove that is a linear
transformation from into
62. Writing Find and from Exercise 61 and give
geometric descriptions of the results.
63. Show that from Exercise 61 is represented by the matrix
64. Use the concept of a fixed point of a linear transformation
A vector is a fixed point if
(a) Prove that 0 is a fixed point of any linear transformation
(b) Prove that the set of fixed points of a linear
transformation is a subspace of
(c) Determine all fixed points of the linear transformation
represented by
(d) Determine all fixed points of the linear transformation
represented by
65. A translation is a function of the form
where at least one of the constants and is
nonzero.
(a) Show that a translation in the plane is not a linear
transformation.
(b) For the translation determine
the images of and
(c) Show that a translation in the plane has no fixed points.
66. Let be a set of linearly dependent
vectors in and let be a linear transformation from into
Prove that the set
is linearly dependent.
67. Let be a set of linearly independent vectors in
Find a linear transformation from into such that the
set is linearly dependent.
68. Prove that the zero transformation is a linear
transformation.
69. Prove that the identity transformation is a linear
transformation.
70. Let be an inner product space. For a fixed vector in
define by Prove that is a linear
transformation.
71. Let be defined by
(the trace of ). Prove that is a linear transformation.
72. Let be an inner product space with a subspace having
as an orthonormal basis. Show that the
function represented by
is a linear transformation. is called the orthogonal projection
of onto V W.
T
Tv  v, w1w1  v, w2w2  . . .  v, wnwn
T: V→W
B  w1, w2, . . . , wn

V W
A T
a T nn A  a11  a T: Mn,n→R 22  . . . 
T: V→R Tv  v, v0. T
v V, V 0
T: V→V
T: V→W
Tv1, Tv2, Tv3
R3 R3 R T 3.
S  v1, v2, v3

Tv1,Tv2, . . . , Tvn
, TV V V.
S  v1, v2, . . . , vn

0, 0, 2, 1, 5, 4.
Tx, y  x  2, y  1,
x  h, y  k, kh
Tx, y
T: R Tx, y   y, x. 2→R2
T: R Tx, y  x, 2y. 2→R2
T: V→V V.
T: V→V.
T: V→V. u Tu  u.
A  
1
2
1
2
1
2
1
2
.
T
T3, 4 TT3, 4
R2 R . 2
T
R2 Tcu  cTu u .
R2.
Tu  w  Tu  Tw wu
Tx, y. T5, 0.
v  1, 1.
Tu  proj R vu, 2 R2 T
R2 Tx, y x, y .
T0, 1  1, 0.
T: R T1, 0  0, 1 2→R2
T.
R2 Tx, y x, y .
T0, 1  0, 0.
T: R T1, 0  1, 0 2→R2
R R.
fx  ax  b
R R.
gx  x3
Pn1 P . n
Dx
R.
fx  cos x R
374 Chapter 6 Linear Transformations
73. Guided Proof Let be a basis for a vector
space Prove that if a linear transformation
satisfies for then T is the zero
transformation.
Getting Started: To prove that is the zero transformation,
you need to show that for every vector in
(i) Let be an arbitrary vector in such that
(ii) Use the definition and properties of linear transformations to rewrite as a linear combination of
(iii) Use the fact that to conclude that ,
making the zero transformation.
74. Guided Proof Prove that is a linear transformation
if and only if for all vectors
and and all scalars and
Getting Started: Because this is an “if and only if” statement,
you need to prove the statement in both directions. To prove that
is a linear transformation, you need to show that the function
satisfies the definition of a linear transformation. In the other
direction, suppose is a linear transformation. You can use the
definition and properties of a linear transformation to prove that
(i) Suppose Show that
preserves the properties of vector addition and scalar
multiplication by choosing appropriate values of a
and b.
(ii) To prove the statement in the other direction, assume
that is a linear transformation. Use the properties and
definition of a linear transformation to show that
Tau  bv  aTu  bTv.
T
Tau  bv  aTu  bTv. T
Tau  bv  aTu  bTv.
T
T
v a b.
Tau  bv  aTu  bTv u
T: V→W
T
Tvi  0 Tv  0
Tvi Tv .
c1v1  c2v2  . . .  cnvn v  .
v V
Tv  0 v V.
T
Tv i  1, 2, . . . , n, i  0
V. T: V→V
v1, v2, . . . , vn

The Kernel and Range of a Linear Transformation
You know from Theorem 6.1 that for any linear transformation the zero vector
in is mapped to the zero vector in That is,
The first question you will consider in this section is whether there are other vectors such
that The collection of all such elements is called the kernel of Note that the
symbol is used to represent the zero vector in both and although these two zero
vectors are often different.
Sometimes the kernel of a transformation is obvious and can be found by inspection, as
demonstrated in Examples 1, 2, and 3.
Let be the linear transformation that maps a matrix to its transpose.
That is,
Find the kernel of T.
TA  AT.
T: M3,2→M2,3 3  2 A
EXAMPLE 1 Finding the Kernel of a Linear Transformation
0 V W,
Tv  0. T.
v
T0  0.
V W.
T: V→W,
6.2
Let be a linear transformation. Then the set of all vectors in that satisfy
Tv  0 is called the kernel of and is denoted by T kerT.
Definition of Kernel of a T: V→W v V
Linear Transformation
Section 6.2 The Kernel and Range of a Linear Transformation 375
SOLUTION For this linear transformation, the zero matrix is clearly the only matrix in whose
transpose is the zero matrix in
Zero Matrix in Zero Matrix in
So, the kernel of consists of a single element: the zero matrix in
(a) The kernel of the zero transformation consists of all of because
for every in That is,
(b) The kernel of the identity transformation consists of the single element .
That is,
Find the kernel of the projection represented by
SOLUTION This linear transformation projects the vector in to the vector in the
-plane. The kernel consists of all vectors lying on the -axis. That is,
is a real number (See Figure 6.4.)
Figure 6.4
x y
(0, 0, 0)
z
(0, 0, z)
(x, y, 0)
T(x, y, z) =
The kernel of T is the set
of all vectors on the z-axis.
kerT  0, 0, z: z .
xy z
R x, y, 0 3 x, y, z
Tx, y, z  x, y, 0.
T: R3→R3
EXAMPLE 3 Finding the Kernel of a Linear Transformation
kerT  0.
T: V→V 0
v V. kerT  V.
T: V→W V Tv  0
EXAMPLE 2 The Kernels of the Zero and Identity Transformations
M3,2 T .
0  
0
0
0
0
0
0
 0  
0
0
0
0
0
0
M3,2 M2,3
M2,3.
M3,2 3  2
376 Chapter 6 Linear Transformations
Finding the kernels of the linear transformations in Examples 1, 2, and 3 was fairly easy.
Generally, the kernel of a linear transformation is not so obvious, and finding it requires a
little work, as illustrated in the next two examples.
Find the kernel of the linear transformation represented by
SOLUTION To find you need to find all in such that
This leads to the homogeneous system
which has only the trivial solution So, you have
Find the kernel of the linear transformation defined by where
SOLUTION The kernel of is the set of all in such that
From this equation you can write the homogeneous system
Writing the augmented matrix of this system in reduced row-echelon form produces
Using the parameter produces the family of solutions

x1
x2
x3
   t
t
t
  t
 1
1
1

.
t  x3
x1
x2


x3
x3. 
1
0
0
1
1
1
0
0
x1
x1


x2
2x2


2x3
3x3


0
0 .  1
1
1
2
2
3 
x1
x2
x3
  
0
0
Tx1, x2, x3  0, 0.
R3 x  x1, x2, x3 T
A   1
1
1
2
2
3.
T: R Tx  Ax, 3→R2
EXAMPLE 5 Finding the Kernel of a Linear Transformation
kerT  0, 0  0.
x1, x2  0, 0.
x1
x1
 2x2
0



0
0
0,
Tx1, x2  x1  2x2, 0, x1  0, 0, 0.
R2 x  x1, x2 kerT,
Tx1, x2  x1  2x2, 0, x1.
T: R2→R3
EXAMPLE 4 Finding the Kernel of a Linear Transformation
Section 6.2 The Kernel and Range of a Linear Transformation 377
So, the kernel of is represented by
Note that in Example 5 the kernel of contains an infinite number of vectors. Of course,
the zero vector is in but the kernel also contains such nonzero vectors as
and as shown in Figure 6.5. Figure 6.5 shows that this particular kernel is a line
passing through the origin, which implies that it is a subspace of In Theorem 6.3 you
will now see that the kernel of every linear transformation is a subspace of
PROOF From Theorem 6.1 you know that is a nonempty subset of So, by Theorem 4.5, you
can show that is a subspace of by showing that it is closed under vector addition
and scalar multiplication. To do so, let and be vectors in the kernel of Then
which implies that is in the kernel. Moreover, if is any scalar, then
which implies that is in the kernel.
The next example shows how to find a basis for the kernel of a transformation defined
by a matrix.
Let be defined by where is in and
Find a basis for as a subspace of R5 kerT .
A

1
2
1
0
2
1
0
0
0
3
2
0
1
1
0
2
1
0
1
8

.
R5 T: R Tx  Ax, x 5→R4
EXAMPLE 6 Finding a Basis for the Kernel
cu
 0,
 c0
Tcu  cTu
u  v c
 0,
 0  0
Tu  v  Tu  Tv
vu T.
kerT V
kerT V.
T: V→W V.
R3.
2, 2, 2,
kerT, 1, 1, 1
T
 span1, 1, 1.
kerT  t1, 1, 1: t is a real number
T
Figure 6.5
y
x
1
1
2
3
2
3
z
−2 2 3
(2, −2, 2)
(1, −1, 1)
Kernel:
t(1, −1, 1)
The kernel of a linear transformation is a subspace of the domain T: V→W V.
THEOREM 6.3
The Kernel Is a
Subspace of V
REMARK : As a result of
Theorem 6.3, the kernel of is
sometimes called the nullspace
of T.
T
What is the rank of the matrix A
in Example 6? Formulate a
conjecture relating the dimension
of the kernel, the rank, and the
number of columns of A. Verify
your conjecture for the matrix in
Example 5.
378 Chapter 6 Linear Transformations
SOLUTION Using the procedure shown in Example 5, reduce the augmented matrix to echelon
form as follows.
Letting and you have
So one basis for the kernel of is
In the solution of Example 6, a basis for the kernel of was found by solving the
homogeneous system represented by This procedure is a familiar one—it is the
same procedure used to find the solution space of In other words, the kernel of
is the nullspace of the matrix as shown in the following corollary to Theorem 6.3.
The Range of a Linear Transformation
The kernel is one of two critical subspaces associated with a linear transformation. The
second is the range of denoted by range Recall from Section 6.1 that the range
of is the set of all vectors in that are images of vectors in That is,
range
PROOF The range of is nonempty because implies that the range contains the zero
vector. To show that it is closed under vector addition, let and be vectors in the
range of Because and are in it follows that is also in So, the sum
Tu  Tv  Tu  v is in the range of T.
T. vu V, u  v V.
Tu Tv
T T0  0
T  Tv: v is in V.
T: V→W w W V.
T, T.
A,
Ax  0. T
Ax  0.
T
B  2, 1, 1, 0, 0, 1, 2, 0, 4, 1.
T
x

x1
x2
x3
x4
x5


2s
s
s
0s
0s





t
2t
0t
4t
t
  s

2
1
1
0
0
  t
 1
2
0
4
1

.
x x 5  t, 3  s
x1  2x3  x5
x2  x3  2x5
x4   4x5 
1
0
0
0
0
1
0
0
2
1
0
0
0
0
1
0
1
2
4
0
0
0
0
0

A  0
Discovery
Let be the linear transformation given by Then the kernel of is
equal to the solution space of Ax  0.
T: R Tx  Ax. T n→R COROLLARY TO m
THEOREM 6.3
The range of a linear transformation is a subspace of T: V→W W.
THEOREM 6.4
The Range of T
Is a Subspace of W
Section 6.2 The Kernel and Range of a Linear Transformation 379
To show closure under scalar multiplication, let be a vector in the range of and
let be a scalar. Because is in it follows that is also in So, the scalar multiple
is in the range of
Note that the kernel and range of a linear transformation are subspaces of
and respectively, as illustrated in Figure 6.6.
To find a basis for the range of a linear transformation defined by observe
that the range consists of all vectors such that the system is consistent. By writing the system
in the form
you can see that is in the range of if and only if is a linear combination of the column
vectors of So the column space of the matrix is the same as the range of
In Example 4 in Section 4.6, you saw two procedures for finding a basis for the column
space of a matrix. In the next example, the second procedure from Example 4 in Section
4.6 will be used to find a basis for the range of a linear transformation defined by a matrix.
For the linear transformation from Example 6, find a basis for the range of
SOLUTION The echelon form of was calculated in Example 6.

1
0
0
0
0
1
0
0
2
1
0
0
0
0
1
0
1
2
4
0
 A  ⇒

1
2
1
0
2
1
0
0
0
3
2
0
1
1
0
2
1
0
1
8

A
R T. 5→R4
EXAMPLE 7 Finding a Basis for the Range of a Linear Transformation
A. A T.
b T b
Ax  x1

a11
a21
.
.
.
am1
  x2

a12
a22
.
.
.
am2
  ...  xn

a1n
a2n
.
.
.
amn


b1
b2
.
.
.
bm
  b

a11
a21
.
.
.
am1
a12
a22
.
.
.
am2
...
...
...
a1n
a2n
.
.
.
amn
 x1
x2
.
.
.
xn


b1
b2
.
.
.
bm

b Ax  b
Tx  Ax,
V W,
T: V→W
cTu  Tcu T.
c u V, cu V.
Tu T
Let be the linear transformation given by Then the column space
of is equal to the range of A T.
T: R Tx  Ax. n→R COROLLARY TO m
THEOREM 6.4
Figure 6.6
Range
T
0 W
V
Domain Kernel
380 Chapter 6 Linear Transformations
Let be a linear transformation. The dimension of the kernel of is called the
nullity of and is denoted by nullity The dimension of the range of is called the
rank of and is denoted by T rankT.
T T. T
T: V→W T Definition of Rank
and Nullity of a
Linear Transformation
Because the leading 1’s appear in columns 1, 2, and 4 of the reduced matrix on the right,
the corresponding column vectors of form a basis for the column space of One basis
for the range of is
The following definition gives the dimensions of the kernel and range of a linear
transformation.
REMARK : If is provided by a matrix then the rank of is equal to the rank of as
defined in Section 4.6.
In Examples 6 and 7, the nullity and rank of are related to the dimension of the domain
as follows.
This relationship is true for any linear transformation from a finite-dimensional vector
space, as stated in the next theorem.
PROOF The proof provided here covers the case in which is represented by an matrix The
general case will follow in the next section, where you will see that any linear transformation from an -dimensional space to an -dimensional space can be represented by a matrix.
To prove this theorem, assume that the matrix has a rank of Then you have
From Theorem 4.17, however, you know that
nullityT  dimkernel of T  dimsolution space  n  r.
rankT  dimrange of T  dimcolumn space  rankA  r.
A r.
n m
T m  n A.
rankT  nullityT  3  2  5  dimension of domain
T
T A, T A,
B  1, 2, 1, 0, 2, 1, 0, 0, 1, 1, 0, 2.
T
A A.
Let be a linear transformation from an -dimensional vector space into a
vector space Then the sum of the dimensions of the range and kernel is equal to the
dimension of the domain. That is,
or
dimrange  dimkernel  dimdomain.
rankT  nullityT  n
W.
THEOREM 6.5 T: V→W n V
Sum of Rank
and Nullity
Section 6.2 The Kernel and Range of a Linear Transformation 381
So, it follows that
Find the rank and nullity of the linear transformation defined by the matrix
SOLUTION Because is in row-echelon form and has two nonzero rows, it has a rank of 2. So, the rank
of is 2, and the nullity is
REMARK : One way to visualize the relationship between the rank and the nullity of a
linear transformation provided by a matrix is to observe that the rank is determined by the
number of leading 1’s, and the nullity by the number of free variables (columns without
leading 1’s). Their sum must be the total number of columns of the matrix, which is the
dimension of the domain. In Example 8, the first two columns have leading 1’s, indicating
that the rank is 2. The third column corresponds to a free variable, indicating that the
nullity is 1.
Let be a linear transformation.
(a) Find the dimension of the kernel of if the dimension of the range is 2.
(b) Find the rank of if the nullity of is 4.
(c) Find the rank of if
SOLUTION (a) By Theorem 6.5, with you have
(b) Again by Theorem 6.5, you have
(c) In this case, the nullity of is 0. So
rankT  n  nullityT  5  0  5.
T
rankT  n  nullityT  5  4  1.
dimkernel  n  dimrange  5  2  3.
n  5,
T kerT  0.
T T
T
T: R5→R7
EXAMPLE 9 Finding the Rank and Nullity of a Linear Transformation
T dimdomain  rank  3  2  1.
A
A  
1
0
0
0
1
0
2
1
0

.
T: R3→R3
EXAMPLE 8 Finding the Rank and Nullity of a Linear Transformation
rankT  nullityT  r  n  r  n.
382 Chapter 6 Linear Transformations
One-to-One and Onto Linear Transformations
This section began with a question: How many vectors in the domain of a linear transformation are mapped to the zero vector? Theorem 6.6 (below) shows that if the zero vector
is the only vector such that then is one-to-one. A function is called
one-to-one if the preimage of every in the range consists of a single vector, as shown
in Figure 6.7. This is equivalent to saying that is one-to-one if and only if, for all and
in implies
Figure 6.7
PROOF Suppose is one-to-one. Then can have only one solution: In that case,
Conversely, suppose and Because is a linear
transformation, it follows that
This implies that the vector lies in the kernel of and must equal 0. So,
and and you can conclude that is one-to-one.
(a) The linear transformation represented by is one-to-one
because its kernel consists of only the zero matrix.
(b) The zero transformation is not one-to-one because its kernel is all of R3 T: R . 3→R3
m  n
TA  AT T: Mm,n→Mn,m
EXAMPLE 10 One-to-One and Not One-to-One Linear Transformations
u  v, T
u  v T u  v  0
Tu  v  Tu  Tv  0.
kerT  0. kerT  0 Tu  Tv. T
T Tv  0 v  0.
T
V
W
Not one-to-one
T
V
W
One-to-one
v V, Tu  Tv u  v.
T u
w
v Tv  0, T T: V→W
Let be a linear transformation. Then is one-to-one if and only if T: V→W T kerT  0.
THEOREM 6.6
One-to-One Linear
Transformations
Section 6.2 The Kernel and Range of a Linear Transformation 383
A function is said to be onto if every element in has a preimage in In
other words, is onto when is equal to the range of
For vector spaces of equal dimensions, you can combine the results of Theorems 6.5,
6.6, and 6.7 to obtain the next theorem relating the concepts of one-to-one and onto.
PROOF If is one-to-one, then by Theorem 6.6 and In that case,
Theorem 6.5 produces
Consequently, by Theorem 6.7, is onto. Similarly, if is onto, then
which by Theorem 6.5 implies that By Theorem 6.6, is one-to-one.
The next example brings together several concepts related to the kernel and range of a
linear transformation.
The linear transformation is represented by Find the nullity and
rank of and determine whether is one-to-one, onto, or neither.
(a) (b)
(c) (d) A  
1
0
0
2
1
0
0
1
0
 A  
1
0
2
1
0
1
A  
1
0
0
2
1
0
 A  
1
0
0
2
1
0
0
1
1

T T
T: R Tx  Ax. n→Rm
EXAMPLE 11 Summarizing Several Results
dimkerT  0. T
dimrange of T  dimW  n,
T T
 dimW.
 n
dimrange of T  n  dimkerT
T kerT  0, dimkerT  0.
WWT T.
T: V→W W V.
Let be a linear transformation, where is finite dimensional. Then is onto if
and only if the rank of is equal to the dimension of T W.
T: V→W W T THEOREM 6.7
Onto Linear
Transformations
Let be a linear transformation with vector spaces and both of dimension
Then is one-to-one if and only if it is onto. T
T: V→W WV n. THEOREM 6.8
One-to-One and Onto
Linear Transformations
384 Chapter 6 Linear Transformations
SOLUTION Note that each matrix is already in echelon form, so that its rank can be determined by
inspection.
Dim(range) Dim(kernel)
Dim(domain) Rank Nullity One-to-One Onto
(a) 3 3 0 Yes Yes
(b) 2 2 0 Yes No
(c) 3 2 1 No Yes
(d) 3 2 1 No No
Isomorphisms of Vector Spaces
This section ends with a very important concept that can be a great aid in your understanding of vector spaces. The concept provides a way to think of distinct vector spaces as
being “essentially the same”—at least with respect to the operations of vector addition and
scalar multiplication. For example, the vector spaces and are essentially the same
with respect to their standard operations. Such spaces are said to be isomorphic to each
other. (The Greek word isos means “equal.”)
One way in which isomorphic spaces are “essentially the same” is that they have
the same dimensions, as stated in the next theorem. In fact, the theorem goes even
further, stating that if two vector spaces have the same finite dimension, then they must be
isomorphic.
PROOF Assume is isomorphic to where has dimension By the definition of isomorphic
spaces, you know there exists a linear transformation that is one-to-one and onto.
Because is one-to-one, it follows that which also implies that
In addition, because is onto, you can conclude that
dimrange  dimW  n.
T
dimrange  dimdomain  n.
T dimkernel  0,
T: V→W
V , VW n.
R M3,1 3
T: R3→R3
T: R3→R2
T: R2→R3
T: R3→R3
R T T → m T: Rn
A linear transformation that is one-to-one and onto is called an isomorphism.
Moreover, if and are vector spaces such that there exists an isomorphism from to
, then and are said to be WVW isomorphic to each other.
WV V
Definition of T: V→W
Isomorphism
Two finite-dimensional vector spaces and are isomorphic if and only if they are of
the same dimension.
WV THEOREM 6.9
Isomorphic Spaces
and Dimension
Section 6.2 The Kernel and Range of a Linear Transformation 385
To prove the theorem in the other direction, assume and both have dimension Let
be a basis of and let be a basis of
Then an arbitrary vector in can be represented as
and you can define a linear transformation as follows.
It can be shown that this linear transformation is both one-to-one and onto. So, and are
isomorphic.
Our study of vector spaces has provided much greater coverage to than to other vector
spaces. This preference for stems from its notational convenience and from the geometric models available for and Theorem 6.9 tells you that is a perfect model for
every -dimensional vector space. Example 12 lists some vector spaces that are isomorphic
to
The vector spaces listed below are isomorphic to each other.
(a) -space
(b) space of all matrices
(c) space of all matrices
(d) space of all polynomials of degree 3 or less
(e) is a real number subspace of
Example 12 tells you that the elements in these spaces behave the same way as vectors
even though they are distinct mathematical entities. The convention of using the notation
for an -tuple and an matrix interchangeably is justified. n n  1
R5 x    V  i x1, x2, x3, x4, 0:
P3
M2, 2  2  2
M4,1  4  1
R4  4
EXAMPLE 12 Isomorphic Vector Spaces
R4.
n
Rn R3 R . 2
Rn
Rn
WV
Tv  c1w1  c2w2  ...  cnwn
T: V→W
v  c1v1  c2v2  ...  cnvn,
V
B  w W. 1, w2, . . . , wn B  v V,  1, v2, . . . , vn

WV n.
SECTION 6.2 Exercises
In Exercises 1–10, find the kernel of the linear transformation.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
In Exercises 11–18, the linear transformation is represented by
Find a basis for (a) the kernel of and (b) the range of
11. 12.
13. 14. A  
1
0
2
2
1
1
A    1
0
1
1
2
2
A   1
2
2
4
A    1
3
2
4
Tv  Av. T T.
T
T: R Tx, y  x  y, y  x 2→R2
,
T: R Tx, y  x  2y, y  x 2→R2,
Ta0  a1x  a2x2  a3x3
  a1  2a2x  3a3x2
T: P3→P2,
Ta0  a1x  a2x2  a1  2a2 T: P x 2→P1,
Ta0  a1x  a2x2  a T 0 : P2→R,
Ta0  a1x  a2x2  a3x3 T   a0 : P3→R,
T: R Tx, y, z  z, y, x 3→R3
,
T: R Tx, y, z, w   y, x, w, z 4→R4
,
T: R Tx, y, z  x, 0, z 3→R3
,
T: R Tx, y, z  0, 0, 0 3→R3
,
386 Chapter 6 Linear Transformations
15. 16.
17.
18.
In Exercises 19–30, the linear transformation is defined by
Find (a) (b) nullity (c) range and
(d) rank
19. 20.
21. 22.
23. 24.
25. 26.
27. 28.
29.
30.
In Exercises 31–38, let T: be a linear transformation. Use
the given information to find the nullity of T and give a geometric
description of the kernel and range of T.
31. rank 32. rank
33. rank 34. rank
35. is the counterclockwise rotation of about the -axis:
36. is the reflection through the -coordinate plane:
37. is the projection onto the vector
38. is the projection onto the -coordinate plane:
In Exercises 39–42, find the nullity of
39. T: rank 40. T: rank
41. T: rank 42. T: rank
43. Identify the zero element and standard basis for each of the
isomorphic vector spaces in Example 12.
44. Which vector spaces are isomorphic to
(a) (b) (c)
(d) (e)
(f) is a real number
45. Calculus Let be represented by What is
the kernel of
46. Calculus Let be represented by
What is the kernel of
47. Let be the linear transformation that projects onto
(a) Find the rank and nullity of
(b) Find a basis for the kernel of
48. Repeat Exercise 47 for
In Exercises 49–52, verify that the matrix defines a linear function
that is one-to-one and onto.
49. 50.
51. 52. A   1
1
0
2
2
4
3
4
1
 A  
1
0
0
0
0
1
0
1
0

A  
1
0
0
1
A    1
0
0
1
T
v  3, 0, 4.
T.
T.
v  2, 1, 1.
T: R u 3→R3
T ?
T p  
1
0
px dx.
T: P2→R
T ?
T: P Tp  p. 4→P3
x  i x1, x2, x3, 0, x5, x6, x7:
M6,1 P5
P C0, 6 M2,3 6
R6?
R T  0 P3→P1, T  2 4→R4,
R T  2 5→R2 R T  2 , 4→R2
,
T.
Tx, y, z  x, y, 0
T xy
Tx, y, z  x  2y  2z
9 1, 2, 2
T v  1, 2, 2:
Tx, y, z  x, y, z
T yz
Tx, y, z  	
2
2 x  2
2 y,
2
2 x  2
2 y, z

T 45 z
T  0 T  3
T  2 T  1
R3→R3
A  
3
4
2
2
3
3
6
8
4
1
10
4
15
14
20
A

2
1
3
6
2
1
3
6
3
1
5
2
1
1
0
4
13
1
14
16
A  
1
0
0
0
0
0
0
0
1
 A  
4
9
4
9
2
9
4
9
4
9
2
9
2
9
2
9
1
9

A   1
26
 5
26
 5
26
25
26 A  
9
10
3
10
3
10
1
10
A  
1
0
1
0
0
1
0
1
A    0
4
2
0
3
11
A  
4
0
2
1
0
3
 A  
5
1
1
3
1
1

A   3
9
2
6
A    1
1
1
1
T.
Tx  Ax. kerT, T, T,
T
A  
1
2
2
3
3
1
2
5
2
1
0
1
4
0
0

A

1
3
4
1
2
1
3
2
1
2
1
1
4
1
3
1

A   1
1
0
1
2
1
 A   1
1
1
2
2
1

Section 6.3 Matrices for Linear Transformations 387
True or False? In Exercises 53 and 54, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
53. (a) The set of all vectors mapped from a vector space to
another vector space by a linear transformation is
known as the kernel of
(b) The range of a linear transformation from a vector space
to a vector space is a subspace of the vector space
(c) A linear transformation from to is called one-to-one
if and only if for all and in V, implies that
(d) The vector spaces and are isomorphic to each other.
54. (a) The kernel of a linear transformation from a vector space
to a vector space is a subspace of the vector space
(b) The dimension of a linear transformation from a vector
space to a vector space is called the rank of
(c) A linear transformation from to is one-to-one if
the preimage of every in the range consists of a single
vector
(d) The vector spaces and are isomorphic to each other.
55. For the transformation represented by
what can be said about the rank of if (a) and
(b)
56. Let be represented by Show
that the kernel of is the set of symmetric matrices.
57. Determine a relationship among and such that is
isomorphic to
58. Guided Proof Let be an invertible matrix. Prove that
the linear transformation represented by
is an isomorphism.
Getting Started: To show that the linear transformation is
an isomorphism, you need to show that is both onto and
one-to-one.
(i) Because is a linear transformation with vector spaces
of equal dimension, then by Theorem 6.8, you only need
to show that is one-to-one.
(ii) To show that is one-to-one, you need to determine the
kernel of and show that it is (Theorem 6.6). Use
the fact that is an invertible matrix and that
(iii) Conclude that is an isomorphism.
59. Let be a linear transformation. Prove that is
one-to-one if and only if the rank of equals the dimension
of
60. Let be a linear transformation, and let be a subspace
of Prove that the set is a subspace of What is if
61. Writing Are the vector spaces and exactly the
same? Describe their similarities and differences.
62. Writing Let be a linear transformation. Explain
the differences between the concepts of one-to-one and onto.
What can you say about and if is onto? What can you say
about and if is one-to-one? Tnm
Tnm
T: Rm→Rn
M2,2 M1,4 R , 4
,
T U  0? 1 V. U
T 1 W. U  v 	 V: Tv 	 U
T: V→W U
V.
T
T: V→W T
T
TA  AB.
B n  n
T 0
T
T
T
T
TA  AB
T: Mn, n→Mn, n
B n  n
Mj, k.
Mm,n m, n, j, k
T n  n
TA  A  AT T: M . n,n→Mn,n
detA  0?
T detA  0
T: R Tv  Av, n→Rn
R P1 2
v.
w
WVT
V W T.
T
V W V.
T
R M3,1 3
u  v.
vu Tu  Tv
WVT
W V.
V
T.
W T
V
Matrices for Linear Transformations
Which representation of is better,
or
The second representation is better than the first for at least three reasons: it is simpler to
write, simpler to read, and more easily adapted for computer use. Later you will see that
matrix representation of linear transformations also has some theoretical advantages. In this
section you will see that for linear transformations involving finite-dimensional vector
spaces, matrix representation is always possible.
Tx  Ax   2
1
0
1
3
3
1
2
4
 x1
x2
x3

?
Tx1, x2, x3  2x1  x2  x3, x1  3x2  2x3, 3x2  4x3
T: R3→R3
6.3
388 Chapter 6 Linear Transformations
The key to representing a linear transformation by a matrix is to determine how
it acts on a basis of Once you know the image of every vector in the basis, you can use
the properties of linear transformations to determine for any in
For convenience, the first three theorems in this section are stated in terms of linear
transformations from into relative to the standard bases in and At the end of
the section these results are generalized to include nonstandard bases and general vector
spaces.
Recall that the standard basis for written in column vector notation, is represented by
PROOF To show that for any in you can write
Because is a linear transformation, you have
 v1Te1  v2Te2  ...  vnTen.
 Tv1e1  Tv2e2  ...  Tvnen
Tv  Tv1e1  v2e2  ...  vnen
T
v

v1
v2
.
.
.
vn
  v1e1  v2e2  ...  vnen.
Rn Tv  Av v ,
B  e1, e2, . . . , en


1
0
.
.
.
0

,

0
1
.
.
.
0

, . . . , 
0
0
.
.
.
1

.
Rn
,
Rm R . n Rm R , n
Tv v V.
V.
T: V→W
Let be a linear transformation such that
Then the matrix whose columns correspond to
is such that for every in is called the AR standard matrix for T. n Tv  Av v .
A

a11
a21
.
.
.
am1
a12
a22
.
.
.
am2
...
...
 ...
a1n
a2n
.
.
.
amn 
,
Tei m  n n ,
Te1

a11
a21
.
.
.
am1

, Te2

a12
a22
.
.
.
am2

, . . . , Ten

a1n
a2n
.
.
.
amn

.
T: Rn→R THEOREM 6.10 m
Standard Matrix for a
Linear Transformation
Section 6.3 Matrices for Linear Transformations 389
On the other hand, the matrix product is represented by
So, for each in
Find the standard matrix for the linear transformation defined by
SOLUTION Begin by finding the images of and
Vector Notation Matrix Notation
By Theorem 6.10, the columns of consist of and and you have
A  Te1  Te2  Te3  
1
2
2
1
0
0.
Te3 Te , 1, Te2 A ,
Te3  T
	
0
0
1

  
0
0 Te3  T0, 0, 1  0, 0
Te2  T
	
0
1
0

  
2
 1 Te2  T0, 1, 0  2, 1
Te1  T
	
1
0
0

  
1
2 Te1  T1, 0, 0  1, 2
e3 e . 1, e2,
Tx, y, z  x  2y, 2x  y.
T: R3→R2
EXAMPLE 1 Finding the Standard Matrix for a Linear Transformation
Rn Tv  Av v .
 v1Te1  v2Te2  ...  vnTen.
 v1

a11
a21
.
.
.
am1
  v2

a12
a22
.
.
.
am2
  ...  vn

a1n
a2n
.
.
.
amn


a11v1
a21v1 .
.
. am1v1



a12v2
a22v2 .
.
. am2v2
 ... 
 ... 
 ... 
a1nvn
a2nvn .
.
. amnvn
 Av

a11
a21
.
.
.
am1
a12
a22
.
.
.
am2
...
...
...
a1n
a2n
.
.
.
amn
 v1
v2
.
.
.
vn

Av
390 Chapter 6 Linear Transformations
As a check, note that
which is equivalent to
A little practice will enable you to determine the standard matrix for a linear transformation, such as the one in Example 1, by inspection. For instance, the standard matrix for
the linear transformation defined by
is found by using the coefficients of and to form the rows of as follows.
The linear transformation is given by projecting each point in onto the -axis,
as shown in Figure 6.8. Find the standard matrix for
SOLUTION This linear transformation is represented by
So, the standard matrix for is
The standard matrix for the zero transformation from into is the zero
matrix, and the standard matrix for the identity transformation from into is
Composition of Linear Transformations
The composition, of with is defined by
where is a vector in This composition is denoted by
T  T2  T1.
Rn v .
Tv  T2T1v,
T2: Rm→Rp T1: Rn→Rm T,
In R . n Rn
R m  n m Rn
 
1
0
0
0.
A  T1, 0  T0, 1
T
Tx, y  x, 0.
T.
R x 2 T: R2→R2
EXAMPLE 2 Finding the Standard Matrix for a Linear Transformation
A  
1
2
4
2
0
1
5
3
2

x A, 3 x1, x2,
Tx1, x2, x3  x1  2x2  5x3, 2x1  3x3, 4x1  x2  2x3
Tx, y, z  x  2y, 2x  y.
A

x
y
z
  
1
2
2
1
0
0 
x
y
z
  
x  2y
2x  y,
4x1  1x2  2x3
2x1  0x2  3x3
1x1  2x2  5x3
Figure 6.8
x
y
Projection onto the x-axis
(x, 0)
(x, y)
T(x, y) = (x, 0)
Section 6.3 Matrices for Linear Transformations 391
The domain of is defined as the domain of Moreover, the composition is not
defined unless the range of lies within the domain of as shown in Figure 6.9.
Figure 6.9
The next theorem emphasizes the usefulness of matrices for representing linear transformations. This theorem not only states that the composition of two linear transformations is
a linear transformation, but also says that the standard matrix for the composition is the
product of the standard matrices for the two original linear transformations.
PROOF To show that is a linear transformation, let and be vectors in and let be any scalar.
Then, because and are linear transformations, you can write
Now, to show that is the standard matrix for use the associative property of matrix
multiplication to write
Tv  T2T1v  T2A1v  A2A1v  A2A1v.
A T, 2A1
 cT2T1v  cTv.
 T2cT1v
Tcv  T2T1cv
 T2T1u  T2T1v  Tu  Tv
 T2T1u  T1v
Tu  v  T2T1u  v
T1 T2
cRn T vu
v
u
w
T
Composition of Transformations
Rm
Rp
Rn T1
T2
T2 T , 1
T1 T .
Let and be linear transformations with standard matrices
and The composition defined by is a linear
transformation. Moreover, the standard matrix for is given by the matrix product
A  A2A1.
TA
Tv  T2T1 T: R v, n→Rp A , 2 A . 1
T2: Rm→Rp T1: Rn→R THEOREM 6.11 m
Composition of
Linear Transformations
REMARK : Theorem 6.11 can
be generalized to cover the
composition of linear transformations. That is, if the standard
matrices of are
then the standard matrix for the composition
is represented by
A  AnAn1 . . . A2A1.
T
A1, A2, . . . , An,
T1, T2, . . . , Tn
n
392 Chapter 6 Linear Transformations
Because matrix multiplication is not commutative, order is important when the compositions of linear transformations are formed. In general, the composition is not the
same as as demonstrated in the next example.
Let and be linear transformations from into such that
and
Find the standard matrices for the compositions and
SOLUTION The standard matrices for and are
and
By Theorem 6.11, the standard matrix for is
and the standard matrix for is
Another benefit of matrix representation is that it can represent the inverse of a linear
transformation. Before seeing how this works, consider the next definition.
Not every linear transformation has an inverse. If the transformation is invertible,
however, then the inverse is unique and is denoted by
Just as the inverse of a function of a real variable can be thought of as undoing what the
function did, the inverse of a linear transformation can be thought of as undoing the mapping done by For instance, if is a linear transformation from onto such that
T1, 4, 5  2, 3, 1
R3 R3 T. T
T
T1
1
.
T1
A  A1A2  
2
0
1
1
0
0
0
0
1
 1
0
0
1
0
1
0
1
0
  
2
0
1
2
0
0
1
0
0

.
T
A  A2A1  
1
0
0
1
0
1
0
1
0
 2
0
1
1
0
0
0
0
1
  
2
1
0
1
0
0
0
1
0

,
T
A2  
1
0
0
1
0
1
0
1
0
 A . 1  
2
0
1
1
0
0
0
0
1

T1 T2
T  T1  T2 T  T . 2  T1
T2 T x, y, z  x  y, z, y. 1x, y, z  2x  y, 0, x  z
R3 R3 T1 T2
EXAMPLE 3 The Standard Matrix for a Composition
T1  T2,
T2  T1
If and are linear transformations such that for every in
and
then is called the inverse of and is said to be T invertible. T1 1 T , 2
T1T2 T v  v, 2T1v  v
Rn T v 2: Rn→Rn T1: Rn→Rn Definition of Inverse
Linear Transformation
Section 6.3 Matrices for Linear Transformations 393
and if exists, then maps back to its preimage under That is,
The next theorem states that a linear transformation is invertible if and only if it is an
isomorphism (one-to-one and onto). You are asked to prove this theorem in Exercise 78.
REMARK : Several other conditions are equivalent to the three given in Theorem 6.12; see
the summary of equivalent conditions from Section 4.6.
The linear transformation is defined by
Show that is invertible, and find its inverse.
SOLUTION The standard matrix for is
Using the techniques for matrix inversion (see Section 2.3), you can find that is invertible
and its inverse is
So, is invertible and its standard matrix is A1 T .
A1  
1
1
6
1
0
2
0
1
3

.
A
A  
2
3
2
3
3
4
1
1
1

.
T
T
Tx1, x2, x3  2x1  3x2  x3, 3x1  3x2  x3, 2x1  4x2  x3.
T: R3→R3
EXAMPLE 4 Finding the Inverse of a Linear Transformation
T 1
2, 3, 1  1, 4, 5.
T 2, 3, 1 T. 1 T1
Let be a linear transformation with standard matrix Then the following
conditions are equivalent.
1. is invertible.
2. is an isomorphism.
3. is invertible.
And, if is invertible with standard matrix then the standard matrix for is A1 T . 1 T A,
A
T
T
T: R A. n→R THEOREM 6.12 n
Existence of an
Inverse Transformation
394 Chapter 6 Linear Transformations
Using the standard matrix for the inverse, you can find the rule for by computing
the image of an arbitrary vector
In other words,
Nonstandard Bases and General Vector Spaces
You will now consider the more general problem of finding a matrix for a linear
transformation where and are ordered bases for and respectively.
Recall that the coordinate matrix of relative to is denoted by In order to represent
the linear transformation must be multiplied by a coordinate matrix relative to The
result of the multiplication will be a coordinate matrix relative to That is,
is called the matrix of relative to the bases and
To find the matrix you will use a procedure similar to the one used to find the
standard matrix for That is, the images of the vectors in are written as coordinate
matrices relative to the basis These coordinate matrices form the columns of B. A.
T. B
A,
A T B B.
TvB  AvB.
B.
, AT B.
vB v B .
T: V→W, B B V W,
T 1
x1, x2, x3  x1  x2, x1  x3, 6x1  2x2  3x3.
A1v  
1
1
6
1
0
2
0
1
3
 x1
x2
x3
  
x1
x1
6x1
 x2
 2x2
 x3
 3x3

v  x1, x2, x3.
T 1
Let and be finite-dimensional vector spaces with bases and respectively, where
If is a linear transformation such that
then the matrix whose columns correspond to
is such that for every in Tv v V. B  AvB
A

a11
a21
.
.
.
am1
a12
a22
.
.
.
am2
...
...
...
a1n
a2n
.
.
.
amn

,
Tvi
B m  n n ,
Tv1B

a11
a21
.
.
.
am1

, Tv2B

a12
a22
.
.
.
am2

, . . . , TvnB

a1n
a2n
.
.
.
amn

,
T: V→W
B  v1, v2, . . . , vn
.
Transformation Matrix for WV B B,
Nonstandard Bases
Section 6.3 Matrices for Linear Transformations 395
Let be a linear transformation defined by
Find the matrix for relative to the bases
and
SOLUTION By the definition of you have
The coordinate matrices for and relative to are
and
The matrix for relative to and is formed by using these coordinate matrices as
columns to produce
For the linear transformation from Example 5, use the matrix to find
where
SOLUTION Using the basis you find
which implies
So, is
Finally, because it follows that
Tv  31, 0  30, 1  3, 3.
B  1, 0, 0, 1,
AvB  
3
0
0
3 1
1  
3
3  TvB.
TvB
vB   1
1.
v  2, 1  11, 2  11, 1,
B  1, 2, 1, 1,
v  2, 1.
T: R A Tv, 2→R2
EXAMPLE 6 Using a Matrix to Represent a Linear Transformation
A  
3
0
0
3.
T B B
Tv2B   0
3 Tv . 1B  
3
0
Tv B 2 Tv  1
Tv2  T1, 1  0, 3  0w1  3w2.
Tv1  T1, 2  3, 0  3w1  0w2
T,
B  1, 2, 1, 1 B  1, 0, 0, 1.
w1 w2 v2 v1
T
Tx1, x2  x1  x2, 2x1  x2.
T: R2→R2
EXAMPLE 5 Finding a Matrix Relative to Nonstandard Bases
396 Chapter 6 Linear Transformations
You can check this result by directly calculating using the definition of from
Example 5:
In the special case where and the matrix is called the matrix of
relative to the basis In such cases the matrix of the identity transformation is simply
To see this, let Because the identity transformation maps each to
itself, you have
and it follows that
In the next example you will construct a matrix representing the differential operator
discussed in Example 10 in Section 6.1.
Let be the differential operator that maps a quadratic polynomial onto its
derivative Find the matrix for using the bases
and
SOLUTION The derivatives of the basis vectors are
So, the coordinate matrices relative to are
and the matrix for is
Note that this matrix does produce the derivative of a quadratic polynomial
Ap  
0
0
1
0
0
2 
a
b
c
  
b
2c ⇒ b  2cx  Dxa  bx  cx2
a  bx  cx2
.
px
A  
0
0
1
0
0
2.
Dx
Dx1B  
0
0, DxxB  
1
0, Dxx 2B  
0
2,
B
Dxx 2  2x  01  2x.
Dxx  1  11  0x
Dx1  0  01  0x
B  1, x, x B  1, x. 2
Dx p.
D p x: P2→P1
EXAMPLE 7 A Matrix for the Differential Operator (Calculus)
A  In.
Tv1B

1
0
.
.
.
0

, Tv2B

0
1
.
.
.
0

, . . . , TvnB

0
0
.
.
.
1

,
v B  i v1, v2, . . . , vn
.
In B. .
V  W B  B, A T
T2, 1  2  1, 22  1  3, 3.
Tv T
Section 6.3 Matrices for Linear Transformations 397
SECTION 6.3 Exercises
In Exercises 1–10, find the standard matrix for the linear transformation
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
In Exercises 11–16, use the standard matrix for the linear transformation to find the image of the vector
11.
12.
13.
14.
15.
16.
In Exercises 17–34, (a) find the standard matrix for the linear
transformation (b) use to find the image of the vector and
(c) sketch the graph of and its image.
17. is the reflection through the origin in
18. is the reflection in the line in
19. is the reflection in the -axis in
20. is the reflection in the -axis in
21. is the counterclockwise rotation of in
22. is the counterclockwise rotation of in
23. is the clockwise rotation ( is negative) of in
24. is the clockwise rotation ( is negative) of in
25. is the reflection through the -coordinate plane in
26. is the reflection through the -coordinate plane in
27. is the reflection through the -coordinate plane in
28. is the counterclockwise rotation of in
29. is the counterclockwise rotation of in
30. is the counterclockwise rotation of in
31. is the projection onto the vector in
32. is the projection onto the vector in
33. is the reflection through the vector in
The reflection of a vector through is
34. Repeat Exercise 33 for and
In Exercises 35–38, (a) find the standard matrix for the linear
transformation (b) use to find the image of the vector and (c)
use a graphing utility or computer software program and to verify
your result from part (b).
35.
36.
37.
38.
In Exercises 39–44, find the standard matrices for and
39.
40.
41.
T2: R3→R3
, T2x, y, z  0, x, 0
T1: R3→R3
, T1x, y, z  x, y, z
T2: R2→R2
, T2x, y   y, 0
T1: R2→R2
, T1x, y  x  2y, 2x  3y
T2: R2→R2
, T2x, y  2x, x  y
T1: R2→R2
, T1x, y  x  2y, 2x  3y
T  T1  T2.
T  T2  T1
v  0, 1, 1, 1
Tx1, x2, x3, x4  x1  2x2, x2  x1, 2x3  x4, x1,
v  1, 0, 1, 1
Tx1, x2, x3, x4  x1  x2, x3, x1  2x2  x4, x4,
v  2, 1, 1
Tx, y, z  3x  2y  z, 2x  3y, y  4z),
v  1, 2, 1
Tx, y, z  2x  3y  z, 3x  2z, 2x  y  z,
A
, AT v,
A
w  4, 2 v  5, 0.
Tv  2 projwv  v.
v  1, 4.  v w
R2 T w  3, 1 ,
Tv  projwv, v  2, 3.
R2 T w  1, 5 :
projwv, v  1, 4.
R2 T w  3, 1 : Tv
R2 T 180 , v  1, 2.
R2 T 30 , v  1, 2.
R2 T 45 , v  2, 2.
R3: Tx, y, z  x, y, z, v  1, 2, 1.
T xz
R3: Tx, y, z  x, y, z, v  2, 3, 4.
T yz
R3: Tx, y, z  x, y, z, v  3, 2, 2.
T xy
v  2, 1.
R2 T   ,30
v  1, 2.
R2 T   ,60
R2 T 120 , v  2, 2.
R2 T 135 , v  4, 4.
v  4, 1.
R2 T x : Tx, y  x, y,
v  2, 3.
R2 T y : Tx, y  x, y,
v  3, 4.
R2 T y  x : Tx, y   y, x,
v  3, 4.
R x, y, 2 T : Tx, y
v
T, A v,
A
v  1, 2, 3, 2
Tx1, x2, x3, x4  2x1  x3, 3x2  4x4, 4x3  x1, x2  x4,
Tx1, x2, x3, x4  x1  x2, x3  x4, v  1, 1, 1, 1
Tx, y  x  y, x  2y, y, v  2, 2
Tx, y  x  y, x  y, 2x, 2y, v  3, 3
Tx, y, z  2x  y, 3y  z, v  0, 1, 1
Tx, y, z  13x  9y  4z, 6x  5y  3z, v  1, 2, 1
T v.
Tx1, x2, x3  0, 0, 0
Tx1, x2, x3, x4  0, 0, 0, 0
Tx, y, z  3x  2z, 2y  z
Tx, y, z  3z  2y, 4x  11z
Tx, y, z  5x  3y  z, 2z  4y, 5x  3y
Tx, y, z  x  y, x  y, z  x
Tx, y  4x  y, 0, 2x  3y
Tx, y  2x  3y, x  y, y  4x
Tx, y  3x  2y, 2y  x
Tx, y  x  2y, x  2y
T.
398 Chapter 6 Linear Transformations
42.
43.
44.
In Exercises 45–56, determine whether the linear transformation is
invertible. If it is, find its inverse.
45.
46.
47.
48.
49.
50.
51.
52.
53.
54.
55.
56.
In Exercises 57–64, find by using (a) the standard matrix and
(b) the matrix relative to and
57.
58.
59.
60.
61.
62.
63.
64.
65. Let be given by Find the matrix for
relative to the bases and
66. Let be given by Find the matrix for
relative to the bases and
67. Calculus Let be a basis of a subspace of
the space of continuous functions, and let be the differential
operator on Find the matrix for relative to the basis
68. Calculus Repeat Exercise 67 for
69. Calculus Use the matrix from Exercise 67 to evaluate
70. Calculus Use the matrix from Exercise 68 to evaluate
71. Calculus Let be a basis for and let
be the linear transformation represented by
(a) Find the matrix for with respect to and the
standard basis for
(b) Use to integrate
True or False? In Exercises 72 and 73, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
72. (a) If is a linear transformation to the matrix
is called the standard matrix such that for every
in
(b) The composition of linear transformations and
defined by has the standard matrix
represented by the matrix product
(c) All linear transformations have a unique inverse T 1 T .
A  A2A1.
Tv  T2T1v, A
T2 T , T 1
Rn v .
Tv  Av
R m  n A m R , n T
px  6  2x  3x3 A .
P4.
TA B
Txk
  
x
0
tk dt.
T: P3→P4
P3 B  1, x, x , 2, x3
Dx5e2x  3xe2x  x2
e2x
.
Dx3x  2xex
.
B  e2x
, xe2x
, x2
e2x.
D B. W x .
Dx
B  1, x, ex
, xex W
B  1, x, x2, x3
, x 4 B  1, x, x . 2
T p  x T 2 T: P p. 2→P4
B  1, x, x2, x3 B  1, x, x . 2
T: P2→P3 T p  xp. T
B  B  4, 1, 3, 1
T: R2→R2, Tx, y  2x  12y, x  5y, v  10, 5,
B  1, 1, 1, 1, 1, 0, 0, 1, 1
B  2, 0, 1, 0, 2, 1, 1, 2, 1,
v  4, 5, 10,
T: R3→R3
, Tx, y, z  x  y  z, 2z  x, 2y  z,
B  1, 1, 2, 0
B  1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
v  4, 3, 1, 1,
T: R4→R2
, Tx1, x2, x3, x4  x1  x2  x3  x4, x4  x1,
B  1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0
B  2, 0, 1, 0, 2, 1, 1, 2, 1,
v  1, 5, 2,
T: R3→R4
, Tx, y, z  2x, x  y, y  z, x  z,
B  2, 0, 1, 0, 2, 1, 1, 2, 1, B  1, 1, 2, 0
T: R3→R2
, Tx, y, z  2x  z, y  2x, v  0, 5, 7,
B  1, 1, 1, 1, 1, 0, 0, 1, 1, B  1, 2, 1, 1
T: R3→R2
, Tx, y, z  x  y, y  z, v  1, 2, 3,
B  1, 2, 1, 1, B  1, 1, 1, 1, 1, 0, 0, 1, 1
T: R2→R3, Tx, y  x  y, 0, x  y, v  3, 2,
B  1, 1, 0, 1, B  1, 1, 0, 0, 1, 1, 1, 0, 1
T: R2→R3, Tx, y  x  y, x, y, v  5, 4,
B B.
Tv
Tx1, x2, x3, x4  x4, x3, x2, x1
Tx1, x2, x3, x4  x1  2x2, x2, x3  x4, x3
Tx, y  2x, 2y
Tx, y  5x, 5y
Tx, y  x  4y, x  4y
Tx, y  x  y, 3x  3y
Tx, y  0, y
Tx, y  2x, 0
Tx1, x2, x3  x1  x2, x2  x3, x1  x3
Tx1, x2, x3  x1, x1  x2, x1  x2  x3
Tx, y  x  2y, x  2y
Tx, y  x  y, x  y
T2: R3→R2, T2x, y, z   y, z
T1: R2→R3, T1x, y  x, y, y
T2: R3→R2, T2x, y, z  x  3y, z  3x
T1: R2→R3, T1x, y  x  2y, x  y, x  y
T2: R3→R3, T2x, y, z   y  z, x  z, 2y  2z
T1: R3→R3
, T1x, y, z  x  2y, y  z, 2x  y  2z
Section 6.4 Transition Matrices and Similarity 399
73. (a) The composition T of linear transformations and
represented by is defined if the range of
lies within the domain of
(b) In general, the compositions and have the
same standard matrix
(c) If is an invertible linear transformation with
standard matrix then has the same standard matrix
74. Let be a linear transformation such that for in
Find the standard matrix for
75. Let be represented by Find the
matrix for relative to the standard bases for and
76. Show that the linear transformation given in Exercise 75 is an
isomorphism, and find the matrix for the inverse of
77. Guided Proof Let and be one-to-one
linear transformations. Prove that the composition
is one-to-one and that exists and is equal to
Getting Started: To show that is one-to-one, you can
use the definition of a one-to-one transformation and show that
implies For the second statement, you first
need to use Theorems 6.8 and 6.12 to show that is invertible,
and then show that and are
identity transformations.
(i) Let Recall that
for all vectors Now use the fact that and are
one-to-one to conclude that
(ii) Use Theorems 6.8 and 6.12 to show that and
are all invertible transformations. So and
exist.
(iii) Form the composition It is a linear
transformation from to To show that it is the
inverse of you need to determine whether the composition of with on both sides gives an identity
transformation.
78. Prove Theorem 6.12.
79. Writing Is it always preferable to use the standard basis for
Discuss the advantages and disadvantages of using different
bases.
80. Writing Look back at Theorem 4.19 and rephrase it in terms
of what you have learned in this chapter.
Rn
?
T T
T,
V V.
T  T1
1  T2
1
.
T2
1 T1
1
T1, 2, TT
u  v.
v T2 T1 .
T2  T1v  T2T1 Tu  Tv. v
T1
1  T2
1
 T  T1  T 1  T2
1

T
Tu  Tv u  v.
T
T1
1  T2
1 T . 1
T  T2  T1
T2 T1:V→V :V→V
T.
T
M3,2 M . T 2,3
TA  AT T: M . 2,3→M3,2
T.
Rn T Tv  kv v .
T A. 1 A,
T: Rn→Rn
A.
T2  T1 T1  T2
T2 T . 1
Tv  T2T1v,
T2 T , 1
Transition Matrices and Similarity
In Section 6.3 you saw that the matrix for a linear transformation depends on the
basis of In other words, the matrix for relative to a basis is different from the matrix
for relative to another basis
A classical problem in linear algebra is this: Is it possible to find a basis such that the
matrix for relative to is diagonal? The solution of this problem is discussed in Chapter
7. This section lays a foundation for solving the problem. You will see how the matrices for
a linear transformation relative to two different bases are related. In this section,
and represent the four square matrices listed below.
1. Matrix for relative to
2. Matrix for relative to
3. Transition matrix from to
4. Transition matrix from to
Note that in Figure 6.10 there are two ways to get from the coordinate matrix to
the coordinate matrix One way is direct, using the matrix to obtain
The other way is indirect, using the matrices and to obtain
P1
APvB  TvB
.
P1 P, A,
A vB  TvB
.
Tv A B
.
vB
P1 B B:
B B: P
T B: A T B: A
P1
A, A, P,
T B
B
T B.
V. T B
T: V→V
6.4
400 Chapter 6 Linear Transformations
But by the definition of the matrix of a linear transformation relative to a basis, this implies
that
This relationship is demonstrated in Example 1.
Figure 6.10
Find the matrix for
relative to the basis
SOLUTION The standard matrix for is
Furthermore, using the techniques of Section 4.7, you can find that the transition matrix
from to the standard basis is
The inverse of this matrix is the transition matrix from to
The matrix for relative to is
A  P1
AP  
1
0
1
1 2
1
2
31
0
1
1   3
1
2
2.
T B
P1  
1
0
1
1.
B B,
P  
1
0
1
1.
B B  1, 0, 0, 1
A   2
1
2
3.
T
B  1, 0, 1, 1.
Tx1, x2  2x1  2x2, x1  3x2,
T: R2→R2 A ,
EXAMPLE 1 Finding a Matrix of a Linear Transformation
A
A′
P P−1
V
[ ] v B
[ ] v B′
[ ( )] T v B
[ ( )] T v B′
V
V V
(Basis B)
(Basis B′)
A  P1AP.
Section 6.4 Transition Matrices and Similarity 401
In Example 1, the basis is the standard basis for In the next example, both and
are nonstandard bases.
Let
and
be bases for and let
be the matrix for relative to Find the matrix of relative to
SOLUTION In Example 5 in Section 4.7, you found that
and
So, the matrix of relative to is
The diagram in Figure 6.10 should help you to remember the roles of the matrices
and
For the linear transformation from Example 2, find and
for the vector whose coordinate matrix is
SOLUTION To find use the transition matrix from to
To find multiply by the matrix to obtain
TvB  AvB  
2
3
7
77
5  
21
14.
TvB, vB A
vB  PvB  
3
2
2
13
1  
7
5
v P B B. B,
vB  
3
1.
v
TvB vB, TvB T: R , 2→R2
EXAMPLE 3 Using a Matrix for a Linear Transformation
P1 A, A, P, .
A  P1
AP  
1
2
2
32
3
7
73
2
2
1   2
1
1
3.
T B
P1  
1
2
2
3.
P  
3
2
2
1
T: R B. A, T B. 2→R2
A  
2
3
7
7
R2
,
B  3, 2, 4,2 B  1, 2, 2, 2
EXAMPLE 2 Finding a Matrix for a Linear Transformation
R B B 2 B .
402 Chapter 6 Linear Transformations
To find multiply by to obtain
or multiply by to obtain
REMARK : It is instructive to note that the transformation in Examples 2 and 3 is
represented by the rule Verify the results of Example 3 by
showing that and
Similar Matrices
Two square matrices and that are related by an equation are called
similar matrices, as indicated in the next definition.
If is similar to then it is also true that is similar to as stated in the next
theorem. So, it makes sense to say simply that and are similar.
PROOF The first property follows from the fact that To prove the second property, write
where
The proof of the third property is left to you. (See Exercise 23.)
Q  P1 Q . 1
AQ  B,
PAP1  B
PAP1  PP1
BPP1
A  P1
BP
A  InAIn.
A A A A, A A,
A  P1 A A AP
v  1, 4 Tv  7, 14.
Tx, y  x  3
2 y, 2x  4y.
T
TvB  AvB   2
1
1
33
1  
7
0.
v A B
TvB  P1
TvB  
1
2
2
321
14  
7
0
P1 TvB TvB,
For square matrices and of order is said to be similar to if there exists an
invertible matrix such that A  P1 P AP.
Definition of A A n, A A
Similar Matrices
Let and be square matrices of order Then the following properties are true.
1. is similar to
2. If is similar to then is similar to
3. If is similar to and is similar to then is similar to A BB , AC C.
A , BB A.
A A.
THEOREM 6.13 , B, CA n.
Properties of
Similar Matrices
Section 6.4 Transition Matrices and Similarity 403
From the definition of similarity, it follows that any two matrices that represent the same
linear transformation with respect to different bases must be similar.
(a) From Example 1, the matrices
and
are similar because where
(b) From Example 2, the matrices
and
are similar because where
You have seen that the matrix for a linear transformation depends on the basis
used for This observation leads naturally to the question: What choice of basis will make
the matrix for as simple as possible? Is it always the standard basis? Not necessarily, as
the next example demonstrates.
Suppose
is the matrix for relative to the standard basis. Find the matrix for relative to
the basis
SOLUTION The transition matrix from to the standard matrix has columns consisting of the vectors
in
P  
1
1
0
1
1
0
0
0
1

,
B,
B
B  1, 1, 0, 1, 1, 0, 0, 0, 1.
T: R T 3→R3
A  
1
3
0
3
1
0
0
0
2

EXAMPLE 5 A Comparison of Two Matrices for a Linear Transformation
T
V.
T: V→V
P  
3
2
2
1 A  P . 1
AP,
A   2
1
1
3
A    2
3
7
7
P  
1
0
1
1 A  P . 1
AP,
A   3
1
2
2
A    2
1
2
3
EXAMPLE 4 Similar Matrices
T: V→V
404 Chapter 6 Linear Transformations
and it follows that
So, the matrix for relative to is
Note that matrix is diagonal.
Diagonal matrices have many computational advantages over nondiagonal ones. For
instance, for the diagonal matrix
the th power is represented as follows.
A diagonal matrix is its own transpose. Moreover, if all the diagonal elements are nonzero,
then the inverse of a diagonal matrix is the matrix whose main diagonal elements are the
reciprocals of corresponding elements in the original matrix. With such computational
advantages, it is important to find ways (if possible) to choose a basis for such that the
transformation matrix is diagonal, as it is in Example 5. You will pursue this problem in
the next chapter.
V
Dk

d1
k
0
.
.
.
0
0
d2
k
.
.
.
0
...
...
...
0
0
.
.
.
dn
k

k
D

d1
0
.
.
.
0
0
d2
.
.
.
0
...
...
...
0
0
.
.
.
dn

,
A
 
4
0
0
0
2
0
0
0
2

.
 
1
2
1
2
0
1
2
1
2
0
0
0
1  
1
 3
0
3
1
0
0
0
2   1
 1
0
1
1
0
0
0
1 
A  P1
AP
T B
P1  
1
2
1
2
0
1
2
1
2
0
0
0
1
 .
Section 6.4 Transition Matrices and Similarity 405
SECTION 6.4 Exercises
In Exercises 1–8, (a) find the matrix for relative to the basis
and (b) show that is similar to the standard matrix for
1.
2.
3.
4.
5.
6.
7.
8.
9. Let and be
bases for and let
be the matrix for relative to
(a) Find the transition matrix from to
(b) Use the matrices and to find and
where
(c) Find (the matrix for relative to ) and
(d) Find in two ways: first as and then
as
10. Repeat Exercise 9 for
and
(Use matrix provided in Exercise 9.)
11. Let and
be bases for and let
be the matrix for relative to
(a) Find the transition matrix from to
(b) Use the matrices and to find and
where
(c) Find (the matrix for relative to ) and
(d) Find in two ways: first as and then
as
12. Repeat Exercise 11 for
and
(Use matrix provided in Exercise 11.)
13. Let and be
bases for and let be the matrix for
relative to
(a) Find the transition matrix from to
(b) Use the matrices and to find and where
(c) Find (the matrix for relative to ) and
(d) Find in two ways: first as and then
as
14. Repeat Exercise 13 for ,
and (Use matrix
provided in Exercise 13.)
15. Prove that if and are similar, then Is the converse
true?
16. Illustrate the result of Exercise 15 using the matrices
where B  P1AP.
P1  
1
0
1
1
1
2
2
2
3
 P  , 
1
2
1
1
1
1
0
2
1

,
A  
1
0
0
0
2
0
0
0
3

, B   11
10
18
7
8
12
10
10
17
,
A  B BA .
vB   A 1
4 B  1, 1, 1, 2, .
B  1, 1, 2, 1
AvB.
P1
TvB TvB
P1 A T B .
vB  
1
4.
TvB v , PA B
P B B.
B.
T: R2→R2 A  
2
0
1
1 R  2,
B  1, 2, 1, 1 B  4, 1, 0, 2
vB  A 
2
1
1

.
B  1, 1, 1, 1, 1, 1, 1, 1, 1,
B  1, 0, 0, 0, 1, 0, 0, 0, 1,
AvB.
P1
TvB TvB
P1 A T B .
vB   1
0
1

.
TvB v , PA B
P B B.
T: R B. 3→R3
A   3
2
1
2
1
2
1
2
1
1
2
1
2
5
2

R3 0, 1, 0, 0, 0, 1 ,
B  1, 1, 0, 1, 0, 1, 0, 1, 1 B  1, 0, 0,
vB   A
1
3.
0, 1,
B  1, 1, 2, 3, B  1, 1,
AvB
.
P1TvB TvB
P1 A T B .
vB  
1
2.
TvB v , PA B
P B B.
T: R B. 2→R2
A  
3
0
2
4
R2,
B  1, 3, 2, 2 B  12, 0, 4, 4
B  1, 1, 0, 0, 0, 1, 0, 1, 1
T: R3→R3
, Tx, y, z  x, x  2y, x  y  3z,
B  1, 0, 1, 0, 2, 2, 1, 2, 0
T: R Tx, y,z  x  y  2z, 2x  y  z, x  2y  z, 3→R3
,
B  1, 1, 0, 1, 0, 1, 0, 1, 1
T: R3→R3
, Tx, y, z  0, 0, 0,
B  1, 1, 0, 1, 0, 1, 0, 1, 1
T: R3→R3, Tx, y, z  x, y, z,
T: R2→R2, Tx, y  x  2y, 4x, B  2, 1, 1, 1
T: R2→R2
, Tx, y  x  y, 4y, B  4, 1, 1, 1
T: R B  1, 2, 0, 4 2→R2, Tx, y  2x  y, x  2y,
T: R B  1, 2, 0, 3 2→R2
, Tx, y  2x  y, y  x,
B A A, T.
 TA
406 Chapter 6 Linear Transformations
17. Let and be similar matrices.
(a) Prove that and are similar.
(b) Prove that if is nonsingular, then is also nonsingular
and and are similar.
(c) Prove that there exists a matrix such that
18. Use the result of Exercise 17 to find where for
the matrices
19. Determine all matrices that are similar to
20. Prove that if is idempotent and is similar to then is
idempotent. An matrix is idempotent if
21. Let be an matrix such that Prove that if is
similar to then
22. Let Prove that if then
23. Complete the proof of Theorem 6.13 by proving that if is
similar to and is similar to then is similar to
24. Writing Suppose and are similar. Explain why they have
the same rank.
25. Prove that if and are similar, then is similar to
26. Prove that if and are similar, then is similar to for any
positive integer
27. Let , where is an invertible matrix. Prove that
the matrix is similar to
28. Let , where is a diagonal matrix with main
diagonal entries Prove that
for
29. Writing Let be a basis for the vector
space let be the standard basis, and consider the identity
transformation What can you say about the matrix for
relative to both and What can you say about the matrix
for relative to Relative to
True or False? In Exercises 30 and 31, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
30. (a) The matrix for a linear transformation relative to the basis
is equal to the product where is the transition matrix from to is the matrix for the linear
transformation relative to basis and is the transition
matrix from to
(b) Two matrices that represent the same linear transformation
with respect to different bases are not necessarily
similar.
31. (a) The matrix for a linear transformation relative to the basis
is equal to the product where is the transition
matrix from to B, is the matrix for the linear transformation relative to basis and is the transition matrix
from to
(b) The standard basis for will always make the coordinate
matrix for the linear transformation T the simplest matrix
possible.





This section presents one of the most important problems in linear algebra, the eigenvalue
problem. Its central question can be stated as follows. If is an matrix, do nonzero
vectors in exist such that is a scalar multiple of The scalar, denoted by the Greek
letter lambda is called an eigenvalue of the matrix and the nonzero vector is called
an eigenvector of corresponding to The terms eigenvalue and eigenvector are derived
from the German word Eigenwert, meaning “proper value.” So, you have
A 	.
	, A, x
R Ax x? n x
A n  n
7.1
422 Chapter 7 Eigenvalues and Eigenvectors
Although you looked at the eigenvalue problem briefly in Section 3.4, the approach in
this chapter will not depend on that material.
Eigenvalues and eigenvectors have many important applications, some of which are discussed in Section 7.4. For now you will consider a geometric interpretation of the problem
in If is an eigenvalue of a matrix and is an eigenvector of corresponding to
then multiplication of by the matrix produces a vector that is parallel to as shown
in Figure 7.1.
Figure 7.1
Only real eigenvalues are presented in this chapter.
REMARK : Note that an eigenvector cannot be zero. Allowing to be the zero vector
would render the definition meaningless, because is true for all real values of
An eigenvalue of however, is possible. (See Example 2.)
A matrix can have more than one eigenvalue, as demonstrated in Examples 1 and 2.
	  0,
A0  	0 	.
x
x
λx
Ax = λx, λ < 0
x
λx
Ax = λx, λ <0
x A 	x x,
R 	 A x A 	, 2
.
Eigenvector
Ax  	x.
Eigenvalue
Let be an matrix. The scalar is called an eigenvalue of if there is a nonzero
vector such that
The vector is called an x eigenvector of corresponding to A 	.
Ax  	x.
x
Definitions of Eigenvalue A n  n 	 A
and Eigenvector
Section 7.1 Eigenvalues and Eigenvectors 423
For the matrix
verify that is an eigenvector of corresponding to the eigenvalue and
that is an eigenvector of corresponding to the eigenvalue
SOLUTION Multiplying by produces
So, is an eigenvector of corresponding to the eigenvalue Similarly,
multiplying by produces
So, is an eigenvector of corresponding to the eigenvalue
For the matrix
verify that
and
are eigenvectors of and find their corresponding eigenvalues. A
x x2  1, 0, 0 1  3, 1, 1
A  
1
0
0
2
0
1
1
0
1

,
EXAMPLE 2 Verifying Eigenvalues and Eigenvectors
x A 	 2  1. 2  0, 1
 1
0
1.
  0
1
Ax2  
2
0
0
10
1
x2 A
x A 	1  2. 1  1, 0
Eigenvalue Eigenvector
 2
1
0.
 
2
0
Ax1  
2
0
0
11
0
x1 A
x A 	 2  1. 2  0, 1
x A 	1  2, 1  1, 0
A  
2
0
0
1,
EXAMPLE 1 Verifying Eigenvalues and Eigenvectors
424 Chapter 7 Eigenvalues and Eigenvectors
SOLUTION Multiplying by produces
So, is an eigenvector of corresponding to the eigenvalue
Similarly, multiplying by produces
So, is an eigenvector of corresponding to the eigenvalue
Eigenspaces
Although Examples 1 and 2 list only one eigenvector for each eigenvalue, each of the four
eigenvalues in Examples 1 and 2 has an infinite number of eigenvectors. For instance, in
Example 1 the vectors and are eigenvectors of corresponding to the eigenvalue 2. In fact, if is an matrix with an eigenvalue and a corresponding
eigenvector then every nonzero scalar multiple of is also an eigenvector of This may
be seen by letting be a nonzero scalar, which then produces
It is also true that if and are eigenvectors corresponding to the same eigenvalue then
their sum is also an eigenvector corresponding to because
In other words, the set of all eigenvectors of a given eigenvalue together with the zero
vector, is a subspace of This special subspace of is called the eigenspace of
Determining the eigenvalues and corresponding eigenspaces of a matrix can be difficult.
Occasionally, however, you can find eigenvalues and eigenspaces by simple inspection, as
demonstrated in Example 3.
R 	. n Rn.
	,
Ax1  x2  Ax1  Ax2  	x1  	x2  	x1  x2.
	,
x 	, 2 x1
Acx  cAx  c	x  	cx.
c
x, x A.
A n  n 	
2, 0 3, 0 A
x A 	 2  1. 2  1, 0, 0
Ax2  
1
0
0
 2
0
1
1
0
1
 1
0
0
  
1
0
0
  1

1
0
0

.
x2 A
x A 	1  0. 1  3, 1, 1
Ax1  
1
0
0
2
0
1
1
0
1
 3
1
1
  
0
0
0
  0

3
1
1

 .
x1 A
In Example 2, is an
eigenvalue of the matrix
Calculate the determinant of the
matrix where is the
identity matrix. Repeat
this experiment for the other
eigenvalue, In general,
if is an eigenvalue of the
matrix what is the value of
 A  	 I
 ?
A,
	
	1  0.
3  3
A  	 I 2 I,
A.
	2  1
Discovery
If is an matrix with an eigenvalue then the set of all eigenvectors of together
with the zero vector
is a subspace of This subspace is called the R eigenspace of 	. n.
0  x: x is an eigenvector of 	,
THEOREM 7.1 A n  n 	, 	,
Eigenvectors of Form a
Subspace
	
Section 7.1 Eigenvalues and Eigenvectors 425
Find the eigenvalues and corresponding eigenspaces of
SOLUTION Geometrically, multiplying a vector in by the matrix corresponds to a reflection
in the -axis. That is, if then
Figure 7.2 illustrates that the only vectors reflected onto scalar multiples of themselves are
those lying on either the -axis or the -axis.
For a vector on the x-axis For a vector on the y-axis
So, the eigenvectors corresponding to are the nonzero vectors on the -axis, and
the eigenvectors corresponding to are the nonzero vectors on the -axis. This
implies that the eigenspace corresponding to is the -axis, and that the eigenspace
corresponding to is the -axis.
Finding Eigenvalues and Eigenvectors
The geometric solution in Example 3 is not typical of the general eigenvalue problem. A
general approach will now be described.
To find the eigenvalues and eigenvectors of an matrix let be the
identity matrix. Writing the equation in the form then produces
This homogeneous system of equations has nonzero solutions if and only if the coefficient
matrix is not invertible—that is, if and only if the determinant of is zero.
This is formally stated in the next theorem.
	I  A 	I  A
	I  Ax  0.
Ax  	x 	Ix  Ax
n  n , IA n  n
	 y 2  1
	 x 1  1
	 y 2  1
	 x 1  1

1
0
0
10
y  
0
y  1
0
y   1
0
0
1x
0  
x
0  1
x
0
x y
Av  
1
0
0
1x
y  
x
y.
y v  x, y,
R A 2 x, y
A  
1
0
0
1.
EXAMPLE 3 An Example of Eigenspaces in the Plane
Eigenvalue is 	1  1. Eigenvalue is 	2  1.
Figure 7.2
x
A reflects vectors
in the y-axis.
(−x, y) (0, y) (0, y) (x, y)
(−x, 0) (x, 0)
y
426 Chapter 7 Eigenvalues and Eigenvectors
The equation is called the characteristic equation of Moreover,
when expanded to polynomial form, the polynomial
is called the characteristic polynomial of This definition tells you that the eigenvalues
of an matrix correspond to the roots of the characteristic polynomial of Because
the characteristic polynomial of is of degree , can have at most distinct eigenvalues.
REMARK: The Fundamental Theorem of Algebra states that an th-degree polynomial has
precisely roots. These roots, however, include both repeated and complex roots. In this
chapter you will be concerned only with the real roots of characteristic polynomials—that
is, real eigenvalues.
Find the eigenvalues and corresponding eigenvectors of
SOLUTION The characteristic polynomial of is
So, the characteristic equation is which gives and
as the eigenvalues of To find the corresponding eigenvectors, use Gauss-Jordan elimination to solve the homogeneous linear system represented by twice: first for
and then for For the coefficient matrix is
1I  A  
1  2
1
 12
1  5  
3
1
12
4,
	  	 	1  1, 	  	 2  2. 1  1,
	 I  Ax  0
A.
	  1	  2  0, 	1  1 	2  2
 	  1	  2.
 	2  3	  2
 	2  3	  10  12
 	  2	  5  12
	I  A  
	  2
1
12
	  5 
A
A  
2
1
12
5.
EXAMPLE 4 Finding Eigenvalues and Eigenvectors
n n
n
A An n
n  n A A.
A.
	 I  A  	n  cn1	n1  ...  c1	  c 0
det	I  A  0 A.
Let be an matrix.
1. An eigenvalue of is a scalar such that
2. The eigenvectors of corresponding to are the nonzero solutions of
	I  Ax  0.
A 	
det	I  A  0.
A 	
THEOREM 7.2 A n  n
Eigenvalues and
Eigenvectors of a Matrix
Section 7.1 Eigenvalues and Eigenvectors 427
which row reduces to
showing that Letting you can conclude that every eigenvector of
is of the form
For you have
Letting you can conclude that every eigenvector of is of the form
Try checking for the eigenvalues and eigenvectors in this example.
The homogeneous systems that arise when you are finding eigenvectors will always
row reduce to a matrix having at least one row of zeros, because the systems must have
nontrivial solutions. The steps used to find the eigenvalues and corresponding eigenvectors
of a matrix are summarized as follows.
Finding the eigenvalues of an matrix can be difficult because it involves the
factorization of an th-degree polynomial. Once an eigenvalue has been found, however,
finding the corresponding eigenvectors is a straightforward application of Gauss-Jordan
reduction.
n
n  n
Ax  	i x
x  
x1
x2
  
3t
t
  t

3
1, t  0.
	2 x2  t,

1
0
3
0  2I  A  . 
2  2
1
 12
2  5   
4
1
12
3
	 2  2,
x  t  0. 
x1
x2
  
4t
t
  t

4
1,
	1 x x 2  t , 1  4x 2  0.

1
0
4
0,
Let be an matrix.
1. Form the characteristic equation It will be a polynomial equation of
degree in the variable
2. Find the real roots of the characteristic equation. These are the eigenvalues of
3. For each eigenvalue find the eigenvectors corresponding to by solving the
homogeneous system This requires row reducing of an matrix.
The resulting reduced row-echelon form must have at least one row of zeros.
	 n  n i I  Ax  0.
	 i 	 i ,
A.
n 	. 	 I  A  0.
Finding Eigenvalues A n  n
and Eigenvectors
428 Chapter 7 Eigenvalues and Eigenvectors
Find the eigenvalues and corresponding eigenvectors of
What is the dimension of the eigenspace of each eigenvalue?
SOLUTION The characteristic polynomial of is
So, the characteristic equation is
So, the only eigenvalue is To find the eigenvectors of solve the homogeneous linear system represented by
This implies that Using the parameters and you can find that the
eigenvectors of are of the form
and not both zero.
Because has two linearly independent eigenvectors, the dimension of its eigenspace
is 2.
If an eigenvalue occurs as a multiple root ( times) of the characteristic polynomial,
then has multiplicity This implies that is a factor of the characteristic
polynomial and is not a factor of the characteristic polynomial. For instance,
in Example 5 the eigenvalue has a multiplicity of 3.
Also note that in Example 5 the dimension of the eigenspace of is 2. In general,
the multiplicity of an eigenvalue is greater than or equal to the dimension of its eigenspace.
	  2
	  2
	  	1k1
	  	1k 	 k. 1
	 k 1
	  2
x  ts 
x1
x2
x3
  
s
0
t
  s

1
0
0
  t

0
0
1
,
	  2
t  x3 s  x , 1 x2  0.
2I  A  
0
0
0
1
0
0
0
0
0

2I  Ax  0.
	  2. 	  2,
	  23  0.
	I  A

	  2
0
0
1
	  2
0
0
0
	  2   	  23
.
A
A  
2
0
0
1
2
0
0
0
2

.
EXAMPLE 5 Finding Eigenvalues and Eigenvectors
Section 7.1 Eigenvalues and Eigenvectors 429
Find the eigenvalues of
and find a basis for each of the corresponding eigenspaces.
SOLUTION The characteristic polynomial of is
So, the characteristic equation is and the eigenvalues are
and (Note that has a multiplicity of 2.)
You can find a basis for the eigenspace of as follows.
Letting and produces
A basis for the eigenspace corresponding to is
Basis for
For and follow the same pattern to obtain the eigenspace bases
Basis for
Basis for
Finding eigenvalues and eigenvectors of matrices of order can be tedious.
Moreover, the procedure followed in Example 6 is generally inefficient when used on a
computer, because finding roots on a computer is both time consuming and subject to
roundoff error. Consequently, numerical methods of approximating the eigenvalues of large
n 
 4
B 3  3 3  0, 5, 0, 1.
B 2  2 2  0, 5, 1, 0
	 	 3  3, 2  2
B 1  1 1  0, 1, 0, 0, 2, 0, 2, 1.
	1  1
x

x1
x2
x3
x4


0s  2t
s  0t
0s  2t
0s  t
  s

0
1
0
0
  t

2
0
2
1

.
t  x 4 s  x2

1
0
0
0
0
0
0
0
0
1
0
0
2
2
0
0
 1I  A

0
0
1
1
0
0
0
0
0
5
1
0
0
10
0
2

	1  1
	1  1, 	2  2, 	3  3. 	1  1
	  12	  2	  3  0
 	  12
	  2	  3.
	I  A

	  1
0
1
1
0
	  1
0
0
0
5
	  2
0
0
10
0
	  3 
A
A

1
0
1
1
0
1
0
0
0
5
2
0
0
10
0
3

EXAMPLE 6 Finding Eigenvalues and Eigenvectors
430 Chapter 7 Eigenvalues and Eigenvectors
matrices are required. These numerical methods can be found in texts on advanced linear
algebra and numerical analysis.
There are a few types of matrices for which eigenvalues are easy to find. The next
theorem states that the eigenvalues of an triangular matrix are the entries on the main
diagonal. Its proof follows from the fact that the determinant of a triangular matrix is the
product of its diagonal elements.
Find the eigenvalues of each matrix.
(a) (b)
SOLUTION (a) Without using Theorem 7.3, you can find that
 	  2	  1	  3.
	I  A

	  2
1
5
0
	  1
3
0
0
	  3 
A

1
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
4
0
0
0
0
0
3
 A   2
1
5
0
1
3
0
0
3

EXAMPLE 7 Finding Eigenvalues of Diagonal and Triangular Matrices
n  n
Many computer software programs and graphing utilities have built-in programs to approximate the
eigenvalues and eigenvectors of an matrix. If you enter the matrix from Example 6, you
should obtain the four eigenvalues
Your computer software program or graphing utility should also be able to produce a matrix in
which the columns are the corresponding eigenvectors, which are sometimes scalar multiples
of those you would obtain by hand calculations. Keystrokes and programming syntax for these
utilities/programs applicable to Example 6 are provided in the Online Technology Guide, available
at college.hmco.com/pic/larsonELA6e.
1 1 2 3.
n  n A Technology
Note
If is an triangular matrix, then its eigenvalues are the entries on its main
diagonal.
A n  n THEOREM 7.3
Eigenvalues of
Triangular Matrices
Section 7.1 Eigenvalues and Eigenvectors 431
So, the eigenvalues are and which are simply the main
diagonal entries of
(b) In this case, use Theorem 7.3 to conclude that the eigenvalues are the main diagonal
entries and
Eigenvalues and Eigenvectors of Linear Transformations
This section began with definitions of eigenvalues and eigenvectors in terms of matrices.
They can also be defined in terms of linear transformations. A number is called an
eigenvalue of a linear transformation if there is a nonzero vector such that
The vector is called an eigenvector of corresponding to and the set of all
eigenvectors of (with the zero vector) is called the eigenspace of
Consider the linear transformation whose matrix relative to the standard
basis is
In Example 5 of Section 6.4, you found that the matrix of relative to the basis is the
diagonal matrix
The question now is: “For a given transformation can you find a basis whose
corresponding matrix is diagonal?” The next example gives an indication of the answer.
Find the eigenvalues and corresponding eigenspaces of
SOLUTION Because
 	  2	2  2	  8  	  22	  4,
 	  2	  12  9
	I  A

	  1
3
0
3
	  1
0
0
0
	  2
A  
1
3
0
3
1
0
0
0
2

.
EXAMPLE 8 Finding Eigenvalues and Eigenspaces
T, B
A  
4
0
0
0
2
0
0
0
2

.
T B
A  
1
3
0
3
1
0
0
0
2

.
T: R3→R3
,
	 	.
Tx  	x. x T 	,
T: V→V x
	
	 	 5  3. 1  1, 	 2  2, 	 3  0, 	 4  4,
A.
	 	 3  3, 1  2, 	 2  1,
Standard basis:
B  {1, 0, 0, 0, 1, 0, 0, 0, 1}
Nonstandard basis:
B  {1, 1, 0, 1, 1, 0, 0, 0, 1}
432 Chapter 7 Eigenvalues and Eigenvectors
the eigenvalues of are and The eigenspaces for these two eigenvalues
are as follows.
Basis for
Basis for
Example 8 illustrates two important and perhaps surprising results.
1. Let be the linear transformation whose standard matrix is and let be the
basis of made up of the three linearly independent eigenvectors found in Example 8.
Then the matrix of relative to the basis is diagonal.
2. The main diagonal entries of the matrix are the eigenvalues of
The next section formalizes these two results and also characterizes linear transformations that can be represented by diagonal matrices.
A A.
A, T B,
R3
T: R A, B 3→R3
B  2  2 2  1, 1, 0, 0, 0, 1
B 1  4 1  1, 1, 0
	 	 2  2. A 1  4
Nonstandard basis:
B  {1, 1, 0, 1, 1, 0, 0, 0, 1}
A  
4
0
0
0
2
0
0
0
2

Eigenvalues of A Eigenvectors of A
SECTION 7.1 Exercises
In Exercises 1–8, verify that is an eigenvalue of and that is
a corresponding eigenvector.
1.
2.
3.
4.
5.
6.
7.
8.
9. Use and from Exercise 3 to show that
(a) for any real number
(b) for any real number
10. Use and from Exercise 5 to show that
(a) for any real number
(b) for any real number
(c) for any real number Acx c. 3  3cx3
Acx c. 2  cx2
Acx c. 1  2cx1
x 	 i i A, ,
Acx c. 2  2cx2
Acx c. 1  0cx1
	 i xi A, ,
A  
4
0
0
1
2
0
3
1
3

,
	1  4, x1  1, 0, 0
	2  2, x2  1, 2, 0
	3  3, x3  2, 1, 1
A  
0
0
1
1
0
0
0
1
0

, 	1  1, x1  1, 1, 1
A  
2
2
1
2
1
2
3
6
0

,
	1  5, x1  1, 2, 1
	2  3, x2  2, 1, 0
	3  3, x3  3, 0, 1
A  
2
0
0
3
1
0
1
2
3

,
	1  2, x1  1, 0, 0
	2  1, x2  1, 1, 0
	3  3, x3  5, 1, 2
A  
2
1
4
1, 	1  2, x1  1, 1
	2  3, x2  4, 1
A  
1
1
1
1, 	1  0, x1  1, 1
	2  2, x2  1, 1
A  
4
2
5
3, 	1  1, x1  1, 1
	2  2, x2  5, 2
A  
1
0
0
1, 	1  1, x1  1, 0
	2  1, x2  0, 1
	i A xi
Section 7.1 Eigenvalues and Eigenvectors 433
In Exercises 11–14, determine whether is an eigenvector of
11. 12.
(a) (a)
(b) (b)
(c) (c)
(d) (d)
13.
14.
In Exercises 15–28, find (a) the characteristic equation and (b) the
eigenvalues (and corresponding eigenvectors) of the matrix.
15. 16.
17. 18.
19. 20.
21. 22.
23. 24.
25. 26.
27. 28.
In Exercises 29–40, use a graphing utility with matrix capabilities
or a computer software program to find the eigenvalues of the
matrix.
29. 30.
31. 32.
33. 34.
35. 36.
37. 38.
39. 40.
In Exercises 41– 48, demonstrate the Cayley-Hamilton Theorem
for the given matrix. The Cayley-Hamilton Theorem states that a
matrix satisfies its characteristic equation. For example, the characteristic equation of
is and by the theorem you have
41. 42.
43. 44.
45. 46. 
3
2
5
1
4
5
4
0
6   0
1
0
2
3
0
1
1
1

 4
2
1
1   2
1
2
5

6
1
1
5   4
3
0
2
A2  6A  11I2  O.
	2  6	  11  0,
A  
1
2
3
5

1
4
0
0
1
4
0
0
0
0
1
2
0
0
1
2   1
2
3
4
1
2
3
4
2
4
6
8
3
6
9
12

1
1
2
1
3
4
0
0
3
3
1
0
3
3
1
0   1
0
2
0
0
1
0
2
1
0
2
0
1
1
2
2


1
1
1
2
0
1
1
1
2   2
1
1
4
0
4
2
1
5

1
2
2
0
0
1
5
0
5
1
4
3   0
1
3
0
1
2
1
6
0
5
1
4
4


6
3
2
1   7
2
2
3

2
3
3
6   4
2
5
3

3
4
0
0
0
1
0
0
0
0
2
0
0
0
1
2   2
0
0
0
0
2
0
0
0
0
3
4
0
0
0
0


1
2
3
2
3
2
13
2
9
2
5
2
10
8
  0
4
0
3
4
0
5
10
4

 3
3
1
2
4
2
3
9
5   1
2
6
2
5
6
2
2
3


3
0
0
2
0
2
1
2
0   2
0
0
2
3
1
3
2
2


5
3
4
0
7
2
0
0
3   2
0
0
0
3
0
1
4
1


1
4
1
2
1
4
 0 1
1
2
3
2
1
 1
2
4
8   6
2
3
1
A  
1
0
1
0
2
2
5
4
9

A  
1
2
3
1
0
3
1
2
1

x  1, 0 x  5, 3
x  1, 2 x  4, 8
x  2, 1 x  8, 4
x  1, 2 x  4, 4
A  
3
5
10
2
A    7
2
2
4
x A.
(a)
(b)
(c)
(d) x  26  3,26  6, 3
x  0, 0, 0
x  5, 2, 1
x  1, 1, 0
(a)
(b)
(c)
(d) x  1, 0, 1
x  2, 2, 0
x  2, 0, 6
x  2, 4, 6
434 Chapter 7 Eigenvalues and Eigenvectors
47. 48.
49. Perform the computational checks listed below on the eigenvalues found in Exercises 15–27 odd.
(a) The sum of the eigenvalues equals the sum of the
diagonal entries of the matrix. (This sum is called the
trace of )
(b) The product of the eigenvalues equals
(If is an eigenvalue of multiplicity remember to enter it
times in the sum or product of these checks.)
50. Perform the computational checks listed below on the eigenvalues found in Exercises 16–28 even.
(a) The sum of the eigenvalues equals the sum of the
diagonal entries of the matrix. (This sum is called the
trace of )
(b) The product of the eigenvalues equals
(If is an eigenvalue of multiplicity remember to enter it
times in the sum or product of these checks.)
51. Show that if is an matrix whose th row is identical to
the th row of then 1 is an eigenvalue of
52. Prove that is an eigenvalue of if and only if is
singular.
53. Writing For an invertible matrix prove that and have
the same eigenvectors. How are the eigenvalues of related to
the eigenvalues of
54. Writing Prove that and have the same eigenvalues. Are
the eigenspaces the same?
55. Prove that the constant term of the characteristic polynomial
is
56. Let be represented by where is a
fixed vector in Show that the eigenvalues of (the standard
matrix of ) are 0 and 1.
57. Guided Proof Prove that a triangular matrix is nonsingular if
and only if its eigenvalues are real and nonzero.
Getting Started: Because this is an “if and only if” statement,
you must prove that the statement is true in both directions.
Review Theorems 3.2 and 3.7.
(i) To prove the statement in one direction, assume that the
triangular matrix is nonsingular. Use your knowledge
of nonsingular and triangular matrices and determinants
to conclude that the entries on the main diagonal of
are nonzero.
(ii) Because is triangular, you can use Theorem 7.3 and
part (i) to conclude that the eigenvalues are real and
nonzero.
(iii) To prove the statement in the other direction, assume
that the eigenvalues of the triangular matrix are real
and nonzero. Repeat parts (i) and (ii) in reverse order to
prove that is nonsingular.
58. Guided Proof Prove that if then 0 is the only
eigenvalue of
Getting Started: You need to show that if there exists a
nonzero vector and a real number such that then
if must be zero.
(i) Because you can write as
(ii) Use the fact that and the properties of matrix
multiplication to conclude that
(iii) Because is a zero matrix, you can conclude that
must be zero.
59. If the eigenvalues of
are and what are the possible values of
and
60. Show that
has no real eigenvalues.
True or False? In Exercises 61 and 62, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false,
provide an example that shows the statement is not true in all cases
or cite an appropriate statement from the text.
61. (a) The scalar is an eigenvalue of an matrix if there
exists a vector such that
(b) If A is an matrix with eigenvalue and corresponding
eigenvector then every nonzero scalar multiple of
is also an eigenvector of
(c) To find the eigenvalue(s) of an matrix you can
solve the characteristic equation, det
62. (a) Geometrically, if is an eigenvalue of a matrix and is an
eigenvector of corresponding to then multiplying by
produces a vector parallel to
(b) An matrix can have only one eigenvalue.
(c) If is an matrix with an eigenvalue then the set of
all eigenvectors of is a subspace of Rn 	 .
A n  n 	,
n  n A
A 	x x.
A 	, x
	 A x
	I  A  0.
n  n A,
x A.
x,
n  n 	
x Ax  	x.
	 n  n A
A   0
1
1
0
d?
	1  0 	 2  1, a
A  
a
0
b
d
A 	 2
A2
x  	2
x.
Ax  	x
A AAx. 2 A x 2  A  A,
A 	 2  O,
x 	 Ax  	x,
A.
A2  O,
A
A
A
A
A
T
R A 2.
T: R Tv  projuv, u 2→R2
±A.
AT A
A1
?
A
A1 A, A
	  0 A A
i I, A.
A n  n i
	 k, k
A n .
A.
n
	 k, k
A n .
A.
n

3
1
0
1
3
4
0
2
3   1
0
2
0
3
0
4
1
1

Section 7.2 Diagonalization 435
In Exercises 63–66, find the dimension of the eigenspace corresponding to the eigenvalue
63. 64.
65. 66.
67. Calculus Let be given by
Show that is an eigenvalue of with corresponding
eigenvector
68. Calculus For the linear transformation given in Exercise 67,
find the eigenvalue corresponding to the eigenvector
69. Let be represented by
Find the eigenvalues and the eigenvectors of relative to the
standard basis
70. Let be represented by
Find the eigenvalues and eigenvectors of relative to the
standard basis
71. Let be represented by
Find the eigenvalues and eigenvectors of relative to the
standard basis
72. A square matrix is called idempotent if What are the
possible eigenvalues of an idempotent matrix?
73. A square matrix is called nilpotent if there exists a positive
integer such that What are the possible eigenvalues of
a nilpotent matrix?
74. Find all values of the angle for which the matrix
has real eigenvalues. Interpret your answer geometrically.
75. Let be an matrix such that the sum of the entries in each
row is a fixed constant Prove that is an eigenvalue of
Illustrate this result with a specific example.
r. r A.
A n  n
A  
cos 
sin 
sin 
cos  

A k k  0.
A
A A 2  A.
B  
1
0
0
0, 
0
0
1
0, 
0
1
0
0, 
0
0
0
1.
T
T 	
a
c
b
d
   a
2a
 c
 2c
 d
 2d
b
2b
 d
 2d .
T: M2,2→M2,2
1, x, x 2.
T
a1  2a2x  a2 x2
.
Ta0  a1x  a2x2
  2a0  a1  a2 
T: P2→P2
1, x, x 2.
T
4a0  4a1  10a2 x  4a2 x2
.
Ta0  a1x  a2x2  3a1  5a2 
T:P2→P2
fx  e2x
.
fx  ex
.
	  1 T
T: C0, 1→C0, 1 T f   f.
A  
3
0
0
1
3
0
1
1
3
 A  
3
0
0
1
3
0
0
1
3

A  
3
0
0
1
3
0
0
0
3
 A  
3
0
0
0
3
0
0
0
3

	  3.
Diagonalization
The preceding section discussed the eigenvalue problem. In this section, you will look at
another classic problem in linear algebra called the diagonalization problem. Expressed in
terms of matrices*, the problem is this: “For a square matrix does there exist an invertible matrix such that is diagonal?”
Recall from Section 6.4 that two square matrices and are called similar if there
exists an invertible matrix such that
Matrices that are similar to diagonal matrices are called diagonalizable.
B  P1 P AP.
BA
P1 P AP
A,
7.2
An matrix is diagonalizable if is similar to a diagonal matrix. That is, is
diagonalizable if there exists an invertible matrix such that is a diagonal matrix. P1 P AP
Definition of a n  n A A A
Diagonalizable Matrix
* At the end of this section, the diagonalization problem will be expressed in terms of linear transformations.
436 Chapter 7 Eigenvalues and Eigenvectors
Provided with this definition, the diagonalization problem can be stated as follows:
“Which square matrices are diagonalizable?” Clearly, every diagonal matrix is diagonalizable, because the identity matrix can play the role of to yield Example 1
shows another example of a diagonalizable matrix.
The matrix from Example 5 in Section 6.4,
is diagonalizable because
has the property
As indicated in Example 8 in the preceding section, the eigenvalue problem is related
closely to the diagonalization problem. The next two theorems shed more light on this
relationship. The first theorem tells you that similar matrices must have the same eigenvalues.
PROOF Because and are similar, there exists an invertible matrix such that By
the properties of determinants, it follows that
 	I  A.
 P1
P	I  A
 P1
P	I  A
 P1
	I  AP
 P1
	I  AP
	I  B  	I  P1
AP  P1
	IP  P1AP
B  P1 BA P AP.
P1
AP  
4
0
0
0
2
0
0
0
2

.
P  
1
1
0
1
1
0
0
0
1

A  
1
3
0
3
1
0
0
0
2

,
EXAMPLE 1 A Diagonalizable Matrix
D  I I P 1
DI.
D
If and are similar matrices, then they have the same eigenvalues. BA n  n
THEOREM 7.4
Similar Matrices Have
the Same Eigenvalues
Section 7.2 Diagonalization 437
But this means that and have the same characteristic polynomial. So, they must have
the same eigenvalues.
The matrices and are similar.
and
Use Theorem 7.4 to find the eigenvalues of and
SOLUTION Because is a diagonal matrix, its eigenvalues are simply the entries on its main
diagonal—that is,
and
Moreover, because is said to be similar to you know from Theorem 7.4 that has the
same eigenvalues. Check this by showing that the characteristic polynomial of is
REMARK : Example 2 simply states that matrices and are similar. Try checking
using the matrices
and
In fact, the columns of are precisely the eigenvectors of corresponding to the
eigenvalues 1, 2, and 3.
The two diagonalizable matrices in Examples 1 and 2 provide a clue to the diagonalization problem. Each of these matrices has a set of three linearly independent eigenvectors.
(See Example 3.) This is characteristic of diagonalizable matrices, as stated in Theorem 7.5.
P A
P1   1
1
0
0
2
1
0
1
1
 P  . 
1
1
1
0
1
1
0
1
2

D  P1
AP
DA
	I  A  	  1	  2	  3.
A
A D, A
	3  3.
	2  2,
	1  1,
D
A D.
D  
1
0
0
0
2
0
0
0
3
 A   1
1
1
0
1
2
0
1
4

DA
EXAMPLE 2 Finding Eigenvalues of Similar Matrices
BA
An matrix is diagonalizable if and only if it has linearly independent
eigenvectors.
n  n A n THEOREM 7.5
Condition for
Diagonalization
438 Chapter 7 Eigenvalues and Eigenvectors
PROOF First, assume is diagonalizable. Then there exists an invertible matrix such that
is diagonal. Letting the main entries of be and the column
vectors of be produces
Because which implies
In other words, for each column vector This means that the column vectors
of are eigenvectors of Moreover, because is invertible, its column vectors are
linearly independent. So, has linearly independent eigenvectors.
Conversely, assume has linearly independent eigenvectors with
corresponding eigenvalues Let be the matrix whose columns are these
eigenvectors. That is, Because each is an eigenvector of
you have and
The right-hand matrix in this equation can be written as the matrix product below.
Finally, because the vectors are linearly independent, is invertible and
you can write the equation as which means that is diagonalizable.
A key result of this proof is the fact that for diagonalizable matrices, the columns of
consist of the linearly independent eigenvectors. Example 3 verifies this important
property for the matrices in Examples 1 and 2.
(a) The matrix in Example 1 has the eigenvalues and corresponding eigenvectors listed
below.
	1  4, p1  
1
1
0

; 	2  2, p2   1
1
0

; 	3  2, p3  
0
0
1

EXAMPLE 3 Diagonalizable Matrices
n
P
P A 1 AP  PD AP  D,
p1, p2, . . . , pn P
AP  p1  p2  . . .  pn

	1
0
.
.
.
0
0
	2
.
.
.
0
...
...
...
0
0
.
.
.
	n
  PD
AP  Ap1  p2  ...  pn  	1p1  	2p2  ...  	npn.
Api  	ipi
p A, P  i p1  p2  ...  pn.
	1, 	2, . . . , 	n . P n
p1, p2 nA , . . . , pn
nA
pi P A. P
pi Ap . i  	ipi
Ap1  Ap2  . . .  Apn  	1p1  	2p2  . . .  	npn.
P AP  PD, 1
AP  D,
 	1p1  	2p2  . . .  	npn.
PD  p1  p2  ...  pn 

	1
0
.
.
.
0
0
	2
.
.
.
0
...
...
...
0
0
.
.
.
	n

p1, p2 P , . . . , pn
	1, 	2 P D , . . . , 	n 1
AP  D
A P
Section 7.2 Diagonalization 439
The matrix whose columns correspond to these eigenvectors is
Moreover, because is row-equivalent to the identity matrix, the eigenvectors
and are linearly independent.
(b) The matrix in Example 2 has the eigenvalues and corresponding eigenvectors listed
below.
The matrix whose columns correspond to these eigenvectors is
Again, because is row-equivalent to the identity matrix, the eigenvectors and
are linearly independent.
The second part of the proof of Theorem 7.5 and Example 3 suggest the steps listed below
for diagonalizing a matrix.
Show that the matrix is not diagonalizable.
A  
1
0
2
1
A
EXAMPLE 4 A Matrix That Is Not Diagonalizable
p3
p1, p2 P ,
P  
1
1
1
0
1
1
0
1
2

.
P
	1  1, p1  
1
1
1

; 	2  2, p2  
0
1
1

; 	3  3, p3  
0
1
2

p3
p1, p2 P ,
P  
1
1
0
1
1
0
0
0
1

.
P
Let be an matrix.
1. Find linearly independent eigenvectors for with corresponding
eigenvalues If linearly independent eigenvectors do not exist, then
is not diagonalizable.
2. If has linearly independent eigenvectors, let be the matrix whose columns
consist of these eigenvectors. That is,
3. The diagonal matrix will have the eigenvalues on its main
diagonal (and zeros elsewhere). Note that the order of the eigenvectors used to form
will determine the order in which the eigenvalues appear on the main diagonal of D.
P
	1, 	2 D  P , . . . , 	n 1
AP
P  p1  p2  . . .  pn.
nA P n  n
A
	1, 	2, . . . , 	n . n
n p1, p2, . . . , pn A
Steps for Diagonalizing A n  n
an Square Matrix n  n
440 Chapter 7 Eigenvalues and Eigenvectors
SOLUTION Because is triangular, the eigenvalues are simply the entries on the main diagonal. So, the
only eigenvalue is The matrix has the reduced row-echelon form shown
below.
This implies that and letting you can find that every eigenvector of has
the form
So, does not have two linearly independent eigenvectors, and you can conclude that is
not diagonalizable.
Show that the matrix is diagonalizable.
Then find a matrix such that is diagonal.
SOLUTION The characteristic polynomial of is
So, the eigenvalues of are From these eigenvalues you
obtain the reduced row-echelon forms and corresponding eigenvectors shown below.
Eigenvector

1
1
1   1
0
0
0
1
0
1
1
0 3I  A   2
1
3
1
0
1
1
1
4

 1
1
4   1
0
0
0
1
0
1
4
1
4
0
 2I  A  
3
1
3
1
5
1
1
1
1


1
0
1   1
0
0
0
1
0
1
0
0
 2I  A   1
1
3
1
1
1
1
1
3

A 	1  2, 	2  2, and 	3  3.
	I  A

	  1
1
3
1
	  3
1
1
1
	  1  	  2	  2	  3.
A
P1 P AP
A   1
1
3
1
3
1
1
1
1

A
EXAMPLE 5 Diagonalizing a Matrix
A A
x  
x1
x2
  
t
0  t

1
0.
x2  0, x1  t, A

0
0
1
0 I  A  
0
0
2
0
	  1. I  A
A
Section 7.2 Diagonalization 441
Form the matrix whose columns are the eigenvectors just obtained.
This matrix is nonsingular, which implies that the eigenvectors are linearly independent and
is diagonalizable. The inverse of is
and it follows that
Show that the matrix is diagonalizable.
Then find a matrix such that is diagonal.
SOLUTION In Example 6 in Section 7.1, you found that the three eigenvalues and
have the eigenvectors shown below.
The matrix whose columns consist of these eigenvectors is
P

0
1
0
0
2
0
2
1
0
5
1
0
0
5
0
1

.
	1:

0
1
0
0

,

2
0
2
1
 	2:

0
5
1
0
 	3:

0
5
0
1

	3  3
	1  1, 	2  2,
P1 P AP
A

1
0
1
1
0
1
0
0
0
5
2
0
0
10
0
3

A
EXAMPLE 6 Diagonalizing a Matrix
P1AP  
2
0
0
0
2
0
0
0
3

.
P1  
1
1
5
1
5
1
0
1
0
1
5
1
5

,
A P
P  
1
0
1
1
1
4
1
1
1

P
442 Chapter 7 Eigenvalues and Eigenvectors
Because is invertible (check this), its column vectors form a linearly independent set.
So, is diagonalizable, and you have
For a square matrix of order to be diagonalizable, the sum of the dimensions of the
eigenspaces must be equal to One way this can happen is if has distinct eigenvalues.
So, you have the next theorem.
PROOF Let be distinct eigenvalues of corresponding to the eigenvectors
To begin, assume the set of eigenvectors is linearly dependent. Moreover,
consider the eigenvectors to be ordered so that the first eigenvectors are linearly independent, but the first are dependent, where Then can be written as a linear
combination of the first eigenvectors:
Equation 1
where the ’s are not all zero. Multiplication of both sides of Equation 1 by yields
Equation 2
whereas multiplication of Equation 1 by yields
Equation 3
Now, subtracting Equation 2 from Equation 3 produces
and, using the fact that the first eigenvectors are linearly independent, you can conclude
that all coefficients of this equation must be zero. That is,
c1	m1  	1  c2	m1  	 2  ...  cm	 m1  	 m  0.
m
c1	 m1  	1x1  c2	 m1  	 2x2  ...  cm	 m1  	 mxm  0,
	m1xm 1  c1	m 1x1  c2	m 1x2  ...  cm	m1xm.
	m 1
	m 1xm 1  c1	1x1  c2	 2x2  ...  cm	mxm,
Axm 1  Ac1x1  Ac2x2  ...  Acmxm
ci A
xm1  c1x1  c2 x2  ...  cmxm,
m
x m < m 1 m  1 n.
m
x1, x2, . . . , xn .
	1, 	2, . . . , 	n n A
n. nA
nA
P1
AP

1
0
0
0
0
1
0
0
0
0
2
0
0
0
0
3

.
A
P 1

5
2
1
2
1
1
2
1
0
0
0
5
0
1
0
5
0
0
1

P
If an matrix has distinct eigenvalues, then the corresponding eigenvectors are
linearly independent and is diagonalizable. A
n  n nA THEOREM 7.6
Sufficient Condition
for Diagonalization
Section 7.2 Diagonalization 443
Because all the eigenvalues are distinct, it follows that But this
result contradicts our assumption that can be written as a linear combination of the
first eigenvectors. So, the set of eigenvectors is linearly independent, and from Theorem
7.5 you can conclude that is diagonalizable.
Determine whether the matrix is diagonalizable.
SOLUTION Because is a triangular matrix, its eigenvalues are the main diagonal entries
Moreover, because these three values are distinct, you can conclude from Theorem 7.6 that
is diagonalizable.
REMARK : Remember that the condition in Theorem 7.6 is sufficient but not necessary for
diagonalization, as demonstrated in Example 6. In other words, a diagonalizable matrix
need not have distinct eigenvalues.
Diagonalization and Linear Transformations
So far in this section, the diagonalization problem has been considered in terms of
matrices. In terms of linear transformations, the diagonalization problem can be stated as
follows. For a linear transformation
does there exist a basis for such that the matrix for relative to is diagonal? The
answer is “yes,” provided the standard matrix for is diagonalizable.
Let be the linear transformation represented by
If possible, find a basis for such that the matrix for relative to is diagonal. R T B 3 B
Tx1, x2, x3  x1  x2  x3, x1  3x2  x3, 3x1  x2  x3.
T: R3→R3
EXAMPLE 8 Finding a Diagonal Matrix for a Linear Transformation
T
VB T B
T: V→V,
A
	1  1, 	2  0, 	3  3.
A
A  
1
0
0
2
0
0
1
1
3

A
EXAMPLE 7 Determining Whether a Matrix Is Diagonalizable
A
m
xm1
ci  0, i  1, 2, . . . , m.
444 Chapter 7 Eigenvalues and Eigenvectors
SOLUTION The standard matrix for is represented by
From Example 5, you know that is diagonalizable. So, the three linearly independent
eigenvectors found in Example 5 can be used to form the basis That is,
The matrix for relative to this basis is
D  
2
0
0
0
2
0
0
0
3

.
T
B  1, 0, 1, 1, 1, 4, 1, 1, 1.
B.
A
A   1
1
3
1
3
1
1
1
1

.
T
SECTION 7.2 Exercises
In Exercises 1–8, verify that is diagonalizable by computing
1.
2.
3.
4.
5.
6.
7.
8.
In Exercises 9–16, show that the matrix is not diagonalizable.
9. 10.
11. 12.
13. 14.
15. 16.
(See Exercise 37, (See Exercise 38,
Section 7.1.) Section 7.1.)
In Exercises 17–20, find the eigenvalues of the matrix and determine
whether there is a sufficient number to guarantee that the matrix is
diagonalizable. (Recall that the matrix may be diagonalizable even
though it is not guaranteed to be diagonalizable by Theorem 7.6.)
17. 18.
19. 20. 
4
0
0
3
1
0
2
1
2   3
3
1
2
4
2
3
9
5


2
5
0
2   1
1
1
1

1
1
2
1
3
4
0
0
3
3
1
0
3
3
1
0   1
0
2
0
0
1
0
2
1
0
2
0
1
1
2
2


2
0
0
1
1
0
1
2
1   1
0
0
2
1
0
1
4
2

 1
2
0
1   1
0
1
1
 1
2
1
2
1   0
5
0
0
P

1
1
1
1
1
1
1
1
0
0
1
1
1
1
0
0
 A

0.80
0.10
0.05
0.05
0.10
0.80
0.05
0.05
0.05
0.05
0.80
0.10
0.05
0.05
0.10
0.80
,
A  
4
0
0
1
2
0
3
1
3

, P  
1
0
0
1
2
0
2
1
1

A  
2
0
0
3
1
0
1
2
3

, P  
1
0
0
1
1
0
5
1
2

A  
1
0
4
1
3
2
0
0
5

, P  
0
0
1
1
4
2
3
0
2

A  
4
2
5
3, P  
1
1
5
2
A  
2
1
4
1, P  
1
1
4
1
A   1
1
3
5, P  
3
1
1
1
A  
11
3
36
10, P  
3
1
4
1
P1
AP.
A
Section 7.2 Diagonalization 445
In Exercises 21–34, for each matrix find (if possible) a nonsingular matrix such that is diagonal. Verify that is
a diagonal matrix with the eigenvalues on the diagonal.
21. 22.
(See Exercise 15, (See Exercise 16,
Section 7.1.) Section 7.1.)
23. 24.
(See Exercise 17, (See Exercise 18,
Section 7.1.) Section 7.1.)
25. 26.
(See Exercise 21, (See Exercise 22,
Section 7.1.) Section 7.1.)
27. 28.
(See Exercise 23, (See Exercise 24,
Section 7.1.) Section 7.1.)
29. 30.
(See Exercise 25, (See Exercise 26,
Section 7.1.) Section 7.1.)
31. 32.
33. 34.
In Exercises 35–38, find a basis for the domain of such that the
matrix of relative to is diagonal.
35.
36.
37.
38.
39. Let be a diagonalizable matrix and an invertible
matrix such that is the diagonal form of
Prove that
(a) where is a positive integer.
(b) where is a positive integer.
40. Let be distinct eigenvalues of the
matrix Use the result of Exercise 39 to find the eigenvalues
of .
In Exercises 41–44, use the result of Exercise 39 to find the
indicated power of
41. 42.
43. 44.
True or False? In Exercises 45 and 46, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false,
provide an example that shows the statement is not true in all cases
or cite an appropriate statement from the text.
45. (a) If and are similar matrices, then they always have
the same characteristic polynomial equation.
(b) The fact that an matrix has distinct eigenvalues
does not guarantee that is diagonalizable.
46. (a) If is a diagonalizable matrix, then it has linearly
independent eigenvectors.
(b) If an matrix is diagonalizable, then it must have
distinct eigenvalues.
47. Writing Can a matrix be similar to two different diagonal
matrices? Explain your answer.
48. Are the two matrices similar? If so, find a matrix such that
49. Prove that if is diagonalizable, then is diagonalizable.
50. Prove that the matrix
is diagonalizable if and is not diagonalizable
if 4bc > a  d2
.
4bc < a  d2
A  
a
c
b
d
AT A
B  
3
0
0
0
2
0
0
0
1
 A  
1
0
0
0
2
0
0
0
3

B  P1
AP.
P
n  n A n
A n
A
n  n nA
BA n  n
A  
2
0
3
0
2
0
2
2
3

, A5 A   3
3
1
2
4
2
3
9
5

, A8
A  
1
2
3
0, A7 A  
10
6
18
11, A6
A.
Ak
A.
	 n n  n 1, 	2, . . . , 	n
A k k  PBk
P1
,
B k k  P1
Ak
P,
B  P A. 1 n  n AP
A n  n P
3a1  4a2x  a2 x 2
T: P2→P2: Ta0  a1x  a2x 2
  2a0  a2  
T: P1→P1: Ta  bx  a  a  2bx
x  2y
T: R3→R3
: T x, y, z  2x  2y  3z, 2x  y  6z,
T: R2→R2: Tx, y  x  y, x  y
T B
B T
A

1
1
0
0
0
0
0
1
0
1
1
0
0
0
0
1
 A

2
3
0
0
0
1
1
0
0
0
1
1
0
0
0
2

A  
4
2
0
0
2
2
0
0
2
 A  
1
1
1
0
2
0
0
1
2

A   1
2
3
2
3
2
13
2
9
2
5
2
10
8
 A   0
4
0
3
4
0
5
10
4

A   3
3
1
2
4
2
3
9
5
 A   1
2
6
2
5
6
2
2
3

A  
3
0
0
2
0
2
1
2
0
 A  
2
0
0
2
3
1
3
2
2

A  
1
4
1
2
1
4
0 A  
1
1
2
3
2
1
A   1
2
4
8
A    6
2
3
1
P 1 P AP 1 P AP
A,
446 Chapter 7 Eigenvalues and Eigenvectors
51. Prove that if is diagonalizable with real eigenvalues
then
52. Calculus If is a real number, then can be defined by the
series
In a similar way, if is a square matrix, you can define by the
series
Evaluate where is the indicated square matrix.
(a) (b)
(c) (d)
(e)
53. Guided Proof Prove that if the eigenvalues of a diagonalizable matrix are all then the matrix is equal to its
inverse.
Getting Started: To show that the matrix is equal to its
inverse, use the fact that there exists an invertible matrix such
that where is a diagonal matrix with along
its main diagonal.
(i) Let where D is a diagonal matrix with
along its main diagonal.
(ii) Find in terms of and
(iii) Use the properties of the inverse of a product of
matrices and the fact that is diagonal to expand to
find
(iv) Conclude that
54. Guided Proof Prove that nonzero nilpotent matrices are not
diagonalizable.
Getting Started: From Exercise 73 in Section 7.1, you know
that 0 is the only eigenvalue of the nilpotent matrix Show that
it is impossible for to be diagonalizable.
(i) Assume is diagonalizable, so there exists an invertible
matrix such that where is the zero
matrix.
(ii) Find in terms of and
(iii) Find a contradiction and conclude that nonzero nilpotent matrices are not diagonalizable.
55. Prove that if is a nonsingular diagonalizable matrix, then
is also diagonalizable.
In Exercises 56 and 57, show that the matrix is not diagonalizable.
Then write a brief statement explaining your reasoning.
56. 57. 
k
0
0
k   3
0
k
3, k  0
A1 A
P D. 1 A P, ,
P P1
AP  D, D
A
A
A.
A1  A.
A1.
D
P D. 1 A P, ,
D  P ±1 1
AP,
D  P D ±1 1
AP,
P
A ±1,
X  
2
0
0
2
X  
0
1
1
0
X    1
1
0
0
X  
0
0
0
0
X    1
0
0
1
XeX,
eX  I  X 
1
2!
X2 
1
3!
X3 
1
4!
X4  . . . .
eX X
ex  1  x  x2
2!
 x3
3!
 x4
4!
 . . . .
ex x
A  	1	2
. . . 	n 	 . 2, . . . , 	n,
	1 A n ,
Symmetric Matrices and Orthogonal Diagonalization
For most matrices you must go through much of the diagonalization process before you can
finally determine whether diagonalization is possible. One exception is a triangular matrix
with distinct entries on the main diagonal. Such a matrix can be recognized as diagonalizable by simple inspection. In this section you will study another type of matrix that is
guaranteed to be diagonalizable: a symmetric matrix.
You can determine easily whether a matrix is symmetric by checking whether it is
symmetric with respect to its main diagonal.
7.3
A square matrix is symmetric if it is equal to its transpose:
A  AT.
Definition of A
Symmetric Matrix
Section 7.3 Symmetric Matrices and Orthogonal Diagonalization 447
The matrices and are symmetric, but the matrix is not.
Symmetric
Symmetric
Nonsymmetric
Nonsymmetric matrices have the following special properties that are not exhibited by
symmetric matrices.
1. A nonsymmetric matrix may not be diagonalizable.
2. A nonsymmetric matrix can have eigenvalues that are not real. For instance, the matrix
has a characteristic equation of So, its eigenvalues are the imaginary
numbers and
3. For a nonsymmetric matrix, the number of linearly independent eigenvectors corresponding to an eigenvalue can be less than the multiplicity of the eigenvalue. (See
Example 4, Section 7.2.)
None of these three properties is exhibited by symmetric matrices.
REMARK : Theorem 7.7 is called the Real Spectral Theorem, and the set of eigenvalues
of is called the spectrum of
A general proof of Theorem 7.7 is beyond the scope of this text. The next example
verifies that every symmetric matrix is diagonalizable. 2  2
A A.
	 	2  i. 1  i
	2  1  0.
A  
0
1
1
0
C  
3
1
1
2
4
0
1
0
5

B  
4
3
3
1
A   0
1
2
1
3
0
2
0
5

BA C
EXAMPLE 1 Symmetric Matrices and Nonsymmetric Matrices
If you have access to a computer
software program or a graphing
utility that can find eigenvalues,
try the following experiment. Pick
an arbitrary square matrix and
calculate its eigenvalues. Can
you find a matrix for which the
eigenvalues are not real?
Now pick an arbitrary symmetric
matrix and calculate its eigenvalues. Can you find a symmetric
matrix for which the eigenvalues
are not real? What can you
conclude about the eigenvalues
of a symmetric matrix?
Discovery
If is an symmetric matrix, then the following properties are true.
1. is diagonalizable.
2. All eigenvalues of are real.
3. If is an eigenvalue of with multiplicity then has linearly independent
eigenvectors. That is, the eigenspace of has dimension 	 k.
	 A k, 	 k
A
A
THEOREM 7.7 A n  n
Eigenvalues of
Symmetric Matrices
448 Chapter 7 Eigenvalues and Eigenvectors
Prove that a symmetric matrix
is diagonalizable.
SOLUTION The characteristic polynomial of is
As a quadratic in this polynomial has a discriminant of
Because this discriminant is the sum of two squares, it must be either zero or positive.
If then and which implies that is already diagonal.
That is,
On the other hand, if then by the Quadratic Formula the characteristic polynomial of has two distinct real roots, which implies that has two distinct real
eigenvalues. So, is diagonalizable in this case also.
Find the eigenvalues of the symmetric matrix
and determine the dimensions of the corresponding eigenspaces.
SOLUTION The characteristic polynomial of is represented by
	I  A

	  1
2
0
0
2
	  1
0
0
0
0
	  1
2
0
0
2
	  1  	  12	  32.
A
A

1
2
0
0
2
1
0
0
0
0
1
2
0
0
2
1

EXAMPLE 3 Dimensions of the Eigenspaces of a Symmetric Matrix
A
A A
a  b2  4c2 > 0,
A  
a
0
0
a.
a  b a  b c  0, A 2  4c2  0,
 a  b2  4c2
.
 a2  2ab  b2  4c2
a  b2  4ab  c2  a2  2ab  b2  4ab  4c2
	,
	I  A  
	  a
c
c
	  b  	2  a  b	  ab  c2
.
A
A  
a
c
c
b
EXAMPLE 2 The Eigenvalues and Eigenvectors of a 2 2 Symmetric Matrix
Section 7.3 Symmetric Matrices and Orthogonal Diagonalization 449
So, the eigenvalues of are and Because each of these eigenvalues has
a multiplicity of 2, you know from Theorem 7.7 that the corresponding eigenspaces also
have dimension 2. Specifically, the eigenspace of has a basis of
and the eigenspace of has a basis of
Orthogonal Matrices
To diagonalize a square matrix you need to find an invertible matrix such that
is diagonal. For symmetric matrices, you will see that the matrix can be chosen to have
the special property that This unusual matrix property is defined as follows.
(a) The matrix
is orthogonal because
(b) The matrix
is orthogonal because
In parts (a) and (b) of Example 4, the columns of the matrices form orthonormal sets in
and respectively. This suggests the next theorem. R3 R , 2
P
P1  PT  
3
5
0
4
5
0
1
0
4
5
0
3
5

.
P  
3
5
0
4
5
0
1
0
4
5
0
3
5

P1  PT  
0
1
1
0.
P   0
1
1
0
EXAMPLE 4 Orthogonal Matrices
P1  PT.
P
P1 A, P AP
	 B2  1, 1, 0, 0, 0, 0, 1, 1. 0, 0, 1, 1 2  3
B 1, 1, 0, 0, 	1  1 1
	 	2  3. A 1  1
A square matrix is called orthogonal if it is invertible and if
P1  PT.
Definition of an P
Orthogonal Matrix
450 Chapter 7 Eigenvalues and Eigenvectors
PROOF Suppose the column vectors of form an orthonormal set:
Then the product has the form
Because the set is orthonormal, you have
and
So, the matrix composed of dot products has the form
This implies that and you can conclude that is orthogonal.
Conversely, if is orthogonal, you can reverse the steps above to verify that the column
vectors of form an orthonormal set. P
P
P P T  P1
,
PTP

1
0
.
.
.
0
0
1
.
.
.
0
...
...
...
0
0
.
.
.
1
  In.
pi  pi  pi p i  j 2  1. i  pj  0,
p1, p2, . . . , pn
PTP

p1  p1
p2  p1 .
.
. pn  p1
p1  p2
p2  p2 .
.
. pn  p2
...
...
...
p1  pn
p2  pn .
.
. pn  pn

.

p11
p21
.
.
.
pn1
p12
p22
.
.
.
pn2
...
...
...
p1n
p2n
.
.
.
pnn
 PTP

p11
p12
.
.
.
p1n
p21
p22
.
.
.
p2n
...
...
...
pn1
pn2
.
.
.
pnn

PTP


p11
p21
.
.
. pn1
p12
p22
.
.
. pn2
...
...
...
p1n
p2n
.
.
. pnn
.
P  p1 p2  ...  pn
P
An matrix is orthogonal if and only if its column vectors form an orthonormal set. n  n P
THEOREM 7.8
Property of
Orthogonal Matrices
Section 7.3 Symmetric Matrices and Orthogonal Diagonalization 451
Show that
is orthogonal by showing that Then show that the column vectors of form an
orthonormal set.
SOLUTION Because
it follows that and you can conclude that is orthogonal. Moreover, letting
produces
and
So, is an orthonormal set, as guaranteed by Theorem 7.8.
It can be shown that for a symmetric matrix, the eigenvectors corresponding to distinct
eigenvalues are orthogonal. This property is stated in the next theorem.
p1, p2, p3

 p1   p2   p3  1.
p1  p2  p1  p3  p2  p3  0
p1
 1
3
 2
5
 2
35
 , p2
 2
3
1
5
 4
35
 , and p3
 2
3
0
 5
35

P P T  P1
,
 
1
0
0
0
1
0
0
0
1
  I3,

1
3
2
3
2
3
 2
5
1
5
0
 2
35
 4
35
5
35
 PPT
 1
3
 2
5
 2
35
2
3
1
5
 4
35
2
3
0
5
35

PP P T  I.
P
 1
3
 2
5
 2
35
2
3
1
5
 4
35
2
3
0
5
35

EXAMPLE 5 An Orthogonal Matrix
452 Chapter 7 Eigenvalues and Eigenvectors
PROOF Let and be distinct eigenvalues of with corresponding eigenvectors and So,
and
To prove the theorem, use the matrix form of the dot product shown below.
Now you can write
Because is symmetric,
This implies that and because it follows that
So, and are orthogonal.
Show that any two eigenvectors of
corresponding to distinct eigenvalues are orthogonal.
SOLUTION The characteristic polynomial of is
	I  A  
	  3
1
1
	  3  	  2	  4,
A
A  
3
1
1
3
EXAMPLE 6 Eigenvectors of a Symmetric Matrix
x1 x2
x1  x 	 2  0. 1 	 2 	1  	2x1  x2  0,
 	2x1  x2.
 x1  	2x2
 x1
T	2x2
 x1
TAx2
A  AT  x A . 1
TAx2
 x1
TAT x2
 Ax1Tx2
 Ax1  x2
	1x1  x2  	1x1  x2
x1  x2  x11 x12 ... x1n

x21
x22
.
.
.
x2n
  x1
Tx2
Ax2  	2x2 Ax . 1  	1x1
x2 x . 	1 	2 A 1
Let be an symmetric matrix. If and are distinct eigenvalues of then their
corresponding eigenvectors and are orthogonal. x x1 2
	 A, 	1 2 A n  n THEOREM 7.9
Property of
Symmetric Matrices
Section 7.3 Symmetric Matrices and Orthogonal Diagonalization 453
which implies that the eigenvalues of are and Every eigenvector corresponding to is of the form
and every eigenvector corresponding to is of the form
So,
and you can conclude that and are orthogonal.
Orthogonal Diagonalization
A matrix is orthogonally diagonalizable if there exists an orthogonal matrix such
that is diagonal. The following important theorem states that the set of
orthogonally diagonalizable matrices is precisely the set of symmetric matrices.
PROOF The proof of the theorem in one direction is fairly straightforward. That is, if you assume
is orthogonally diagonalizable, then there exists an orthogonal matrix such that
is diagonal. Moreover, because you have
which implies that So, is symmetric.
The proof of the theorem in the other direction is more involved, but it is
important because it is constructive. Assume is symmetric. If has an eigenvalue of
multiplicity then by Theorem 7.7, has linearly independent eigenvectors. Through the
Gram-Schmidt orthonormalization process, this set of vectors can be used to form an
orthonormal basis of eigenvectors for the eigenspace corresponding to This procedure is
repeated for each eigenvalue of The collection of all resulting eigenvectors is orthogonal
by Theorem 7.9, and you know from the normalization process that the collection is also
orthonormal. Now let be the matrix whose columns consist of these orthonormal eigenvectors. By Theorem 7.8, is an orthogonal matrix. Finally, by Theorem 7.5, you can
conclude that is diagonal. So, is orthogonally diagonalizable. P A 1
AP
P
P n
A.
	.
k
k, 	 k
A A 	
A A T  PDPTT  PTTDTPT  PDPT  A.
A  PDP1  PDPT P , 1  PT D  P , 1
AP
P
A
P1
AP  D
A P
x x1 2
x1  x 2   s
s
  
t
t
  st  st  0,
x2  
t
t
, t  0.
	2  4
x1   s
s
, s  0
	1  2
	 	2  4. A 1  2
Let be an matrix. Then is orthogonally diagonalizable and has real eigenvalues
if and only if is symmetric. A
A n  n A THEOREM 7.10
Fundamental Theorem
of Symmetric Matrices
454 Chapter 7 Eigenvalues and Eigenvectors
Which matrices are orthogonally diagonalizable?
SOLUTION By Theorem 7.10, the orthogonally diagonalizable matrices are the symmetric ones:
and
It was mentioned that the second part of the proof of Theorem 7.10 is constructive. That
is, it gives you steps to follow to diagonalize a symmetric matrix orthogonally. These steps
are summarized as follows.
Find an orthogonal matrix that orthogonally diagonalizes
SOLUTION 1. The characteristic polynomial of is
So the eigenvalues are and
2. For each eigenvalue, find an eigenvector by converting the matrix to reduced
row-echelon form.
	I  A
	 	2  2. 1  3
	I  A  
	  2
2
2
	  1  	  3	  2.
A
A  
2
2
2
1.
P
EXAMPLE 8 Orthogonal Diagonalization
A4.
A1
A4  
0
0
0
2
A3    3
2
2
0
0
1
A2   5
2
1
2
1
8
1
8
0
 A1  
1
1
1
1
0
1
1
1
1

EXAMPLE 7 Determining Whether a Matrix Is Orthogonally Diagonalizable
Let be an symmetric matrix.
1. Find all eigenvalues of and determine the multiplicity of each.
2. For each eigenvalue of multiplicity 1, choose a unit eigenvector. (Choose any
eigenvector and then normalize it.)
3. For each eigenvalue of multiplicity find a set of linearly independent
eigenvectors. (You know from Theorem 7.7 that this is possible.) If this set is not
orthonormal, apply the Gram-Schmidt orthonormalization process.
4. The composite of steps 2 and 3 produces an orthonormal set of eigenvectors. Use
these eigenvectors to form the columns of The matrix will be
diagonal. (The main diagonal entries of are the eigenvalues of ) D A.
P1
AP  PT P. AP  D
n
k 
 2, k
A
Orthogonal Diagonalization A n  n
of a Symmetric Matrix
Section 7.3 Symmetric Matrices and Orthogonal Diagonalization 455
Eigenvector
The eigenvectors and form an orthogonal basis for Each of these
eigenvectors is normalized to produce an orthonormal basis.
3. Because each eigenvalue has a multiplicity of 1, go directly to step 4.
4. Using and as column vectors, construct the matrix
Verify that is correct by computing
Find an orthogonal matrix that diagonalizes
SOLUTION 1. The characteristic polynomial of yields the eigenvalues
and has a multiplicity of 1 and has a multiplicity of 2.
2. An eigenvector for is which normalizes to
u1  v1
v1
 	
1
3
, 2
3
, 2
3
.
v 	 1  1, 2, 2, 1
	2  	2 	1   3. 	1 6
	I  A  	  32 A, 	  6,
A   2
2
2
2
1
4
2
4
1

.
P
EXAMPLE 9 Orthogonal Diagonalization

 2
5
1
5
1
5
2
5
  
3
0
0
2 P  TAP

 2
5
1
5
1
5
2
5
 
2
2
2
1
P1
AP  PT P AP.
P

 2
5
1
5
1
5
2
5

p P. p1 2
p2  1, 2
1, 2
 1
5
1, 2  	
1
5
, 2
5

p1  2, 1
2, 1
 1
5
2, 1  	 2
5
, 1
5

R2 2, 1 1, 2 .

1
2   1
0
1
2
0 2I  A   4
2
2
1

2
11
0
2
0   3I  A  
1
2
2
4
456 Chapter 7 Eigenvalues and Eigenvectors
3. Two eigenvectors for are and Note that is
orthogonal to and as guaranteed by Theorem 7.9. The eigenvectors and
however, are not orthogonal to each other. To find two orthonormal eigenvectors for
use the Gram-Schmidt process as follows.
These vectors normalize to
4. The matrix has and as its column vectors.
A check shows that
P1
AP  PTAP  
6
0
0
0
3
0
0
0
3

.
P

1
3
2
3
2
3
2
5
1
5
0
 2
35
4
35
5
35

u1 u3 , u2 P ,
u3  w3
w3
 	 2
35
, 4
35
, 5
35
.
u2  w2
w2
 	
2
5
, 1
5
, 0

w3  v3  	
v3  w2
w2  w2

w2  	2
5
, 4
5
, 1

w2  v2  2, 1, 0
	2,
v3 v , 2 v3 v , 2
v1 v v 3  2, 0, 1. 	 2  2, 1, 0 2
SECTION 7.3 Exercises
In Exercises 1–6, determine whether the matrix is symmetric.
1. 2.
3. 4.
5. 6. 
2
0
3
5
0
11
0
2
3
0
5
0
5
2
0
1   0
1
2
1
1
0
3
2
2
3
0
1
1
2
1
2
  1
5
4
5
3
6
4
6
2   4
3
1
2
1
2
1
2
1

 6
2
2
1   1
3
3
1
Section 7.3 Symmetric Matrices and Orthogonal Diagonalization 457
In Exercises 7–14, find the eigenvalues of the symmetric matrix.
For each eigenvalue, find the dimension of the corresponding
eigenspace.
7. 8.
9. 10.
11. 12.
13. 14.
In Exercises 15–22, determine whether the matrix is orthogonal.
15. 16.
17. 18.
19.
20.
21.
22.
In Exercises 23–32, find an orthogonal matrix such that
diagonalizes Verify that gives the proper diagonal form.
23. 24.
25. 26.
27. 28.
29. 30.
31. 32.
True or False? In Exercises 33 and 34, determine whether each
statement is true or false. If a statement is true, give a reason or cite
an appropriate statement from the text. If a statement is false, provide
an example that shows the statement is not true in all cases or cite an
appropriate statement from the text.
33. (a) Let be an matrix. Then is symmetric if and only
if is orthogonally diagonalizable.
(b) The eigenvectors corresponding to distinct eigenvalues are
orthogonal for symmetric matrices.
A
A n  n A
A

1
1
0
0
1
1
0
0
0
0
1
1
0
0
1
1
 A

4
2
0
0
2
4
0
0
0
0
4
2
0
0
2
4

A  
2
2
4
2
2
4
4
4
4
 A   1
1
2
1
1
2
2
2
2

A  
0
3
0
3
0
4
0
4
0
 A  
0
10
10
10
5
0
10
0
5

A  
0
1
1
1
0
1
1
1
0
 A   2
2
2
1 
A  
4
2
2
4
A    1
1
1
1
PT A. AP
PT P AP
 1
8
0
0
37
8
0
1
0
0
0
0
1
0
37
8
0
0
1
8


 1
10
0
0
 3
10
10
10
0
0
1
0
0
1
0
0
 3
10
0
0
 1
10
10
10 

2
3
0
2
6
0
25
5
5
5
5
2
0
1
2


2
2
0
2
2
6
6
6
3
6
6
3
3
3
3
3
3


4
5
0
3
5
0
1
0
3
5
0
4
5
 
4
0
3
0
1
0
3
0
4


2
3
2
3
2
3
1
3   2
2
2
2
2
2
2
2 
 2
1
1
1
2
1
1
1
2   0
1
1
1
0
1
1
1
1


0
4
4
4
2
0
4
0
2   0
2
2
2
0
2
2
2
0


2
1
1
1
2
1
1
1
2   3
0
0
0
2
0
0
0
2


2
0
0
2   3
1
1
3
458 Chapter 7 Eigenvalues and Eigenvectors
Applications of Eigenvalues and Eigenvectors
Population Growth
Matrices can be used to form models for population growth. The first step in this process is
to group the population into age classes of equal duration. For instance, if the maximum
life span of a member is years, then the age classes are represented by the intervals
shown below.
First age Second age nth age
class class class
The number of population members in each age class is then represented by the age
distribution vector
Over a period of years, the probability that a member of the th age class will survive
to become a member of the age class is given by where
The average number of offspring produced by a member of the th age class is given by
where
0  bi
, i  1, 2, . . . , n.
bi i ,
0  pi  1, i  1, 2, . . . , n  1.
pi i  1th ,
L
n i
x

x1
x2
.
.
.
xn

.

L
n
,
2L
n 
, . . . , 
n  1L
n , L   0, L
n
,
L n
7.4
34. (a) A square matrix is orthogonal if it is invertible—that is, if
(b) If is an symmetric matrix, then is orthogonally
diagonalizable and has real eigenvalues.
35. Prove that if is an matrix, then and are
symmetric.
36. Find and for the matrix below.
37. Prove that if is an orthogonal matrix, then
38. Prove that if and are orthogonal matrices, then and
are orthogonal.
39. Show that the matrix below is orthogonal for any value of
40. Prove that if a symmetric matrix has only one eigenvalue
then
41. Prove that if is an orthogonal matrix, then so are and A1 A .