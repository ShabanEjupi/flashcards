# Matricat Simetrike dhe Diagonalizimi Ortogonal

## Kapitulli 1: PÃ«rkufizime BazÃ« dhe Koncepte tÃ« Matricave

- 1.1 Matrica: pÃ«rkufizimi, tipi, madhÃ«sia
- 1.2 Operacionet me Matrica
- 1.3 Transpozita dhe Inversa
- 1.4 PÃ«rkufizimi i njÃ« matrice simetrike
- 1.5 Karakteristikat e matricave simetrike

## Kapitulli 2: Eigenvlera dhe EigenvektorÃ«

- 2.1 PÃ«rkufizimi dhe kuptimi gjeometrik
- 2.2 Si tÃ« gjejmÃ« eigenvlera: determinanta dhe polinomi karakteristik
- 2.3 Eigenspace dhe dimensioni i tij
- 2.4 Rastet kur eigenvlera pÃ«rsÃ«ritet (multipliciteti algjebrik dhe gjeometrik)

## Kapitulli 3: Teorema Spektrale dhe Matricat Simetrike

- 3.1 Teorema Spektrale pÃ«r Matricat Simetrike
- 3.2 Ortogonaliteti i eigenvektorÃ«ve
- 3.3 Prova e teoremÃ«s spektrale
- 3.4 Shembuj tÃ« plotÃ« me zgjidhje hap pas hapi

## Kapitulli 4: Diagonalizimi Ortogonal

- 4.1 PÃ«rkufizimi i diagonalizimit
- 4.2 Matrica ortogonale dhe vetitÃ« e saj
- 4.3 Procesi i diagonalizimit pÃ«r matrica simetrike
- 4.4 Si tÃ« ndÃ«rtojmÃ« matricÃ«n ortogonale P dhe matricÃ«n diagonale D
- 4.5 Shembuj tÃ« plotÃ«

## Kapitulli 5: Ushtrime dhe Aplikime

- 5.1 Verifikimi i simetrisÃ« dhe ortogonalitetit tÃ« matricave
- 5.2 Gjetja e eigenvlerave dhe eigenspace
- 5.3 Diagonalizimi ortogonal: aplikime numerike dhe teorike
- 5.4 Aplikime nÃ« modelimin e rritjes sÃ« popullsisÃ«
- 5.5 Pyetje True/False me argumentim teorik

## Kapitulli 6: ShtojcÃ« Teorike

- 6.1 PÃ«rmbledhje e rezultateve kryesore
- 6.2 Teorema qÃ« ndihmojnÃ« nÃ« diagonalizim
- 6.3 Veti tÃ« rÃ«ndÃ«sishme tÃ« matricave ortogonale
- 6.4 Lidhja me analizÃ«n spektrale



import matplotlib.pyplot as plt
import numpy as np

# Set style for consistency
plt.style.use('seaborn-vibrant')

# Define a symmetric matrix A
A = np.array([[2, 1],
              [1, 3]])

# Define a set of vectors
v1 = np.array([1, 0])
v2 = np.array([0, 1])
v3 = np.array([1, 1])

# Apply matrix A to each vector
Av1 = A @ v1
Av2 = A @ v2
Av3 = A @ v3

# Plot original and transformed vectors
fig, ax = plt.subplots(figsize=(8, 8))
origin = np.zeros(2)

# Original vectors in blue
ax.quiver(*origin, *v1, angles='xy', scale_units='xy', scale=1, color='blue', label='v1')
ax.quiver(*origin, *v2, angles='xy', scale_units='xy', scale=1, color='blue', label='v2')
ax.quiver(*origin, *v3, angles='xy', scale_units='xy', scale=1, color='blue', label='v3')

# Transformed vectors in red
ax.quiver(*origin, *Av1, angles='xy', scale_units='xy', scale=1, color='red', label='A * v1')
ax.quiver(*origin, *Av2, angles='xy', scale_units='xy', scale=1, color='red', label='A * v2')
ax.quiver(*origin, *Av3, angles='xy', scale_units='xy', scale=1, color='red', label='A * v3')

# Plot settings
ax.set_xlim(-1, 6)
ax.set_ylim(-1, 6)
ax.set_aspect('equal')
ax.grid(True)
ax.set_title("Transformimi i vektorÃ«ve me njÃ« matricÃ« simetrike A")
ax.set_xlabel("Boshti x")
ax.set_ylabel("Boshti y")

# Avoid duplicate labels in legend
handles, labels = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels, handles))
ax.legend(by_label.values(), by_label.keys())

plt.tight_layout()
plt.show()


-------
# Fixing the style to a default one that exists
plt.style.use('seaborn-darkgrid')

# Define a symmetric matrix A
A = np.array([[2, 1],
              [1, 3]])

# Define a set of vectors
v1 = np.array([1, 0])
v2 = np.array([0, 1])
v3 = np.array([1, 1])

# Apply matrix A to each vector
Av1 = A @ v1
Av2 = A @ v2
Av3 = A @ v3

# Plot original and transformed vectors
fig, ax = plt.subplots(figsize=(8, 8))
origin = np.zeros(2)

# Original vectors in blue
ax.quiver(*origin, *v1, angles='xy', scale_units='xy', scale=1, color='blue', label='v1')
ax.quiver(*origin, *v2, angles='xy', scale_units='xy', scale=1, color='blue', label='v2')
ax.quiver(*origin, *v3, angles='xy', scale_units='xy', scale=1, color='blue', label='v3')

# Transformed vectors in red
ax.quiver(*origin, *Av1, angles='xy', scale_units='xy', scale=1, color='red', label='A * v1')
ax.quiver(*origin, *Av2, angles='xy', scale_units='xy', scale=1, color='red', label='A * v2')
ax.quiver(*origin, *Av3, angles='xy', scale_units='xy', scale=1, color='red', label='A * v3')

# Plot settings
ax.set_xlim(-1, 6)
ax.set_ylim(-1, 6)
ax.set_aspect('equal')
ax.grid(True)
ax.set_title("Transformimi i vektorÃ«ve me njÃ« matricÃ« simetrike A")
ax.set_xlabel("Boshti x")
ax.set_ylabel("Boshti y")

# Avoid duplicate labels in legend
handles, labels = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels, handles))
ax.legend(by_label.values(), by_label.keys())

plt.tight_layout()
plt.show()

----
# Kapitulli 1: Matricat Simetrike dhe Transformimet Lineare

## 1.1 Hyrje nÃ« Matricat Simetrike

NjÃ« matricÃ« A âˆˆ â„™^{n x n} quhet simetrike nÃ«se A^T = A. Kjo do tÃ« thotÃ« qÃ« elementÃ«t e saj janÃ« tÃ« vendosur nÃ« mÃ«nyrÃ« tÃ« tillÃ« qÃ« a\_{ij} = a\_{ji}.

**Shembull**:
$A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$

Ky Ã«shtÃ« njÃ« shembull i njÃ« matrice 2x2 simetrike, sepse a\_{12} = a\_{21}.

## 1.2 Vetivlerat dhe VetivektorÃ«t

PÃ«r njÃ« matricÃ« A, njÃ« vektor jo-zero $\mathbf{v}$ quhet vetivektor nÃ«se:
$A\mathbf{v} = \lambda\mathbf{v}$
ku $\lambda$ Ã«shtÃ« vetivlera pÃ«rkatÃ«se.

### Veti tÃ« rÃ«ndÃ«sishme:

* Ã‡do matricÃ« simetrike ka vetÃ«m vetivlera reale.
* VetivektorÃ«t qÃ« korrespondojnÃ« me vetivlera tÃ« ndryshme janÃ« ortogonalÃ«.

## 1.3 Transformimi i VektorÃ«ve

Kur njÃ« vektor shumÃ«zohet me njÃ« matricÃ«, ai transformohet nÃ« njÃ« drejtim tÃ« ri. PÃ«r matricat simetrike, ky transformim Ã«shtÃ« i veÃ§antÃ« sepse ruan ortogonalitetin dhe strukturÃ«n gjeometrike tÃ« vektorÃ«ve.

**Shembull Vizual**:
MÃ« poshtÃ« kemi njÃ« shembull ku matrica simetrike:
$A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$

transformon tre vektorÃ« $\mathbf{v}_1 = [1, 0], \mathbf{v}_2 = [0, 1], \mathbf{v}_3 = [1, 1]$.

Transformimet respektive janÃ«:

* $A\mathbf{v}_1 = [2, 1]$
* $A\mathbf{v}_2 = [1, 3]$
* $A\mathbf{v}_3 = [3, 4]$

Grafiku mÃ« sipÃ«r tregon drejtimet fillestare (blu) dhe drejtimet pas transformimit (tÃ« kuqe).

---

NÃ« kapitullin tjetÃ«r do tÃ« prezantojmÃ« konceptin e diagonalizimit ortogonal dhe aplikimin e tij nÃ« dekompozimin e matricave.


# Kapitulli 2: Diagonalizimi Ortogonal i Matricave Simetrike

## 2.1 Hyrje

NÃ« kÃ«tÃ« kapitull do tÃ« shqyrtojmÃ« procesin e diagonalizimit ortogonal tÃ« matricave simetrike. Matricat simetrike kanÃ« veti tÃ« veÃ§anta qÃ« u mundÃ«sojnÃ« tÃ« diagonalizohen pÃ«rmes matricave ortogonale. Kjo Ã«shtÃ« njÃ« nga arsyet pse matricat simetrike janÃ« shumÃ« tÃ« rÃ«ndÃ«sishme nÃ« matematikÃ«, fizikÃ« dhe inxhinieri.

## 2.2 PÃ«rkufizimi

NjÃ« matricÃ« reale A Ã«shtÃ« simetrike nÃ«se A^T = A.
NjÃ« matricÃ« A Ã«shtÃ« ortogonalisht diagonalizueshme nÃ«se ekziston njÃ« matricÃ« ortogonale P dhe njÃ« matricÃ« diagonale D e tillÃ« qÃ«:

A = P D P^T

## 2.3 Teorema Kryesore

Ã‡do matricÃ« reale simetrike Ã«shtÃ« ortogonalisht diagonalizueshme. Pra, nÃ«se A Ã«shtÃ« matricÃ« simetrike, atÃ«herÃ« ekziston njÃ« matricÃ« ortogonale P dhe njÃ« matricÃ« diagonale D tÃ« tilla qÃ« A = P D P^T.

## 2.4 Procesi i Diagonalizimit Ortogonal

1. Gjej vlerat e veta (eigenvalues) tÃ« matricÃ«s simetrike A.
2. PÃ«r secilÃ«n vlerÃ« tÃ« vet Î», gjej hapÃ«sirÃ«n e vet (eigenspace) dhe njÃ« bazÃ« ortonormale tÃ« saj.
3. NdÃ«rto matricÃ«n P me kolonat e saj tÃ« jenÃ« vektorÃ«t ortonormalÃ« tÃ« vet.
4. NdÃ«rto matricÃ«n D me vlerat pÃ«rkatÃ«se tÃ« vet nÃ« diagonale.
5. VÃ«rteto qÃ« A = P D P^T.

## 2.5 Shembull i Punuar

Jepet matrica A:

A = | 4 1 |
\| 1 4 |

1. Gjej vlerat e veta:
   Det(A - Î»I) = (4-Î»)^2 - 1 = Î»^2 - 8Î» + 15 = 0
   Î»â‚ = 3, Î»â‚‚ = 5

2. PÃ«r Î» = 3: (A - 3I)x = 0 â†’ sistemi linear â†’ bazÃ«: \[1, -1]
   PÃ«r Î» = 5: (A - 5I)x = 0 â†’ bazÃ«: \[1, 1]

3. Ortonormalizimi:
   vâ‚ = 1/âˆš2 \[1, -1], vâ‚‚ = 1/âˆš2 \[1, 1]

4. Matrica P = \[vâ‚ vâ‚‚], D = diag(3, 5)

5. A = P D P^T vÃ«rtetohet me shumÃ«zim matricor.

## 2.6 Interpretim Grafik (pÃ«r t'u shtuar mÃ« vonÃ« me grafikÃ«)

Transformimi i matricÃ«s A shpjegohet si njÃ« kombinim i rrotullimit dhe shtrirjes pÃ«rgjatÃ« drejtimeve tÃ« vektorÃ«ve tÃ« vet. Kjo ndihmon nÃ« kuptimin vizual tÃ« efektit tÃ« A mbi hapÃ«sirÃ«n vektoriale â„Â².

---

Vijon nÃ« Kapitullin 3: Matricat Ortogonale.



# Kapitulli 3: Matricat Ortogonale

## 3.1 PÃ«rkufizimi i Matricave Ortogonale

NjÃ« matricÃ« katrore $P$ quhet matricÃ« ortogonale nÃ«se rreshtat (ose kolonat) e saj janÃ« vektorÃ« njÃ«sie dhe ortogonalÃ« ndaj njÃ«ri-tjetrit. Formalisht, njÃ« matricÃ« $P \in \mathbb{R}^{n \times n}$ Ã«shtÃ« ortogonale nÃ«se:

$$
P^T P = I \quad \text{ose ekuivalent:} \quad P^{-1} = P^T
$$

Kjo do tÃ« thotÃ« qÃ« kolona (dhe gjithashtu rreshtat) e $P$ pÃ«rbÃ«jnÃ« njÃ« bazÃ« ortonormale tÃ« $\mathbb{R}^n$.

## 3.2 Veti tÃ« Matricave Ortogonale

* RuajnÃ« gjatÃ«sinÃ« e vektorÃ«ve: $\| P\vec{x} \| = \| \vec{x} \|$
* RuajnÃ« kÃ«ndet ndÃ«rmjet vektorÃ«ve.
* JanÃ« tÃ« kthyeshme dhe inversi i tyre Ã«shtÃ« baras me transpozitÃ«n: $P^{-1} = P^T$
* Produkti i matricave ortogonale Ã«shtÃ« gjithashtu matricÃ« ortogonale.

## 3.3 Shembull i MatricÃ«s Ortogonale

Le tÃ« marrim matricÃ«n:

$$
P = \begin{bmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}
$$

VÃ«rtetojmÃ« qÃ« Ã«shtÃ« ortogonale:

$$
P^T = \begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}
$$

$$
P^T P = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I
$$

Pra, $P$ Ã«shtÃ« matricÃ« ortogonale.

## 3.4 RÃ«ndÃ«sia nÃ« Diagonalizim

NÃ« diagonalizimin ortogonal, kÃ«rkojmÃ« njÃ« matricÃ« ortogonale $P$ e cila e diagonalizon njÃ« matricÃ« simetrike $A$:

$$
A = P D P^T
$$

Kjo ndarje Ã«shtÃ« e dobishme nÃ« shumÃ« fusha tÃ« matematikÃ«s dhe inxhinierisÃ« si pÃ«r shembull nÃ« analizÃ«n e spektrit, dekompozime tÃ« sinjaleve dhe mÃ« shumÃ«.

---

A je gati tÃ« vazhdojmÃ« me Kapitullin 4 pÃ«r "Diagonalizimin Ortogonal tÃ« Matricave 2Ã—2 dhe 3Ã—3" me shembuj konkretÃ«?



---# Kapitulli 4: Diagonalizimi Ortogonal i Matricave 2Ã—2 dhe 3Ã—3

## 4.1 PÃ«rkufizimi i Diagonalizimit Ortogonal

NjÃ« matricÃ« A âˆˆ â„â¿Ë£â¿ quhet diagonalizueshme nÃ« mÃ«nyrÃ« ortogonale nÃ«se ekziston njÃ« matricÃ« ortogonale P dhe njÃ« matricÃ« diagonale D e tillÃ« qÃ«:

A = P D Páµ€

Kjo do tÃ« thotÃ« qÃ« mund ta shkruajmÃ« matricÃ«n A si produkt i njÃ« matrice ortogonale, njÃ« matrice diagonale dhe transpozitÃ«s sÃ« saj. Kjo Ã«shtÃ« veÃ§anÃ«risht e mundur kur A Ã«shtÃ« matricÃ« simetrike.

## 4.2 Procedura pÃ«r Diagonalizim Ortogonal

PÃ«r njÃ« matricÃ« simetrike A âˆˆ â„â¿Ë£â¿:

1. Gjej vlerat karakteristike (eigenvalues) tÃ« A.
2. Gjej vektorÃ«t karakteristikÃ« pÃ«r secilÃ«n vlerÃ« karakteristike.
3. Ortonormalizo vektorÃ«t karakteristikÃ« nÃ«pÃ«rmjet Gram-Schmidt nÃ«se nuk janÃ« ortonormalÃ«.
4. Formo matricÃ«n ortogonale P nga kÃ«ta vektorÃ« si kolona.
5. D = Páµ€ A P do tÃ« jetÃ« matricÃ« diagonale.

## 4.3 Shembull pÃ«r MatricÃ« 2Ã—2

Le tÃ« shohim matricÃ«n:

A = \[ 4  2
2  3 ]

1. Gjej vlerat karakteristike:

Det(A - Î»I) = |4-Î»   2|
|2    3-Î»| = (4-Î»)(3-Î») - 4 = Î»Â² - 7Î» + 8 â†’ Î»â‚ = 4.561, Î»â‚‚ = 2.439

2. Gjej vektorÃ«t karakteristikÃ« pÃ«r secilÃ«n vlerÃ« karakteristike.

PÃ«r Î»â‚ = 4.561: zgjidh (A - Î»I)x = 0 â†’ vâ‚ â‰ˆ \[0.788, 0.615]
PÃ«r Î»â‚‚ = 2.439: zgjidh (A - Î»I)x = 0 â†’ vâ‚‚ â‰ˆ \[-0.615, 0.788]

3. VektorÃ«t janÃ« tÃ« ortonormalizuar â†’ formojmÃ« matricÃ«n P:

P = \[ 0.788  -0.615
0.615   0.788 ] (matricÃ« ortogonale)

4. D = Páµ€ A P = \[ 4.561  0
   0      2.439 ]

Pra A Ã«shtÃ« diagonalizuar nÃ« mÃ«nyrÃ« ortogonale.

## 4.4 Shembull pÃ«r MatricÃ« 3Ã—3

Marrim matricÃ«n simetrike:

A = \[ 6  2  0
2  3  0
0  0  1 ]

1. Vlerat karakteristike janÃ«:
   Î»â‚ = 7, Î»â‚‚ = 2, Î»â‚ƒ = 1

2. VektorÃ«t karakteristikÃ« pÃ«rkatÃ«s:
   vâ‚ = \[2,1,0], vâ‚‚ = \[-1,2,0], vâ‚ƒ = \[0,0,1]

3. OrtonormalizojmÃ«:

* Normalizo vâ‚: uâ‚ = vâ‚ / ||vâ‚|| = \[0.894, 0.447, 0]
* Normalizo vâ‚‚: uâ‚‚ = \[-0.447, 0.894, 0]
* vâ‚ƒ Ã«shtÃ« veÃ§se ortonormal: uâ‚ƒ = \[0, 0, 1]

4. FormojmÃ« matricÃ«n P:

P = \[ 0.894  -0.447   0
0.447   0.894   0
0        0      1 ]

5. Diagonalizimi:
   D = Páµ€ A P = diag(7, 2, 1)

## 4.5 Interpretim Grafik

PÃ«r matricat 2Ã—2, diagonalizimi ortogonal mund tÃ« vizualizohet si njÃ« rotacion qÃ« transformon bazÃ«n nÃ« njÃ« sistem ku matrica sillet si shtrirje (scaling) pÃ«rgjatÃ« akseve tÃ« reja.

Do tÃ« shtojmÃ« grafikÃ« nÃ« kapitullin 5 pÃ«r ilustrime mÃ« tÃ« qarta.

---

Gati tÃ« vazhdojmÃ« me Kapitullin 5 ku do tÃ« pÃ«rdorim grafikÃ« pÃ«r tÃ« ilustruar diagonalizimin ortogonal nÃ« mÃ«nyrÃ« vizuale.

-----
# Kapitulli 4: Diagonalizimi Ortogonal i Matricave Simetrike

## 4.1 Diagonalizimi Ortogonal i Matricave 2Ã—2

NjÃ« matricÃ« A Ã«shtÃ« e diagonalizueshme ortogonalisht nÃ«se ekziston njÃ« matricÃ« ortogonale P dhe njÃ« matricÃ« diagonale D e tillÃ« qÃ«:

A = PDPáµ€

PÃ«r matricat simetrike 2Ã—2, procesi pÃ«rfshin:

1. Gjetjen e vlerave vetjake (eigenvalues) Î» tÃ« A duke zgjidhur ekuacionin det(A - Î»I) = 0.
2. PÃ«r secilÃ«n vlerÃ« vetjake Î», gjej vektorÃ«t vetjakÃ« pÃ«rkatÃ«s duke zgjidhur (A - Î»I)x = 0.
3. Ortogonalizo dhe normalizo vektorÃ«t vetjakÃ« pÃ«r tÃ« ndÃ«rtuar matricÃ«n ortogonale P.
4. Formo matricÃ«n diagonale D duke vendosur vlerat vetjake nÃ« diagonale.

### Shembull

Jepet matrica simetrike:

A =
\| 4  1 |
\| 1  3 |

1. GjejmÃ« vlerat vetjake:

det(A - Î»I) = det
\| 4âˆ’Î»  1   |
\| 1    3âˆ’Î» | = (4âˆ’Î»)(3âˆ’Î») âˆ’ 1 = Î»Â² âˆ’ 7Î» + 11 = 0

Zgjidhim pÃ«r Î»:

Î»â‚ = (7 + âˆš5)/2, Î»â‚‚ = (7 âˆ’ âˆš5)/2

2. GjejmÃ« vektorÃ«t vetjakÃ« pÃ«r secilÃ«n Î».

3. NormalizojmÃ« ata pÃ«r tÃ« formuar P.

4. FormojmÃ« D = diag(Î»â‚, Î»â‚‚).

## 4.2 Diagonalizimi Ortogonal i Matricave 3Ã—3

PÃ«r matricat 3Ã—3 simetrike, procedura Ã«shtÃ« e njÃ«jtÃ« por mÃ« e ndÃ«rlikuar pÃ«r shkak tÃ« llogaritjeve:

1. Gjej vlerat vetjake duke zgjidhur det(A âˆ’ Î»I) = 0, njÃ« polinom kubik.
2. PÃ«r secilÃ«n vlerÃ« vetjake, gjej bazÃ«n e hapÃ«sirÃ«s vetjake pÃ«rkatÃ«se (eigenspace).
3. PÃ«rdor procesin Gram-Schmidt pÃ«r tÃ« ortogonalizuar dhe normalizuar vektorÃ«t nÃ«se Ã«shtÃ« e nevojshme.
4. Konstruo matricÃ«n ortogonale P dhe matricÃ«n diagonale D.

### Shembull pÃ«r 3Ã—3

A =
\| 6  2  0 |
\| 2  3  1 |
\| 0  1  1 |

1. Llogarit det(A âˆ’ Î»I) = 0 â†’ Gjej Î»â‚, Î»â‚‚, Î»â‚ƒ
2. Gjej vektorÃ«t vetjakÃ« pÃ«r secilÃ«n Î».
3. Normalizo dhe ortogonalizo pÃ«r tÃ« marrÃ« P.
4. PÃ«rdor A = PDPáµ€ pÃ«r tÃ« verifikuar diagonalizimin.

NÃ« kapitullin tjetÃ«r do tÃ« pÃ«rfshijmÃ« grafikÃ« pÃ«r tÃ« ndihmuar nÃ« vizualizimin e vektorÃ«ve vetjakÃ« dhe mÃ«nyrÃ«n sesi ata transformojnÃ« hapÃ«sirÃ«n.
----

ShkÃ«lqyeshÃ«m. Po vazhdojmÃ« me:

## ğŸ“˜ Kapitulli 5: Interpretimi Gjeometrik i VektorÃ«ve VetjakÃ« dhe Vlerave Vetjake

NÃ« kÃ«tÃ« kapitull do tÃ« pÃ«rqendrohemi nÃ« kuptimin vizual dhe gjeometrik tÃ« vektorÃ«ve vetjakÃ« dhe vlerave vetjake tÃ« matricave, duke pÃ«rdorur grafikÃ« pÃ«r tÃ« ilustruar konceptet.

---

### 5.1 Interpretimi Gjeometrik i VektorÃ«ve VetjakÃ«

NjÃ« vektor vetjak i njÃ« matrice A Ã«shtÃ« njÃ« drejtim nÃ« hapÃ«sirÃ« i cili, kur shumÃ«zohet me matricÃ«n A, nuk ndryshon drejtimin â€“ ndryshon vetÃ«m gjatÃ«sia (shkallÃ«zohet).

ğŸ“Œ Shembull intuitiv:

* Mendo njÃ« vektor nÃ« njÃ« plan qÃ« kur shumÃ«zohet me njÃ« transformim (matricÃ«), nuk kthehet nÃ« ndonjÃ« drejtim tjetÃ«r â€“ thjesht shtrihet ose tkurret.

ğŸ“ NÃ« mÃ«nyrÃ« grafike:

* VektorÃ«t vetjakÃ« shfaqen si shigjeta nÃ« hapÃ«sirÃ« qÃ« ndjekin tÃ« njÃ«jtin drejtim si mÃ« parÃ«.
* Vlerat vetjake shprehin shkallÃ«zimin e kÃ«tyre vektorÃ«ve.

---

### 5.2 Shembull me Grafik nÃ« â„Â²

Marrim matricÃ«n:

$$
A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}
$$

Kjo matricÃ« ka vlerat vetjake Î»â‚ = 3 dhe Î»â‚‚ = 2, me vektorÃ« vetjakÃ« pÃ«rkatÃ«s:

* pÃ«r Î»â‚ = 3: vektor vetjak vâ‚ = \[1, 0]áµ€
* pÃ«r Î»â‚‚ = 2: vektor vetjak vâ‚‚ = \[1, 1]áµ€

ğŸŸ¦ NÃ« grafik:

* do tÃ« vizatojmÃ« planin 2D
* do tÃ« tregojmÃ« vektorÃ«t vâ‚ dhe vâ‚‚ pÃ«rpara dhe pas shumÃ«zimit me matricÃ«n A
* do tÃ« theksojmÃ« qÃ« drejtimi i tyre mbetet i njÃ«jtÃ«, por gjatÃ«sia ndryshon

ğŸ’¡ Ky Ã«shtÃ« thelbi i vektorÃ«ve vetjakÃ«: drejtimi qÃ« nuk ndryshon nga transformimi linear.

---

ğŸ“Š Do tÃ« shtojmÃ« edhe njÃ« grafik ku njÃ« grup vektorÃ«sh tÃ« rastÃ«sishÃ«m transformohen nga A dhe vetÃ«m vektorÃ«t vetjakÃ« ruajnÃ« drejtimin.

---

### 5.3 Aplikime Gjeometrike

* NÃ« fizikÃ«: vektorÃ«t vetjakÃ« pÃ«rfaqÃ«sojnÃ« drejtimet ku njÃ« sistem qÃ«ndron nÃ« ekuilibÃ«r.
* NÃ« grafikÃ« kompjuterike: pÃ«rdoren pÃ«r tÃ« zbuluar rrotullime, shtrirje dhe reflektime.
* NÃ« sistemet dinamike: tregojnÃ« drejtimet e stabilitetit apo rritjes sÃ« sistemit me kalimin e kohÃ«s.

---

NÃ« vazhdim:

* Do tÃ« vizatojmÃ« grafikÃ«t pÃ«r â„Â² me shembullin mÃ« sipÃ«r.
* Pastaj do tÃ« kalojmÃ« nÃ« â„Â³ dhe do tÃ« shqyrtojmÃ« njÃ« shembull 3D pÃ«r interpretim hapÃ«sinor tÃ« vektorÃ«ve vetjakÃ«.

A tÃ« filloj me gjenerimin e grafikÃ«ve pÃ«r â„Â² dhe â„Â³, apo ke ndonjÃ« kÃ«rkesÃ« tÃ« veÃ§antÃ« para se tÃ« vazhdoj?



---------


import matplotlib.pyplot as plt
import numpy as np

# Define the matrix A
A = np.array([[3, 1],
              [0, 2]])

# Define the eigenvectors (before transformation)
v1 = np.array([1, 0])  # Eigenvector corresponding to Î»=3
v2 = np.array([1, 1])  # Eigenvector corresponding to Î»=2

# Apply the matrix A to the eigenvectors
Av1 = A @ v1
Av2 = A @ v2

# Create plot
fig, ax = plt.subplots(figsize=(8, 8))

# Original vectors (blue)
ax.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', label='vâ‚')
ax.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='blue', label='vâ‚‚')

# Transformed vectors (red)
ax.quiver(0, 0, Av1[0], Av1[1], angles='xy', scale_units='xy', scale=1, color='red', label='AÂ·vâ‚')
ax.quiver(0, 0, Av2[0], Av2[1], angles='xy', scale_units='xy', scale=1, color='red', label='AÂ·vâ‚‚')

# Plot settings
ax.set_xlim(-1, 5)
ax.set_ylim(-1, 5)
ax.set_aspect('equal')
ax.grid(True)
ax.axhline(0, color='black', linewidth=0.5)
ax.axvline(0, color='black', linewidth=0.5)
ax.set_title('VektorÃ« VetjakÃ« dhe Transformimi i Tyre nga A')
ax.legend()
plt.show()
----

Ky grafik paraqet dy vektorÃ« vetjakÃ« (vâ‚ dhe vâ‚‚) pÃ«r matricÃ«n A dhe transformimin e tyre pas shumÃ«zimit me matricÃ«n A:

* VektorÃ«t blu (vâ‚ = \[1, 0] dhe vâ‚‚ = \[1, 1]) janÃ« vektorÃ«t vetjakÃ« pÃ«rkatÃ«s pÃ«r vlerat vetjake Î»â‚ = 3 dhe Î»â‚‚ = 2.
* VektorÃ«t e kuq tregojnÃ« rezultatet e shumÃ«zimit AÂ·vâ‚ dhe AÂ·vâ‚‚. SiÃ§ vÃ«rehet:

  * AÂ·vâ‚ = 3Â·vâ‚ â†’ drejtimi nuk ndryshon, por zgjatet 3 herÃ«.
  * AÂ·vâ‚‚ = 2Â·vâ‚‚ â†’ drejtimi ruhet dhe vektori zgjatet 2 herÃ«.

Ky ilustrim ndihmon pÃ«r tÃ« kuptuar vizualisht Ã§farÃ« do tÃ« thotÃ« tÃ« jesh vektor vetjak: vektori ndryshon vetÃ«m pÃ«r nga gjatÃ«sia, por jo pÃ«r nga drejtimi, kur shumÃ«zohet me matricÃ«n. NÃ« kapitullin tjetÃ«r, ne do tÃ« vazhdojmÃ« me mÃ«nyrÃ«n se si tÃ« gjejmÃ« kÃ«to vektorÃ« dhe vlera pÃ«r Ã§do matricÃ« tÃ« dhÃ«nÃ«.


ShumÃ« mirÃ«! Do tÃ« vazhdojmÃ« me kapitullin tjetÃ«r sipas strukturÃ«s sÃ« ndÃ«rtuar mÃ« herÃ«t.

ğŸŸ¦ Kapitulli 2: Gjetja e Vlerave dhe VektorÃ«ve VetjakÃ« tÃ« njÃ« Matrice

Ky kapitull ka pÃ«r qÃ«llim tÃ« shpjegojÃ« nÃ« mÃ«nyrÃ« tÃ« plotÃ« dhe metodike se si tÃ« gjejmÃ« vlerat vetjake (eigenvalues) dhe vektorÃ«t vetjakÃ« (eigenvectors) pÃ«r njÃ« matricÃ« tÃ« dhÃ«nÃ« katrore A âˆˆ â„â¿Ë£â¿. Ky proces pÃ«rfshin:

* Zgjidhjen e ekuacionit karakteristik pÃ«r tÃ« gjetur vlerat vetjake Î».
* PÃ«rdorimin e secilÃ«s Î» pÃ«r tÃ« gjetur vektorÃ«t pÃ«rkatÃ«s tÃ« vetjakÃ«.
* Interpretimin e rezultateve.

---

## 2.1 Teoria Themeltare

PÃ«r njÃ« matricÃ« katrore A âˆˆ â„â¿Ë£â¿, njÃ« numÃ«r Î» âˆˆ â„ quhet vlerÃ« vetjake e A nÃ«se ekziston njÃ« vektor jo-zero x âˆˆ â„â¿ i tillÃ« qÃ«:

A Â· x = Î» Â· x

Ky ekuacion mund tÃ« riformulohet si:

(A âˆ’ Î»I) Â· x = 0

Kjo Ã«shtÃ« njÃ« sistem homogjen linear. Ai ka zgjidhje jo-triviale (dmth. x â‰  0) vetÃ«m nÃ«se det(A âˆ’ Î»I) = 0. Ky Ã«shtÃ« ekuacioni karakteristik.

---

## 2.2 Shembull me MatricÃ« 2Ã—2

Le tÃ« konsiderojmÃ« matricÃ«n:

A = | 4â€ƒ1 |
â€ƒâ€ƒ| 2â€ƒ3 |

â® Hapi 1: GjejmÃ« Ekuacionin Karakteristik

Llogarisim det(A âˆ’ Î»I):

\| 4âˆ’Î»â€ƒ1â€ƒ|
\| 2â€ƒâ€ƒ3âˆ’Î» |

det(A âˆ’ Î»I) = (4âˆ’Î»)(3âˆ’Î») âˆ’ (2Ã—1)
â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ= Î»Â² âˆ’ 7Î» + 10

â® Hapi 2: Zgjidhim Ekuacionin Karakteristik

Î»Â² âˆ’ 7Î» + 10 = 0
(Î» âˆ’ 5)(Î» âˆ’ 2) = 0
â‡’ Î»â‚ = 5â€ƒâ€ƒÎ»â‚‚ = 2

â® Hapi 3: GjejmÃ« VektorÃ«t pÃ«rkatÃ«s tÃ« vetjakÃ«

PÃ«r Î»â‚ = 5:

A âˆ’ 5I = | âˆ’1â€ƒ1 |
â€ƒâ€ƒâ€ƒâ€ƒâ€ƒ|  2 âˆ’2 |
Zgjidhim sistemin:

âˆ’x + y = 0 â‡’ y = x
â‡’ vâ‚ = kâ‚ Â· \[1, 1]áµ—

PÃ«r Î»â‚‚ = 2:

A âˆ’ 2I = | 2â€ƒ1 |
â€ƒâ€ƒâ€ƒâ€ƒâ€ƒ| 2â€ƒ1 |
â‡’ 2x + y = 0 â‡’ y = âˆ’2x
â‡’ vâ‚‚ = kâ‚‚ Â· \[1, âˆ’2]áµ—

---

## 2.3 VÃ«zhgim Grafik

â® NÃ«se dÃ«shiron, mund tÃ« pÃ«rfshij njÃ« grafik si mÃ« parÃ« qÃ« ilustron si A Â· v = Î» Â· v pÃ«r tÃ« dy vektorÃ«t vetjakÃ« tÃ« mÃ«sipÃ«rm.

---

TÃ« vazhdoj tani me Kapitullin 3: Diagonalizimi i matricave tÃ« simetrike dhe ortogonale? Apo dÃ«shiron tÃ« pÃ«rfshijmÃ« njÃ« grafik pÃ«r kÃ«tÃ« shembull mÃ« parÃ«?


ShumÃ« mirÃ« â€” atÃ«herÃ« po e ndjek rrjedhÃ«n logjike dhe tÃ« pÃ«rgatitur. MÃ« poshtÃ« do vazhdoj me kapitullin e radhÃ«s sipas strukturÃ«s qÃ« kemi ndÃ«rtuar:

ğŸŸ¦ Kapitulli 3: Diagonalizimi i Matricave â€“ Rasti i Matricave Simetrike dhe Ortogonale

QÃ«llimi i kÃ«tij kapitulli Ã«shtÃ« tÃ« shpjegojÃ« qartÃ« dhe me hollÃ«si konceptin e diagonalizimit tÃ« matricave, veÃ§anÃ«risht kur ato janÃ« simetrike. Kjo Ã«shtÃ« njÃ« temÃ« qendrore nÃ« analizÃ«n spektrale dhe nÃ« zbatime tÃ« shumta tÃ« matematikÃ«s dhe inteligjencÃ«s artificiale.

---

## 3.1 Diagonalizimi â€“ Koncepti Themelor

NjÃ« matricÃ« A âˆˆ â„â¿Ë£â¿ Ã«shtÃ« e diagonalizueshme nÃ«se ekziston njÃ« matricÃ« P e inverteueshme dhe njÃ« matricÃ« D diagonale e tillÃ« qÃ«:

A = P D Pâ»Â¹

Kjo do tÃ« thotÃ« se mund ta paraqesim A si transformim pÃ«rmes njÃ« baze tÃ« re tÃ« pÃ«rbÃ«rÃ« nga vektorÃ«t vetjakÃ« tÃ« saj.

â® Kur A Ã«shtÃ« matricÃ« reale dhe simetrike (A = Aáµ€), ajo Ã«shtÃ« gjithmonÃ« diagonalizueshme pÃ«rmes njÃ« matrice ortogonale P:

A = P D Páµ€

Ky rezultat njihet si Teorema Spektrale pÃ«r matricat simetrike.

---

## 3.2 Shembull Praktik: Diagonalizim i njÃ« Matrice Simetrike

Le tÃ« marrim matricÃ«n:

A = | 4â€ƒ2 |
â€ƒâ€ƒ| 2â€ƒ3 |

Kjo matricÃ« Ã«shtÃ« reale dhe simetrike (A = Aáµ€).

â® Hapi 1: GjejmÃ« vlerat vetjake tÃ« A

Llogarisim det(A âˆ’ Î»I):

\| 4âˆ’Î»â€ƒ2â€ƒ|
\| 2â€ƒâ€ƒ3âˆ’Î» | = (4âˆ’Î»)(3âˆ’Î») âˆ’ 4 = Î»Â² âˆ’ 7Î» + 8

Zgjidhim: Î»Â² âˆ’ 7Î» + 8 = 0 â‡’ (Î»âˆ’1)(Î»âˆ’8) = 0
âŸ¹ Î»â‚ = 1â€ƒâ€ƒÎ»â‚‚ = 8

â® Hapi 2: GjejmÃ« vektorÃ«t vetjakÃ«

PÃ«r Î»â‚ = 1:

(A âˆ’ I)x = 0 â‡’ | 3â€ƒ2 |
â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ| 2â€ƒ2 | â‡’ zgjidhja: x = kâ‚ Â· \[âˆ’2, 3]áµ—

PÃ«r Î»â‚‚ = 8:

(A âˆ’ 8I)x = 0 â‡’ | âˆ’4â€ƒ2 |
â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ| 2â€ƒâˆ’5 | â‡’ zgjidhja: x = kâ‚‚ Â· \[1, 2]áµ—

â® Hapi 3: Normalizimi i vektorÃ«ve (pÃ«r tÃ« krijuar matricÃ«n ortogonale P)

vâ‚ = \[âˆ’2, 3]áµ—â€ƒâ€ƒâ‡’ vâ‚\_norm = 1/âˆš13 Â· \[âˆ’2, 3]áµ—
vâ‚‚ = \[1, 2]áµ—â€ƒâ€ƒâ‡’ vâ‚‚\_norm = 1/âˆš5 Â· \[1, 2]áµ—

P = \[vâ‚\_normâ€ƒvâ‚‚\_norm] Ã«shtÃ« njÃ« matricÃ« ortogonale.

D = diag(1, 8)

â® Verifikim:

Páµ€ A P = D

---

## 3.3 Interpretim Gjeometrik dhe Zbatim

* Diagonalizimi pÃ«r matricat simetrike pÃ«rfaqÃ«son njÃ« rotacion tÃ« hapÃ«sirÃ«s vektoriale nÃ« njÃ« bazÃ« ku A vepron si shtrirje pÃ«rgjatÃ« akseve.
* NÃ« mÃ«simin e algoritmeve tÃ« reduktimit tÃ« dimensionit si PCA (Principal Component Analysis), pÃ«rdorim diagonalizimin ortogonal tÃ« matricave tÃ« kovariancÃ«s (qÃ« janÃ« simetrike).
* PÃ«r Ã§do funksion ku pÃ«rfshihet zbutja, pÃ«rpunimi ose ndryshimi i shkallÃ«s sÃ« njÃ« strukture vektorÃ«sh, diagonalizimi ofron ndarjen mÃ« tÃ« pastÃ«r pÃ«r analizÃ«.

---

Tani, nÃ«se dÃ«shiron, mund tÃ«:

* Kaloj te Kapitulli 4 (Aplikime tÃ« diagonalizimit nÃ« reduktimin e dimensionit â€“ PCA).
* Vizualizoj me grafik ndikimin e diagonalizimit mbi njÃ« grup vektorÃ«sh.
* NdÃ«rtoj njÃ« shembull nÃ« R ose Python.

A preferon tÃ« vazhdoj me kapitullin tjetÃ«r apo tÃ« pÃ«rfshij vizualizime?


ShkÃ«lqyeshÃ«m! AtÃ«herÃ« po vazhdoj me Kapitullin 4 sipas strukturÃ«s sÃ« planifikuar.

ğŸŸ¦ Kapitulli 4: Reduktimi i Dimensionit dhe Analiza Kryesore e KomponentÃ«ve (PCA)

QÃ«llimi i kÃ«tij kapitulli Ã«shtÃ« tÃ« tregojÃ« se si diagonalizimi i matricave simetrike â€“ si matrica e kovariancÃ«s â€“ luan rol kyÃ§ nÃ« reduktimin e dimensionit pÃ«r tÃ« analizuar tÃ« dhÃ«na me shumÃ« variabla. PCA Ã«shtÃ« njÃ« nga metodat mÃ« tÃ« rÃ«ndÃ«sishme nÃ« statistikÃ«, machine learning dhe analizÃ«n e tÃ« dhÃ«nave.

---

## 4.1 Parimet Themelore tÃ« PCA

Principal Component Analysis (PCA) Ã«shtÃ« njÃ« teknikÃ« qÃ«:

* Zbulon drejtimet kryesore (komponentÃ«t kryesorÃ«) pÃ«rgjatÃ« tÃ« cilave tÃ« dhÃ«nat ndryshojnÃ« mÃ« shumÃ«.
* Redukton dimensionin duke ruajtur sa mÃ« shumÃ« informacion tÃ« jetÃ« e mundur.

Kjo bÃ«het pÃ«rmes:

* Standardizimit tÃ« tÃ« dhÃ«nave (nÃ«se kÃ«rkohet).
* Llogaritjes sÃ« matrices sÃ« kovariancÃ«s.
* Gjetjes sÃ« vlerave dhe vektorÃ«ve vetjakÃ« (eigenvalues, eigenvectors).
* PÃ«rzgjedhjes sÃ« komponentÃ«ve mÃ« tÃ« rÃ«ndÃ«sishÃ«m (me vlera vetjake mÃ« tÃ« mÃ«dha).
* ProjektojmÃ« tÃ« dhÃ«nat nÃ« bazÃ«n e re pÃ«r tÃ« ulur dimensionin.

---

## 4.2 Shembull Konceptual

Supozoni se kemi tÃ« dhÃ«na nÃ« â„Â² me dy variabla tÃ« korreluar:

X = { (2, 1), (3, 2), (4, 3), (5, 4), (6, 5) }

â® Hapi 1: StandardizojmÃ« tÃ« dhÃ«nat (nÃ« kÃ«tÃ« rast nuk Ã«shtÃ« domosdoshmÃ«ri pasi kanÃ« rritje lineare).

â® Hapi 2: GjejmÃ« matricÃ«n e kovariancÃ«s Î£

Î£ = cov(X) =
\| Var(xâ‚)â€ƒCov(xâ‚,xâ‚‚) |
\| Cov(xâ‚,xâ‚‚)â€ƒVar(xâ‚‚) |

NÃ« kÃ«tÃ« rast:

Var(xâ‚) = Var(xâ‚‚) = 2.5,â€ƒCov(xâ‚,xâ‚‚) = 2.5

âŸ¹ Î£ = | 2.5â€ƒ2.5 |
â€ƒâ€ƒâ€ƒâ€ƒ| 2.5â€ƒ2.5 |

â® Hapi 3: GjejmÃ« vlerat vetjake

\| 2.5âˆ’Î»â€ƒ2.5â€ƒ|
\| 2.5â€ƒâ€ƒ2.5âˆ’Î» | â‡’ det = (2.5âˆ’Î»)Â² âˆ’ 6.25 = 0
â‡’ Î»â‚ = 5,â€ƒÎ»â‚‚ = 0

â® Hapi 4: VektorÃ«t vetjakÃ« pÃ«r:

* Î»â‚ = 5: drejtimi \[1, 1]áµ—
* Î»â‚‚ = 0: drejtimi \[âˆ’1, 1]áµ—

â® Interpretim:

Komponenti kryesor Ã«shtÃ« drejtimi \[1,1] qÃ« pÃ«rfaqÃ«son 100% tÃ« variancÃ«s (sepse komponenti tjetÃ«r ka vlerÃ« 0).

â® ProjektojmÃ« tÃ« dhÃ«nat nÃ« kÃ«tÃ« drejtim dhe kemi njÃ« pÃ«rfaqÃ«sim nÃ« â„Â¹.

---

## 4.3 Vizualizim i Procesit tÃ« PCA

Do tÃ« vizualizoj nÃ« njÃ« grafik:

* TÃ« dhÃ«nat origjinale nÃ« â„Â²
* Drejtimet e komponentÃ«ve kryesorÃ«
* Projektime nÃ« drejtimin e parÃ« vetjak (reduktim i dimensionit)

Po e pÃ«rgatis grafikÃ«n taniâ€¦
import numpy as np
import matplotlib.pyplot as plt

# Origjinal data points
X = np.array([[2, 1], [3, 2], [4, 3], [5, 4], [6, 5]])
mean_X = np.mean(X, axis=0)
X_centered = X - mean_X

# Covariance matrix
cov_matrix = np.cov(X_centered.T)

# Eigen decomposition
eigvals, eigvecs = np.linalg.eig(cov_matrix)

# Sort eigenvectors by eigenvalues in descending order
idx = eigvals.argsort()[::-1]
eigvecs = eigvecs[:, idx]
eigvals = eigvals[idx]

# Project data onto first principal component
X_pca = X_centered @ eigvecs[:, 0].reshape(-1, 1)

# Reconstruct projected data for visualization
X_reconstructed = X_pca @ eigvecs[:, 0].reshape(1, -1) + mean_X

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], color='blue', label='Original Data')
plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], color='red', label='PCA Projection', marker='x')
plt.quiver(mean_X[0], mean_X[1], eigvecs[0, 0], eigvecs[1, 0], 
           angles='xy', scale_units='xy', scale=1, color='green', label='First PC')

for i in range(len(X)):
    plt.plot([X[i, 0], X_reconstructed[i, 0]], [X[i, 1], X_reconstructed[i, 1]], 'k--', linewidth=0.5)

plt.xlabel('Xâ‚')
plt.ylabel('Xâ‚‚')
plt.title('PCA: Projektime nÃ« Komponentin e ParÃ« Kryesor')
plt.legend()
plt.axis('equal')
plt.grid(True)
plt.tight_layout()
plt.show()


------

# Re-run code after session reset
import numpy as np
import matplotlib.pyplot as plt

# Original data points
X = np.array([[2, 1], [3, 2], [4, 3], [5, 4], [6, 5]])
mean_X = np.mean(X, axis=0)
X_centered = X - mean_X

# Covariance matrix
cov_matrix = np.cov(X_centered.T)

# Eigen decomposition
eigvals, eigvecs = np.linalg.eig(cov_matrix)

# Sort eigenvectors by eigenvalues in descending order
idx = eigvals.argsort()[::-1]
eigvecs = eigvecs[:, idx]
eigvals = eigvals[idx]

# Project data onto first principal component
X_pca = X_centered @ eigvecs[:, 0].reshape(-1, 1)

# Reconstruct projected data for visualization
X_reconstructed = X_pca @ eigvecs[:, 0].reshape(1, -1) + mean_X

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], color='blue', label='Original Data')
plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], color='red', label='PCA Projection', marker='x')
plt.quiver(mean_X[0], mean_X[1], eigvecs[0, 0], eigvecs[1, 0], 
           angles='xy', scale_units='xy', scale=1, color='green', label='First PC')

for i in range(len(X)):
    plt.plot([X[i, 0], X_reconstructed[i, 0]], [X[i, 1], X_reconstructed[i, 1]], 'k--', linewidth=0.5)

plt.xlabel('Xâ‚')
plt.ylabel('Xâ‚‚')
plt.title('PCA: Projektime nÃ« Komponentin e ParÃ« Kryesor')
plt.legend()
plt.axis('equal')
plt.grid(True)
plt.tight_layout()
plt.show()
------

Ky grafik paraqet shembullin e PCA pÃ«r tÃ« dhÃ«nat dy-dimensionale:

* PikÃ«t blu janÃ« tÃ« dhÃ«nat origjinale.
* Vija jeshile tregon drejtimin e komponentit tÃ« parÃ« kryesor (PC1).
* PikÃ«t e kuqe tregojnÃ« projektimin e tÃ« dhÃ«nave nÃ« kÃ«tÃ« komponent.
* Vijat e zeza me ndÃ«rprerje tregojnÃ« distancat midis tÃ« dhÃ«nave origjinale dhe projektimeve â€“ kjo Ã«shtÃ« humbja e informacionit nÃ« reduktimin e dimensionit.

NÃ« vazhdim do kaloj te Kapitulli 5 nÃ«se je gati. Ai do trajtojÃ« zbatime praktike tÃ« PCA dhe interpretimin e rezultateve nÃ« kontekste mÃ« tÃ« ndÃ«rlikuara. TÃ« vazhdoj?


