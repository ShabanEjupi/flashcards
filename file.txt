# Matricat Simetrike dhe Diagonalizimi Ortogonal

## Kapitulli 1: Përkufizime Bazë dhe Koncepte të Matricave

- 1.1 Matrica: përkufizimi, tipi, madhësia
- 1.2 Operacionet me Matrica
- 1.3 Transpozita dhe Inversa
- 1.4 Përkufizimi i një matrice simetrike
- 1.5 Karakteristikat e matricave simetrike

## Kapitulli 2: Eigenvlera dhe Eigenvektorë

- 2.1 Përkufizimi dhe kuptimi gjeometrik
- 2.2 Si të gjejmë eigenvlera: determinanta dhe polinomi karakteristik
- 2.3 Eigenspace dhe dimensioni i tij
- 2.4 Rastet kur eigenvlera përsëritet (multipliciteti algjebrik dhe gjeometrik)

## Kapitulli 3: Teorema Spektrale dhe Matricat Simetrike

- 3.1 Teorema Spektrale për Matricat Simetrike
- 3.2 Ortogonaliteti i eigenvektorëve
- 3.3 Prova e teoremës spektrale
- 3.4 Shembuj të plotë me zgjidhje hap pas hapi

## Kapitulli 4: Diagonalizimi Ortogonal

- 4.1 Përkufizimi i diagonalizimit
- 4.2 Matrica ortogonale dhe vetitë e saj
- 4.3 Procesi i diagonalizimit për matrica simetrike
- 4.4 Si të ndërtojmë matricën ortogonale P dhe matricën diagonale D
- 4.5 Shembuj të plotë

## Kapitulli 5: Ushtrime dhe Aplikime

- 5.1 Verifikimi i simetrisë dhe ortogonalitetit të matricave
- 5.2 Gjetja e eigenvlerave dhe eigenspace
- 5.3 Diagonalizimi ortogonal: aplikime numerike dhe teorike
- 5.4 Aplikime në modelimin e rritjes së popullsisë
- 5.5 Pyetje True/False me argumentim teorik

## Kapitulli 6: Shtojcë Teorike

- 6.1 Përmbledhje e rezultateve kryesore
- 6.2 Teorema që ndihmojnë në diagonalizim
- 6.3 Veti të rëndësishme të matricave ortogonale
- 6.4 Lidhja me analizën spektrale



import matplotlib.pyplot as plt
import numpy as np

# Set style for consistency
plt.style.use('seaborn-vibrant')

# Define a symmetric matrix A
A = np.array([[2, 1],
              [1, 3]])

# Define a set of vectors
v1 = np.array([1, 0])
v2 = np.array([0, 1])
v3 = np.array([1, 1])

# Apply matrix A to each vector
Av1 = A @ v1
Av2 = A @ v2
Av3 = A @ v3

# Plot original and transformed vectors
fig, ax = plt.subplots(figsize=(8, 8))
origin = np.zeros(2)

# Original vectors in blue
ax.quiver(*origin, *v1, angles='xy', scale_units='xy', scale=1, color='blue', label='v1')
ax.quiver(*origin, *v2, angles='xy', scale_units='xy', scale=1, color='blue', label='v2')
ax.quiver(*origin, *v3, angles='xy', scale_units='xy', scale=1, color='blue', label='v3')

# Transformed vectors in red
ax.quiver(*origin, *Av1, angles='xy', scale_units='xy', scale=1, color='red', label='A * v1')
ax.quiver(*origin, *Av2, angles='xy', scale_units='xy', scale=1, color='red', label='A * v2')
ax.quiver(*origin, *Av3, angles='xy', scale_units='xy', scale=1, color='red', label='A * v3')

# Plot settings
ax.set_xlim(-1, 6)
ax.set_ylim(-1, 6)
ax.set_aspect('equal')
ax.grid(True)
ax.set_title("Transformimi i vektorëve me një matricë simetrike A")
ax.set_xlabel("Boshti x")
ax.set_ylabel("Boshti y")

# Avoid duplicate labels in legend
handles, labels = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels, handles))
ax.legend(by_label.values(), by_label.keys())

plt.tight_layout()
plt.show()


-------
# Fixing the style to a default one that exists
plt.style.use('seaborn-darkgrid')

# Define a symmetric matrix A
A = np.array([[2, 1],
              [1, 3]])

# Define a set of vectors
v1 = np.array([1, 0])
v2 = np.array([0, 1])
v3 = np.array([1, 1])

# Apply matrix A to each vector
Av1 = A @ v1
Av2 = A @ v2
Av3 = A @ v3

# Plot original and transformed vectors
fig, ax = plt.subplots(figsize=(8, 8))
origin = np.zeros(2)

# Original vectors in blue
ax.quiver(*origin, *v1, angles='xy', scale_units='xy', scale=1, color='blue', label='v1')
ax.quiver(*origin, *v2, angles='xy', scale_units='xy', scale=1, color='blue', label='v2')
ax.quiver(*origin, *v3, angles='xy', scale_units='xy', scale=1, color='blue', label='v3')

# Transformed vectors in red
ax.quiver(*origin, *Av1, angles='xy', scale_units='xy', scale=1, color='red', label='A * v1')
ax.quiver(*origin, *Av2, angles='xy', scale_units='xy', scale=1, color='red', label='A * v2')
ax.quiver(*origin, *Av3, angles='xy', scale_units='xy', scale=1, color='red', label='A * v3')

# Plot settings
ax.set_xlim(-1, 6)
ax.set_ylim(-1, 6)
ax.set_aspect('equal')
ax.grid(True)
ax.set_title("Transformimi i vektorëve me një matricë simetrike A")
ax.set_xlabel("Boshti x")
ax.set_ylabel("Boshti y")

# Avoid duplicate labels in legend
handles, labels = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels, handles))
ax.legend(by_label.values(), by_label.keys())

plt.tight_layout()
plt.show()

----
# Kapitulli 1: Matricat Simetrike dhe Transformimet Lineare

## 1.1 Hyrje në Matricat Simetrike

Një matricë A ∈ ℙ^{n x n} quhet simetrike nëse A^T = A. Kjo do të thotë që elementët e saj janë të vendosur në mënyrë të tillë që a\_{ij} = a\_{ji}.

**Shembull**:
$A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$

Ky është një shembull i një matrice 2x2 simetrike, sepse a\_{12} = a\_{21}.

## 1.2 Vetivlerat dhe Vetivektorët

Për një matricë A, një vektor jo-zero $\mathbf{v}$ quhet vetivektor nëse:
$A\mathbf{v} = \lambda\mathbf{v}$
ku $\lambda$ është vetivlera përkatëse.

### Veti të rëndësishme:

* Çdo matricë simetrike ka vetëm vetivlera reale.
* Vetivektorët që korrespondojnë me vetivlera të ndryshme janë ortogonalë.

## 1.3 Transformimi i Vektorëve

Kur një vektor shumëzohet me një matricë, ai transformohet në një drejtim të ri. Për matricat simetrike, ky transformim është i veçantë sepse ruan ortogonalitetin dhe strukturën gjeometrike të vektorëve.

**Shembull Vizual**:
Më poshtë kemi një shembull ku matrica simetrike:
$A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$

transformon tre vektorë $\mathbf{v}_1 = [1, 0], \mathbf{v}_2 = [0, 1], \mathbf{v}_3 = [1, 1]$.

Transformimet respektive janë:

* $A\mathbf{v}_1 = [2, 1]$
* $A\mathbf{v}_2 = [1, 3]$
* $A\mathbf{v}_3 = [3, 4]$

Grafiku më sipër tregon drejtimet fillestare (blu) dhe drejtimet pas transformimit (të kuqe).

---

Në kapitullin tjetër do të prezantojmë konceptin e diagonalizimit ortogonal dhe aplikimin e tij në dekompozimin e matricave.


# Kapitulli 2: Diagonalizimi Ortogonal i Matricave Simetrike

## 2.1 Hyrje

Në këtë kapitull do të shqyrtojmë procesin e diagonalizimit ortogonal të matricave simetrike. Matricat simetrike kanë veti të veçanta që u mundësojnë të diagonalizohen përmes matricave ortogonale. Kjo është një nga arsyet pse matricat simetrike janë shumë të rëndësishme në matematikë, fizikë dhe inxhinieri.

## 2.2 Përkufizimi

Një matricë reale A është simetrike nëse A^T = A.
Një matricë A është ortogonalisht diagonalizueshme nëse ekziston një matricë ortogonale P dhe një matricë diagonale D e tillë që:

A = P D P^T

## 2.3 Teorema Kryesore

Çdo matricë reale simetrike është ortogonalisht diagonalizueshme. Pra, nëse A është matricë simetrike, atëherë ekziston një matricë ortogonale P dhe një matricë diagonale D të tilla që A = P D P^T.

## 2.4 Procesi i Diagonalizimit Ortogonal

1. Gjej vlerat e veta (eigenvalues) të matricës simetrike A.
2. Për secilën vlerë të vet λ, gjej hapësirën e vet (eigenspace) dhe një bazë ortonormale të saj.
3. Ndërto matricën P me kolonat e saj të jenë vektorët ortonormalë të vet.
4. Ndërto matricën D me vlerat përkatëse të vet në diagonale.
5. Vërteto që A = P D P^T.

## 2.5 Shembull i Punuar

Jepet matrica A:

A = | 4 1 |
\| 1 4 |

1. Gjej vlerat e veta:
   Det(A - λI) = (4-λ)^2 - 1 = λ^2 - 8λ + 15 = 0
   λ₁ = 3, λ₂ = 5

2. Për λ = 3: (A - 3I)x = 0 → sistemi linear → bazë: \[1, -1]
   Për λ = 5: (A - 5I)x = 0 → bazë: \[1, 1]

3. Ortonormalizimi:
   v₁ = 1/√2 \[1, -1], v₂ = 1/√2 \[1, 1]

4. Matrica P = \[v₁ v₂], D = diag(3, 5)

5. A = P D P^T vërtetohet me shumëzim matricor.

## 2.6 Interpretim Grafik (për t'u shtuar më vonë me grafikë)

Transformimi i matricës A shpjegohet si një kombinim i rrotullimit dhe shtrirjes përgjatë drejtimeve të vektorëve të vet. Kjo ndihmon në kuptimin vizual të efektit të A mbi hapësirën vektoriale ℝ².

---

Vijon në Kapitullin 3: Matricat Ortogonale.



# Kapitulli 3: Matricat Ortogonale

## 3.1 Përkufizimi i Matricave Ortogonale

Një matricë katrore $P$ quhet matricë ortogonale nëse rreshtat (ose kolonat) e saj janë vektorë njësie dhe ortogonalë ndaj njëri-tjetrit. Formalisht, një matricë $P \in \mathbb{R}^{n \times n}$ është ortogonale nëse:

$$
P^T P = I \quad \text{ose ekuivalent:} \quad P^{-1} = P^T
$$

Kjo do të thotë që kolona (dhe gjithashtu rreshtat) e $P$ përbëjnë një bazë ortonormale të $\mathbb{R}^n$.

## 3.2 Veti të Matricave Ortogonale

* Ruajnë gjatësinë e vektorëve: $\| P\vec{x} \| = \| \vec{x} \|$
* Ruajnë këndet ndërmjet vektorëve.
* Janë të kthyeshme dhe inversi i tyre është baras me transpozitën: $P^{-1} = P^T$
* Produkti i matricave ortogonale është gjithashtu matricë ortogonale.

## 3.3 Shembull i Matricës Ortogonale

Le të marrim matricën:

$$
P = \begin{bmatrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}
$$

Vërtetojmë që është ortogonale:

$$
P^T = \begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}
$$

$$
P^T P = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I
$$

Pra, $P$ është matricë ortogonale.

## 3.4 Rëndësia në Diagonalizim

Në diagonalizimin ortogonal, kërkojmë një matricë ortogonale $P$ e cila e diagonalizon një matricë simetrike $A$:

$$
A = P D P^T
$$

Kjo ndarje është e dobishme në shumë fusha të matematikës dhe inxhinierisë si për shembull në analizën e spektrit, dekompozime të sinjaleve dhe më shumë.

---

A je gati të vazhdojmë me Kapitullin 4 për "Diagonalizimin Ortogonal të Matricave 2×2 dhe 3×3" me shembuj konkretë?



---# Kapitulli 4: Diagonalizimi Ortogonal i Matricave 2×2 dhe 3×3

## 4.1 Përkufizimi i Diagonalizimit Ortogonal

Një matricë A ∈ ℝⁿˣⁿ quhet diagonalizueshme në mënyrë ortogonale nëse ekziston një matricë ortogonale P dhe një matricë diagonale D e tillë që:

A = P D Pᵀ

Kjo do të thotë që mund ta shkruajmë matricën A si produkt i një matrice ortogonale, një matrice diagonale dhe transpozitës së saj. Kjo është veçanërisht e mundur kur A është matricë simetrike.

## 4.2 Procedura për Diagonalizim Ortogonal

Për një matricë simetrike A ∈ ℝⁿˣⁿ:

1. Gjej vlerat karakteristike (eigenvalues) të A.
2. Gjej vektorët karakteristikë për secilën vlerë karakteristike.
3. Ortonormalizo vektorët karakteristikë nëpërmjet Gram-Schmidt nëse nuk janë ortonormalë.
4. Formo matricën ortogonale P nga këta vektorë si kolona.
5. D = Pᵀ A P do të jetë matricë diagonale.

## 4.3 Shembull për Matricë 2×2

Le të shohim matricën:

A = \[ 4  2
2  3 ]

1. Gjej vlerat karakteristike:

Det(A - λI) = |4-λ   2|
|2    3-λ| = (4-λ)(3-λ) - 4 = λ² - 7λ + 8 → λ₁ = 4.561, λ₂ = 2.439

2. Gjej vektorët karakteristikë për secilën vlerë karakteristike.

Për λ₁ = 4.561: zgjidh (A - λI)x = 0 → v₁ ≈ \[0.788, 0.615]
Për λ₂ = 2.439: zgjidh (A - λI)x = 0 → v₂ ≈ \[-0.615, 0.788]

3. Vektorët janë të ortonormalizuar → formojmë matricën P:

P = \[ 0.788  -0.615
0.615   0.788 ] (matricë ortogonale)

4. D = Pᵀ A P = \[ 4.561  0
   0      2.439 ]

Pra A është diagonalizuar në mënyrë ortogonale.

## 4.4 Shembull për Matricë 3×3

Marrim matricën simetrike:

A = \[ 6  2  0
2  3  0
0  0  1 ]

1. Vlerat karakteristike janë:
   λ₁ = 7, λ₂ = 2, λ₃ = 1

2. Vektorët karakteristikë përkatës:
   v₁ = \[2,1,0], v₂ = \[-1,2,0], v₃ = \[0,0,1]

3. Ortonormalizojmë:

* Normalizo v₁: u₁ = v₁ / ||v₁|| = \[0.894, 0.447, 0]
* Normalizo v₂: u₂ = \[-0.447, 0.894, 0]
* v₃ është veçse ortonormal: u₃ = \[0, 0, 1]

4. Formojmë matricën P:

P = \[ 0.894  -0.447   0
0.447   0.894   0
0        0      1 ]

5. Diagonalizimi:
   D = Pᵀ A P = diag(7, 2, 1)

## 4.5 Interpretim Grafik

Për matricat 2×2, diagonalizimi ortogonal mund të vizualizohet si një rotacion që transformon bazën në një sistem ku matrica sillet si shtrirje (scaling) përgjatë akseve të reja.

Do të shtojmë grafikë në kapitullin 5 për ilustrime më të qarta.

---

Gati të vazhdojmë me Kapitullin 5 ku do të përdorim grafikë për të ilustruar diagonalizimin ortogonal në mënyrë vizuale.

-----
# Kapitulli 4: Diagonalizimi Ortogonal i Matricave Simetrike

## 4.1 Diagonalizimi Ortogonal i Matricave 2×2

Një matricë A është e diagonalizueshme ortogonalisht nëse ekziston një matricë ortogonale P dhe një matricë diagonale D e tillë që:

A = PDPᵀ

Për matricat simetrike 2×2, procesi përfshin:

1. Gjetjen e vlerave vetjake (eigenvalues) λ të A duke zgjidhur ekuacionin det(A - λI) = 0.
2. Për secilën vlerë vetjake λ, gjej vektorët vetjakë përkatës duke zgjidhur (A - λI)x = 0.
3. Ortogonalizo dhe normalizo vektorët vetjakë për të ndërtuar matricën ortogonale P.
4. Formo matricën diagonale D duke vendosur vlerat vetjake në diagonale.

### Shembull

Jepet matrica simetrike:

A =
\| 4  1 |
\| 1  3 |

1. Gjejmë vlerat vetjake:

det(A - λI) = det
\| 4−λ  1   |
\| 1    3−λ | = (4−λ)(3−λ) − 1 = λ² − 7λ + 11 = 0

Zgjidhim për λ:

λ₁ = (7 + √5)/2, λ₂ = (7 − √5)/2

2. Gjejmë vektorët vetjakë për secilën λ.

3. Normalizojmë ata për të formuar P.

4. Formojmë D = diag(λ₁, λ₂).

## 4.2 Diagonalizimi Ortogonal i Matricave 3×3

Për matricat 3×3 simetrike, procedura është e njëjtë por më e ndërlikuar për shkak të llogaritjeve:

1. Gjej vlerat vetjake duke zgjidhur det(A − λI) = 0, një polinom kubik.
2. Për secilën vlerë vetjake, gjej bazën e hapësirës vetjake përkatëse (eigenspace).
3. Përdor procesin Gram-Schmidt për të ortogonalizuar dhe normalizuar vektorët nëse është e nevojshme.
4. Konstruo matricën ortogonale P dhe matricën diagonale D.

### Shembull për 3×3

A =
\| 6  2  0 |
\| 2  3  1 |
\| 0  1  1 |

1. Llogarit det(A − λI) = 0 → Gjej λ₁, λ₂, λ₃
2. Gjej vektorët vetjakë për secilën λ.
3. Normalizo dhe ortogonalizo për të marrë P.
4. Përdor A = PDPᵀ për të verifikuar diagonalizimin.

Në kapitullin tjetër do të përfshijmë grafikë për të ndihmuar në vizualizimin e vektorëve vetjakë dhe mënyrën sesi ata transformojnë hapësirën.
----

Shkëlqyeshëm. Po vazhdojmë me:

## 📘 Kapitulli 5: Interpretimi Gjeometrik i Vektorëve Vetjakë dhe Vlerave Vetjake

Në këtë kapitull do të përqendrohemi në kuptimin vizual dhe gjeometrik të vektorëve vetjakë dhe vlerave vetjake të matricave, duke përdorur grafikë për të ilustruar konceptet.

---

### 5.1 Interpretimi Gjeometrik i Vektorëve Vetjakë

Një vektor vetjak i një matrice A është një drejtim në hapësirë i cili, kur shumëzohet me matricën A, nuk ndryshon drejtimin – ndryshon vetëm gjatësia (shkallëzohet).

📌 Shembull intuitiv:

* Mendo një vektor në një plan që kur shumëzohet me një transformim (matricë), nuk kthehet në ndonjë drejtim tjetër – thjesht shtrihet ose tkurret.

📐 Në mënyrë grafike:

* Vektorët vetjakë shfaqen si shigjeta në hapësirë që ndjekin të njëjtin drejtim si më parë.
* Vlerat vetjake shprehin shkallëzimin e këtyre vektorëve.

---

### 5.2 Shembull me Grafik në ℝ²

Marrim matricën:

$$
A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}
$$

Kjo matricë ka vlerat vetjake λ₁ = 3 dhe λ₂ = 2, me vektorë vetjakë përkatës:

* për λ₁ = 3: vektor vetjak v₁ = \[1, 0]ᵀ
* për λ₂ = 2: vektor vetjak v₂ = \[1, 1]ᵀ

🟦 Në grafik:

* do të vizatojmë planin 2D
* do të tregojmë vektorët v₁ dhe v₂ përpara dhe pas shumëzimit me matricën A
* do të theksojmë që drejtimi i tyre mbetet i njëjtë, por gjatësia ndryshon

💡 Ky është thelbi i vektorëve vetjakë: drejtimi që nuk ndryshon nga transformimi linear.

---

📊 Do të shtojmë edhe një grafik ku një grup vektorësh të rastësishëm transformohen nga A dhe vetëm vektorët vetjakë ruajnë drejtimin.

---

### 5.3 Aplikime Gjeometrike

* Në fizikë: vektorët vetjakë përfaqësojnë drejtimet ku një sistem qëndron në ekuilibër.
* Në grafikë kompjuterike: përdoren për të zbuluar rrotullime, shtrirje dhe reflektime.
* Në sistemet dinamike: tregojnë drejtimet e stabilitetit apo rritjes së sistemit me kalimin e kohës.

---

Në vazhdim:

* Do të vizatojmë grafikët për ℝ² me shembullin më sipër.
* Pastaj do të kalojmë në ℝ³ dhe do të shqyrtojmë një shembull 3D për interpretim hapësinor të vektorëve vetjakë.

A të filloj me gjenerimin e grafikëve për ℝ² dhe ℝ³, apo ke ndonjë kërkesë të veçantë para se të vazhdoj?



---------


import matplotlib.pyplot as plt
import numpy as np

# Define the matrix A
A = np.array([[3, 1],
              [0, 2]])

# Define the eigenvectors (before transformation)
v1 = np.array([1, 0])  # Eigenvector corresponding to λ=3
v2 = np.array([1, 1])  # Eigenvector corresponding to λ=2

# Apply the matrix A to the eigenvectors
Av1 = A @ v1
Av2 = A @ v2

# Create plot
fig, ax = plt.subplots(figsize=(8, 8))

# Original vectors (blue)
ax.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', label='v₁')
ax.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='blue', label='v₂')

# Transformed vectors (red)
ax.quiver(0, 0, Av1[0], Av1[1], angles='xy', scale_units='xy', scale=1, color='red', label='A·v₁')
ax.quiver(0, 0, Av2[0], Av2[1], angles='xy', scale_units='xy', scale=1, color='red', label='A·v₂')

# Plot settings
ax.set_xlim(-1, 5)
ax.set_ylim(-1, 5)
ax.set_aspect('equal')
ax.grid(True)
ax.axhline(0, color='black', linewidth=0.5)
ax.axvline(0, color='black', linewidth=0.5)
ax.set_title('Vektorë Vetjakë dhe Transformimi i Tyre nga A')
ax.legend()
plt.show()
----

Ky grafik paraqet dy vektorë vetjakë (v₁ dhe v₂) për matricën A dhe transformimin e tyre pas shumëzimit me matricën A:

* Vektorët blu (v₁ = \[1, 0] dhe v₂ = \[1, 1]) janë vektorët vetjakë përkatës për vlerat vetjake λ₁ = 3 dhe λ₂ = 2.
* Vektorët e kuq tregojnë rezultatet e shumëzimit A·v₁ dhe A·v₂. Siç vërehet:

  * A·v₁ = 3·v₁ → drejtimi nuk ndryshon, por zgjatet 3 herë.
  * A·v₂ = 2·v₂ → drejtimi ruhet dhe vektori zgjatet 2 herë.

Ky ilustrim ndihmon për të kuptuar vizualisht çfarë do të thotë të jesh vektor vetjak: vektori ndryshon vetëm për nga gjatësia, por jo për nga drejtimi, kur shumëzohet me matricën. Në kapitullin tjetër, ne do të vazhdojmë me mënyrën se si të gjejmë këto vektorë dhe vlera për çdo matricë të dhënë.


Shumë mirë! Do të vazhdojmë me kapitullin tjetër sipas strukturës së ndërtuar më herët.

🟦 Kapitulli 2: Gjetja e Vlerave dhe Vektorëve Vetjakë të një Matrice

Ky kapitull ka për qëllim të shpjegojë në mënyrë të plotë dhe metodike se si të gjejmë vlerat vetjake (eigenvalues) dhe vektorët vetjakë (eigenvectors) për një matricë të dhënë katrore A ∈ ℝⁿˣⁿ. Ky proces përfshin:

* Zgjidhjen e ekuacionit karakteristik për të gjetur vlerat vetjake λ.
* Përdorimin e secilës λ për të gjetur vektorët përkatës të vetjakë.
* Interpretimin e rezultateve.

---

## 2.1 Teoria Themeltare

Për një matricë katrore A ∈ ℝⁿˣⁿ, një numër λ ∈ ℝ quhet vlerë vetjake e A nëse ekziston një vektor jo-zero x ∈ ℝⁿ i tillë që:

A · x = λ · x

Ky ekuacion mund të riformulohet si:

(A − λI) · x = 0

Kjo është një sistem homogjen linear. Ai ka zgjidhje jo-triviale (dmth. x ≠ 0) vetëm nëse det(A − λI) = 0. Ky është ekuacioni karakteristik.

---

## 2.2 Shembull me Matricë 2×2

Le të konsiderojmë matricën:

A = | 4 1 |
  | 2 3 |

⮞ Hapi 1: Gjejmë Ekuacionin Karakteristik

Llogarisim det(A − λI):

\| 4−λ 1 |
\| 2  3−λ |

det(A − λI) = (4−λ)(3−λ) − (2×1)
      = λ² − 7λ + 10

⮞ Hapi 2: Zgjidhim Ekuacionin Karakteristik

λ² − 7λ + 10 = 0
(λ − 5)(λ − 2) = 0
⇒ λ₁ = 5  λ₂ = 2

⮞ Hapi 3: Gjejmë Vektorët përkatës të vetjakë

Për λ₁ = 5:

A − 5I = | −1 1 |
     |  2 −2 |
Zgjidhim sistemin:

−x + y = 0 ⇒ y = x
⇒ v₁ = k₁ · \[1, 1]ᵗ

Për λ₂ = 2:

A − 2I = | 2 1 |
     | 2 1 |
⇒ 2x + y = 0 ⇒ y = −2x
⇒ v₂ = k₂ · \[1, −2]ᵗ

---

## 2.3 Vëzhgim Grafik

⮞ Nëse dëshiron, mund të përfshij një grafik si më parë që ilustron si A · v = λ · v për të dy vektorët vetjakë të mësipërm.

---

Të vazhdoj tani me Kapitullin 3: Diagonalizimi i matricave të simetrike dhe ortogonale? Apo dëshiron të përfshijmë një grafik për këtë shembull më parë?


Shumë mirë — atëherë po e ndjek rrjedhën logjike dhe të përgatitur. Më poshtë do vazhdoj me kapitullin e radhës sipas strukturës që kemi ndërtuar:

🟦 Kapitulli 3: Diagonalizimi i Matricave – Rasti i Matricave Simetrike dhe Ortogonale

Qëllimi i këtij kapitulli është të shpjegojë qartë dhe me hollësi konceptin e diagonalizimit të matricave, veçanërisht kur ato janë simetrike. Kjo është një temë qendrore në analizën spektrale dhe në zbatime të shumta të matematikës dhe inteligjencës artificiale.

---

## 3.1 Diagonalizimi – Koncepti Themelor

Një matricë A ∈ ℝⁿˣⁿ është e diagonalizueshme nëse ekziston një matricë P e inverteueshme dhe një matricë D diagonale e tillë që:

A = P D P⁻¹

Kjo do të thotë se mund ta paraqesim A si transformim përmes një baze të re të përbërë nga vektorët vetjakë të saj.

⮞ Kur A është matricë reale dhe simetrike (A = Aᵀ), ajo është gjithmonë diagonalizueshme përmes një matrice ortogonale P:

A = P D Pᵀ

Ky rezultat njihet si Teorema Spektrale për matricat simetrike.

---

## 3.2 Shembull Praktik: Diagonalizim i një Matrice Simetrike

Le të marrim matricën:

A = | 4 2 |
  | 2 3 |

Kjo matricë është reale dhe simetrike (A = Aᵀ).

⮞ Hapi 1: Gjejmë vlerat vetjake të A

Llogarisim det(A − λI):

\| 4−λ 2 |
\| 2  3−λ | = (4−λ)(3−λ) − 4 = λ² − 7λ + 8

Zgjidhim: λ² − 7λ + 8 = 0 ⇒ (λ−1)(λ−8) = 0
⟹ λ₁ = 1  λ₂ = 8

⮞ Hapi 2: Gjejmë vektorët vetjakë

Për λ₁ = 1:

(A − I)x = 0 ⇒ | 3 2 |
          | 2 2 | ⇒ zgjidhja: x = k₁ · \[−2, 3]ᵗ

Për λ₂ = 8:

(A − 8I)x = 0 ⇒ | −4 2 |
          | 2 −5 | ⇒ zgjidhja: x = k₂ · \[1, 2]ᵗ

⮞ Hapi 3: Normalizimi i vektorëve (për të krijuar matricën ortogonale P)

v₁ = \[−2, 3]ᵗ  ⇒ v₁\_norm = 1/√13 · \[−2, 3]ᵗ
v₂ = \[1, 2]ᵗ  ⇒ v₂\_norm = 1/√5 · \[1, 2]ᵗ

P = \[v₁\_norm v₂\_norm] është një matricë ortogonale.

D = diag(1, 8)

⮞ Verifikim:

Pᵀ A P = D

---

## 3.3 Interpretim Gjeometrik dhe Zbatim

* Diagonalizimi për matricat simetrike përfaqëson një rotacion të hapësirës vektoriale në një bazë ku A vepron si shtrirje përgjatë akseve.
* Në mësimin e algoritmeve të reduktimit të dimensionit si PCA (Principal Component Analysis), përdorim diagonalizimin ortogonal të matricave të kovariancës (që janë simetrike).
* Për çdo funksion ku përfshihet zbutja, përpunimi ose ndryshimi i shkallës së një strukture vektorësh, diagonalizimi ofron ndarjen më të pastër për analizë.

---

Tani, nëse dëshiron, mund të:

* Kaloj te Kapitulli 4 (Aplikime të diagonalizimit në reduktimin e dimensionit – PCA).
* Vizualizoj me grafik ndikimin e diagonalizimit mbi një grup vektorësh.
* Ndërtoj një shembull në R ose Python.

A preferon të vazhdoj me kapitullin tjetër apo të përfshij vizualizime?


Shkëlqyeshëm! Atëherë po vazhdoj me Kapitullin 4 sipas strukturës së planifikuar.

🟦 Kapitulli 4: Reduktimi i Dimensionit dhe Analiza Kryesore e Komponentëve (PCA)

Qëllimi i këtij kapitulli është të tregojë se si diagonalizimi i matricave simetrike – si matrica e kovariancës – luan rol kyç në reduktimin e dimensionit për të analizuar të dhëna me shumë variabla. PCA është një nga metodat më të rëndësishme në statistikë, machine learning dhe analizën e të dhënave.

---

## 4.1 Parimet Themelore të PCA

Principal Component Analysis (PCA) është një teknikë që:

* Zbulon drejtimet kryesore (komponentët kryesorë) përgjatë të cilave të dhënat ndryshojnë më shumë.
* Redukton dimensionin duke ruajtur sa më shumë informacion të jetë e mundur.

Kjo bëhet përmes:

* Standardizimit të të dhënave (nëse kërkohet).
* Llogaritjes së matrices së kovariancës.
* Gjetjes së vlerave dhe vektorëve vetjakë (eigenvalues, eigenvectors).
* Përzgjedhjes së komponentëve më të rëndësishëm (me vlera vetjake më të mëdha).
* Projektojmë të dhënat në bazën e re për të ulur dimensionin.

---

## 4.2 Shembull Konceptual

Supozoni se kemi të dhëna në ℝ² me dy variabla të korreluar:

X = { (2, 1), (3, 2), (4, 3), (5, 4), (6, 5) }

⮞ Hapi 1: Standardizojmë të dhënat (në këtë rast nuk është domosdoshmëri pasi kanë rritje lineare).

⮞ Hapi 2: Gjejmë matricën e kovariancës Σ

Σ = cov(X) =
\| Var(x₁) Cov(x₁,x₂) |
\| Cov(x₁,x₂) Var(x₂) |

Në këtë rast:

Var(x₁) = Var(x₂) = 2.5, Cov(x₁,x₂) = 2.5

⟹ Σ = | 2.5 2.5 |
    | 2.5 2.5 |

⮞ Hapi 3: Gjejmë vlerat vetjake

\| 2.5−λ 2.5 |
\| 2.5  2.5−λ | ⇒ det = (2.5−λ)² − 6.25 = 0
⇒ λ₁ = 5, λ₂ = 0

⮞ Hapi 4: Vektorët vetjakë për:

* λ₁ = 5: drejtimi \[1, 1]ᵗ
* λ₂ = 0: drejtimi \[−1, 1]ᵗ

⮞ Interpretim:

Komponenti kryesor është drejtimi \[1,1] që përfaqëson 100% të variancës (sepse komponenti tjetër ka vlerë 0).

⮞ Projektojmë të dhënat në këtë drejtim dhe kemi një përfaqësim në ℝ¹.

---

## 4.3 Vizualizim i Procesit të PCA

Do të vizualizoj në një grafik:

* Të dhënat origjinale në ℝ²
* Drejtimet e komponentëve kryesorë
* Projektime në drejtimin e parë vetjak (reduktim i dimensionit)

Po e përgatis grafikën tani…
import numpy as np
import matplotlib.pyplot as plt

# Origjinal data points
X = np.array([[2, 1], [3, 2], [4, 3], [5, 4], [6, 5]])
mean_X = np.mean(X, axis=0)
X_centered = X - mean_X

# Covariance matrix
cov_matrix = np.cov(X_centered.T)

# Eigen decomposition
eigvals, eigvecs = np.linalg.eig(cov_matrix)

# Sort eigenvectors by eigenvalues in descending order
idx = eigvals.argsort()[::-1]
eigvecs = eigvecs[:, idx]
eigvals = eigvals[idx]

# Project data onto first principal component
X_pca = X_centered @ eigvecs[:, 0].reshape(-1, 1)

# Reconstruct projected data for visualization
X_reconstructed = X_pca @ eigvecs[:, 0].reshape(1, -1) + mean_X

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], color='blue', label='Original Data')
plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], color='red', label='PCA Projection', marker='x')
plt.quiver(mean_X[0], mean_X[1], eigvecs[0, 0], eigvecs[1, 0], 
           angles='xy', scale_units='xy', scale=1, color='green', label='First PC')

for i in range(len(X)):
    plt.plot([X[i, 0], X_reconstructed[i, 0]], [X[i, 1], X_reconstructed[i, 1]], 'k--', linewidth=0.5)

plt.xlabel('X₁')
plt.ylabel('X₂')
plt.title('PCA: Projektime në Komponentin e Parë Kryesor')
plt.legend()
plt.axis('equal')
plt.grid(True)
plt.tight_layout()
plt.show()


------

# Re-run code after session reset
import numpy as np
import matplotlib.pyplot as plt

# Original data points
X = np.array([[2, 1], [3, 2], [4, 3], [5, 4], [6, 5]])
mean_X = np.mean(X, axis=0)
X_centered = X - mean_X

# Covariance matrix
cov_matrix = np.cov(X_centered.T)

# Eigen decomposition
eigvals, eigvecs = np.linalg.eig(cov_matrix)

# Sort eigenvectors by eigenvalues in descending order
idx = eigvals.argsort()[::-1]
eigvecs = eigvecs[:, idx]
eigvals = eigvals[idx]

# Project data onto first principal component
X_pca = X_centered @ eigvecs[:, 0].reshape(-1, 1)

# Reconstruct projected data for visualization
X_reconstructed = X_pca @ eigvecs[:, 0].reshape(1, -1) + mean_X

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], color='blue', label='Original Data')
plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], color='red', label='PCA Projection', marker='x')
plt.quiver(mean_X[0], mean_X[1], eigvecs[0, 0], eigvecs[1, 0], 
           angles='xy', scale_units='xy', scale=1, color='green', label='First PC')

for i in range(len(X)):
    plt.plot([X[i, 0], X_reconstructed[i, 0]], [X[i, 1], X_reconstructed[i, 1]], 'k--', linewidth=0.5)

plt.xlabel('X₁')
plt.ylabel('X₂')
plt.title('PCA: Projektime në Komponentin e Parë Kryesor')
plt.legend()
plt.axis('equal')
plt.grid(True)
plt.tight_layout()
plt.show()
------

Ky grafik paraqet shembullin e PCA për të dhënat dy-dimensionale:

* Pikët blu janë të dhënat origjinale.
* Vija jeshile tregon drejtimin e komponentit të parë kryesor (PC1).
* Pikët e kuqe tregojnë projektimin e të dhënave në këtë komponent.
* Vijat e zeza me ndërprerje tregojnë distancat midis të dhënave origjinale dhe projektimeve – kjo është humbja e informacionit në reduktimin e dimensionit.

Në vazhdim do kaloj te Kapitulli 5 nëse je gati. Ai do trajtojë zbatime praktike të PCA dhe interpretimin e rezultateve në kontekste më të ndërlikuara. Të vazhdoj?


